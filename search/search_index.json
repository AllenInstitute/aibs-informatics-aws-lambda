{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AIBS Informatics AWS Lambda","text":""},{"location":"#overview","title":"Overview","text":"<p>This is a base package that can be used standalone with some core lambda functionality or as a dependency. It contains several classes and functions that make it easy to create strongly typed lambda functions with many nice-to-have features including:</p> <ul> <li>Serialization/deserialization</li> <li>Easy metrics integration</li> <li>Utilities for batch SQS and DynamoDB event bridge processing</li> <li>Collection of general purpose lambda handler classes</li> </ul>"},{"location":"#features","title":"Features","text":""},{"location":"#base-classes","title":"Base Classes","text":"Class Description <code>LambdaHandler</code> Base class for creating strongly typed lambda functions <code>ApiLambdaHandler</code> Base class for API Gateway handlers <code>ApiResolverBuilder</code> Utility class for building API Gateway resolvers"},{"location":"#standalone-lambda-handlers","title":"Standalone Lambda Handlers","text":""},{"location":"#aws-batch","title":"AWS Batch","text":"<ul> <li><code>CreateDefinitionAndPrepareArgsHandler</code> - Handles creation and preparation of AWS Batch job definitions</li> </ul>"},{"location":"#data-sync","title":"Data Sync","text":"<ul> <li><code>GetJSONFromFileHandler</code> - Retrieves JSON data from a file</li> <li><code>PutJSONToFileHandler</code> - Writes JSON data to a file</li> <li><code>DataSyncHandler</code> - Simple data sync task</li> <li><code>BatchDataSyncHandler</code> - Handles batch of data sync tasks</li> <li><code>PrepareBatchDataSyncHandler</code> - Prepares data synchronization for AWS Batch</li> <li><code>GetDataPathStatsHandler</code> - Retrieves statistics about data paths</li> <li><code>ListDataPathsHandler</code> - Lists data paths</li> <li><code>OutdatedDataPathScannerHandler</code> - Scans for outdated data paths</li> <li><code>RemoveDataPathsHandler</code> - Removes data paths</li> </ul>"},{"location":"#demand-execution","title":"Demand Execution","text":"<ul> <li><code>PrepareDemandScaffoldingHandler</code> - Prepares scaffolding for demand execution</li> </ul>"},{"location":"#notifications","title":"Notifications","text":"<ul> <li><code>NotificationRouter</code> - Routes notifications to appropriate notifier</li> <li><code>SESNotifier</code> - Sends notifications via Amazon SES</li> <li><code>SNSNotifier</code> - Sends notifications via Amazon SNS</li> </ul>"},{"location":"#ecr","title":"ECR","text":"<ul> <li><code>ImageReplicatorHandler</code> - Handles replication of ECR images between repositories</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install aibs-informatics-aws-lambda\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from dataclasses import dataclass\nfrom aibs_informatics_core.models.base import SchemaModel\nfrom aibs_informatics_aws_lambda.common.handler import LambdaHandler\n\n@dataclass\nclass MyRequest(SchemaModel):\n    name: str\n\n@dataclass\nclass MyResponse(SchemaModel):\n    message: str\n\nclass MyHandler(LambdaHandler[MyRequest, MyResponse]):\n    def handle(self, request: MyRequest) -&gt; MyResponse:\n        return MyResponse(message=f\"Hello, {request.name}!\")\n\n# Create handler function for AWS Lambda\nhandler = MyHandler.get_handler()\n</code></pre>"},{"location":"#cli-invocation","title":"CLI Invocation","text":"<pre><code>handle-lambda-request \\\n    --handler-qualified-name aibs_informatics_aws_lambda.handlers.data_sync.operations.GetJSONFromFileHandler \\\n    --payload '{\"path\": \"/path/to/file.json\"}' \\\n    --response-location /tmp/response.json\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Any and all PRs are welcome. Please see CONTRIBUTING.md for more information.</p>"},{"location":"#license","title":"License","text":"<p>This software is licensed under the Allen Institute Software License. For more information, please visit Allen Institute Terms of Use.</p>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the AIBS Informatics AWS Lambda API Reference. This section provides detailed documentation for all modules, classes, and functions in the library.</p>"},{"location":"api/#module-overview","title":"Module Overview","text":""},{"location":"api/#core-modules","title":"Core Modules","text":"Module Description Main CLI entry point and main execution logic Common Base classes, utilities, and shared components"},{"location":"api/#handlers","title":"Handlers","text":"Module Description Batch AWS Batch job definition and execution handlers Data Sync Data synchronization and file system handlers Demand Demand execution scaffolding handlers ECR ECR image replication handlers Notifications Notification routing and delivery handlers"},{"location":"api/#quick-links","title":"Quick Links","text":""},{"location":"api/#base-classes","title":"Base Classes","text":"<ul> <li><code>LambdaHandler</code> - Base class for all Lambda handlers</li> <li><code>ApiLambdaHandler</code> - Base class for API Gateway handlers</li> <li><code>ApiResolverBuilder</code> - Utility for building API resolvers</li> </ul>"},{"location":"api/#common-handlers","title":"Common Handlers","text":"<ul> <li><code>GetJSONFromFileHandler</code> - Read JSON from file</li> <li><code>PutJSONToFileHandler</code> - Write JSON to file</li> <li><code>DataSyncHandler</code> - Synchronize data</li> <li><code>NotificationRouter</code> - Route notifications</li> </ul>"},{"location":"api/main/","title":"Main","text":"<p>CLI entry point and main execution logic for the Lambda handler.</p>"},{"location":"api/main/#aibs_informatics_aws_lambda.main.handle","title":"handle","text":"<pre><code>handle(event, context)\n</code></pre> <p>Route and execute a Lambda handler function invocation.</p> <p>This function acts as a router that deserializes the incoming event, loads the appropriate handler, and invokes it with the event payload.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>LambdaEvent</code> <p>The Lambda event containing the handler reference and payload. Must be a dictionary with handler and event fields.</p> required <code>context</code> <code>LambdaContext</code> <p>The AWS Lambda context object providing runtime information.</p> required <p>Returns:</p> Type Description <code>Optional[JSON]</code> <p>The response from the invoked handler, or None if no response.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the event is not a dictionary type.</p> Source code in <code>src/aibs_informatics_aws_lambda/main.py</code> <pre><code>@logger.inject_lambda_context(log_event=True)\ndef handle(event: LambdaEvent, context: LambdaContext) -&gt; Optional[JSON]:\n    \"\"\"Route and execute a Lambda handler function invocation.\n\n    This function acts as a router that deserializes the incoming event,\n    loads the appropriate handler, and invokes it with the event payload.\n\n    Args:\n        event: The Lambda event containing the handler reference and payload.\n            Must be a dictionary with handler and event fields.\n        context: The AWS Lambda context object providing runtime information.\n\n    Returns:\n        The response from the invoked handler, or None if no response.\n\n    Raises:\n        ValueError: If the event is not a dictionary type.\n    \"\"\"\n    logger.info(\"Parsing event\")\n    if not isinstance(event, dict):\n        raise ValueError(\"Unable to parse event - events must be Dict type\")\n    request = LambdaHandlerRequest.from_dict(event)\n    logger.info(f\"Successfully deserialized request. Loading handler from '{request.handler}'\")\n    target_handler = request.handler\n    logger.info(\"Successfully loaded handler. Invoking..\")\n    response = target_handler(request.event, context)\n    logger.info(\"Invocation is complete. Returning result\")\n    return response\n</code></pre>"},{"location":"api/main/#aibs_informatics_aws_lambda.main.handle_cli","title":"handle_cli","text":"<pre><code>handle_cli(args=None)\n</code></pre> <p>Execute a Lambda handler from the command line interface.</p> <p>Parses command line arguments to determine the handler to invoke, the event payload, and optionally where to store the response. Supports loading payloads from JSON strings, local files, or S3.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Optional[Sequence[str]]</code> <p>Optional sequence of command line arguments. If None, uses sys.argv.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If handler or payload cannot be resolved from arguments or environment variables.</p> <code>ValueError</code> <p>If response location is specified but invalid.</p> Example <pre><code>handle_cli([\n    '--handler', 'my_module.MyHandler',\n    '--payload', '{\"key\": \"value\"}',\n    '--response-location', '/tmp/response.json'\n])\n</code></pre> Source code in <code>src/aibs_informatics_aws_lambda/main.py</code> <pre><code>def handle_cli(args: Optional[Sequence[str]] = None):\n    \"\"\"Execute a Lambda handler from the command line interface.\n\n    Parses command line arguments to determine the handler to invoke,\n    the event payload, and optionally where to store the response.\n    Supports loading payloads from JSON strings, local files, or S3.\n\n    Args:\n        args: Optional sequence of command line arguments. If None,\n            uses sys.argv.\n\n    Raises:\n        ValueError: If handler or payload cannot be resolved from\n            arguments or environment variables.\n        ValueError: If response location is specified but invalid.\n\n    Example:\n        ```python\n        handle_cli([\n            '--handler', 'my_module.MyHandler',\n            '--payload', '{\"key\": \"value\"}',\n            '--response-location', '/tmp/response.json'\n        ])\n        ```\n    \"\"\"\n    logger.info(\"Processing request from CLI\")\n\n    parser = argparse.ArgumentParser(description=\"CLI AWS Lambda Handler\")\n    parser.add_argument(\n        \"--handler-qualified-name\",\n        \"--handler-name\",\n        \"--handler\",\n        dest=\"handler_qualified_name\",\n        required=False,\n        default=get_env_var(\n            AWS_LAMBDA_FUNCTION_HANDLER_KEY, AWS_LAMBDA_FUNCTION_HANDLER_STANDARD_KEY\n        ),\n        help=(\n            f\"handler function qualified name. If not provided, will try to load from \"\n            f\"{(AWS_LAMBDA_FUNCTION_HANDLER_KEY, AWS_LAMBDA_FUNCTION_HANDLER_STANDARD_KEY)} \"\n            \"env variables\"\n        ),\n    )\n    parser.add_argument(\n        \"--payload\",\n        \"--event\",\n        \"-e\",\n        required=False,\n        default=get_env_var(AWS_LAMBDA_EVENT_PAYLOAD_KEY),\n        help=(\n            f\"event payload of function. If not provided, will try to load from \"\n            f\"{AWS_LAMBDA_EVENT_PAYLOAD_KEY} env variable\"\n        ),\n    )\n    parser.add_argument(\n        \"--response-location\",\n        \"-o\",\n        dest=\"response_location\",\n        required=False,\n        default=get_env_var(AWS_LAMBDA_EVENT_RESPONSE_LOCATION_KEY),\n        help=(\n            f\"optional response location to store response at. can be S3 or local file. If not \"\n            f\"provided, will load from {AWS_LAMBDA_EVENT_RESPONSE_LOCATION_KEY} env variable.\"\n        ),\n    )\n\n    logger.info(\"Parsing arguments\")\n\n    parsed_args = parser.parse_args(args=args)\n\n    if parsed_args.handler_qualified_name is None:\n        raise ValueError(\"Handler could not be resolved from argument or environment variable\")\n    target_handler = deserialize_handler(parsed_args.handler_qualified_name)\n\n    if parsed_args.payload is None:\n        raise ValueError(\"Payload could not be resolved by argument or environment variable\")\n    if S3URI.is_valid(parsed_args.payload):\n        logger.info(\"Payload is an S3 path. downloading to JSON\")\n        event = download_to_json_object(S3URI(parsed_args.payload))\n    else:\n        event = load_json_object(parsed_args.payload)\n\n    logger.info(\"Successfully loaded handler. Invoking..\")\n    response = target_handler(event, DefaultLambdaContext())\n    logger.info(\"Invocation complete.\")\n\n    response_location = parsed_args.response_location\n    if response_location:\n        response = response or {}\n        logger.info(f\"Response location specified: {response_location}\")\n        if S3URI.is_valid(response_location):\n            logger.info(\"Uploading result to S3\")\n            upload_json(response, S3URI(response_location))\n        elif not os.path.isdir(response_location) and os.access(\n            os.path.dirname(response_location), os.W_OK\n        ):\n            logger.info(\"Writing result locally\")\n            with open(response_location, \"w\") as f:\n                json.dump(response, f, sort_keys=True)\n        else:\n            raise ValueError(\n                f\"Response location specified {response_location}, but not a valid s3/local path\"\n            )\n    else:\n        logger.info(\"Response location NOT specified. Response not saved.\")\n\n    logger.info(\"Handler execution complete.\")\n</code></pre>"},{"location":"api/common/","title":"Common","text":"<p>The <code>common</code> module provides base classes and utilities for creating Lambda handlers.</p>"},{"location":"api/common/#overview","title":"Overview","text":"Module Description Handler <code>LambdaHandler</code> base class Base Base utilities and helpers Logging Logging configuration and utilities Metrics Metrics collection utilities Models Common data models API API Gateway handler utilities"},{"location":"api/common/#quick-start","title":"Quick Start","text":"<pre><code>from aibs_informatics_aws_lambda.common.handler import LambdaHandler\n\nclass MyHandler(LambdaHandler):\n    def handle(self, event, context):\n        return {\"status\": \"success\"}\n\n# Export handler for AWS Lambda\nhandler = MyHandler().get_handler()\n</code></pre>"},{"location":"api/common/base/","title":"Base","text":"<p>Base utilities and helpers for Lambda handlers.</p>"},{"location":"api/common/base/#aibs_informatics_aws_lambda.common.base.HandlerMixins","title":"HandlerMixins","text":"<p>Mixin class providing common handler utilities.</p> <p>Provides access to the Lambda context and handler/service name utilities that are shared across all Lambda handlers.</p>"},{"location":"api/common/base/#aibs_informatics_aws_lambda.common.base.HandlerMixins.context","title":"context  <code>property</code> <code>writable</code>","text":"<pre><code>context\n</code></pre> <p>Get the Lambda context for the current invocation.</p> <p>Returns:</p> Type Description <code>LambdaContext</code> <p>The AWS Lambda context object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If context has not been set.</p>"},{"location":"api/common/base/#aibs_informatics_aws_lambda.common.base.HandlerMixins.handler_name","title":"handler_name  <code>classmethod</code>","text":"<pre><code>handler_name()\n</code></pre> <p>Get the name of this handler class.</p> <p>Returns:</p> Type Description <code>str</code> <p>The class name as a string.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/base.py</code> <pre><code>@classmethod\ndef handler_name(cls) -&gt; str:\n    \"\"\"Get the name of this handler class.\n\n    Returns:\n        The class name as a string.\n    \"\"\"\n    return cls.__name__\n</code></pre>"},{"location":"api/common/base/#aibs_informatics_aws_lambda.common.base.HandlerMixins.service_name","title":"service_name  <code>classmethod</code>","text":"<pre><code>service_name()\n</code></pre> <p>Get the service name for logging and metrics.</p> <p>Returns:</p> Type Description <code>str</code> <p>The class name as the service identifier.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/base.py</code> <pre><code>@classmethod\ndef service_name(cls) -&gt; str:\n    \"\"\"Get the service name for logging and metrics.\n\n    Returns:\n        The class name as the service identifier.\n    \"\"\"\n    return cls.__name__\n</code></pre>"},{"location":"api/common/handler/","title":"Lambda Handler","text":"<p>Base class for creating strongly typed Lambda functions with serialization, logging, and metrics support.</p>"},{"location":"api/common/handler/#aibs_informatics_aws_lambda.common.handler.LambdaHandler","title":"LambdaHandler  <code>dataclass</code>","text":"<pre><code>LambdaHandler()\n</code></pre> <p>               Bases: <code>LoggingMixins</code>, <code>MetricsMixins</code>, <code>HandlerMixins</code>, <code>BaseExecutor[REQUEST, RESPONSE]</code>, <code>Generic[REQUEST, RESPONSE]</code></p> <p>Base class for creating strongly-typed AWS Lambda handlers.</p> <p>Provides a foundation for Lambda functions with built-in support for: - Request/response serialization and deserialization - Structured logging via AWS Lambda Powertools - CloudWatch metrics collection - SQS batch processing - DynamoDB Streams processing</p> <p>Inherit from the LambdaHandler class to create a custom strongly typed lambda handler that expects a REQUEST object and returns a RESPONSE object that follow the <code>ModelProtocol</code>.</p> <p>          Class Type Parameters:        </p> Name Bound or Constraints Description Default <code>REQUEST</code> <p>The request model type (must implement ModelProtocol).</p> required <code>RESPONSE</code> <p>The response model type (must implement ModelProtocol).</p> required Example <pre><code>@dataclass\nclass MyRequest(SchemaModel):\n    name: str\n\n@dataclass\nclass MyResponse(SchemaModel):\n    message: str\n\nclass MyHandler(LambdaHandler[MyRequest, MyResponse]):\n    def handle(self, request: MyRequest) -&gt; MyResponse:\n        return MyResponse(message=f\"Hello, {request.name}!\")\n\nhandler = MyHandler.get_handler()\n</code></pre>"},{"location":"api/common/handler/#aibs_informatics_aws_lambda.common.handler.LambdaHandler.deserialize_dynamodb_record","title":"deserialize_dynamodb_record  <code>classmethod</code>","text":"<pre><code>deserialize_dynamodb_record(record)\n</code></pre> <p>Parse a DynamoDB record into a request object.</p> <p>This should be implemented if expected to process a dynamo DB record</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>DynamoDBRecord</code> <p>A DynamoDB record generated from a DynamoDB Stream</p> required <p>Returns:</p> Type Description <code>REQUEST</code> <p>Expected Request object</p> Source code in <code>src/aibs_informatics_aws_lambda/common/handler.py</code> <pre><code>@classmethod\ndef deserialize_dynamodb_record(cls, record: DynamoDBRecord) -&gt; REQUEST:\n    \"\"\"Parse a DynamoDB record into a request object.\n\n    This should be implemented if expected to process a dynamo DB record\n\n    Args:\n        record (DynamoDBRecord): A DynamoDB record generated from a DynamoDB Stream\n\n    Returns:\n        Expected Request object\n    \"\"\"\n    raise NotImplementedError(  # pragma: no cover\n        \"You must implement this method if processing dynamoDB stream events\"\n    )\n</code></pre>"},{"location":"api/common/handler/#aibs_informatics_aws_lambda.common.handler.LambdaHandler.deserialize_sqs_record","title":"deserialize_sqs_record  <code>classmethod</code>","text":"<pre><code>deserialize_sqs_record(record)\n</code></pre> <p>Deserialize an SQS Record into the Request object of this handler</p> <p>By default, the \"body\" of the SQS record is deserialized using the class default <code>deserialize_request</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>SQSRecord</code> <p>An SQS record</p> required <p>Returns:</p> Type Description <code>REQUEST</code> <p>The expected Request object for this handler class</p> Source code in <code>src/aibs_informatics_aws_lambda/common/handler.py</code> <pre><code>@classmethod\ndef deserialize_sqs_record(cls, record: SQSRecord) -&gt; REQUEST:\n    \"\"\"Deserialize an SQS Record into the Request object of this handler\n\n    By default, the \"body\" of the SQS record is deserialized using\n    the class default `deserialize_request` method.\n\n    Args:\n        record (SQSRecord): An SQS record\n\n    Returns:\n        The expected Request object for this handler class\n    \"\"\"\n    return cls.deserialize_request(json.loads(record[\"body\"]))\n</code></pre>"},{"location":"api/common/handler/#aibs_informatics_aws_lambda.common.handler.LambdaHandler.get_dynamodb_stream_handler","title":"get_dynamodb_stream_handler  <code>classmethod</code>","text":"<pre><code>get_dynamodb_stream_handler(*args, **kwargs)\n</code></pre> <p>Create a handler for processing DynamoDB Stream events.</p> <p>Creates a Lambda handler that processes batches of DynamoDB Stream records with partial failure support.</p> See Also <p>https://docs.powertools.aws.dev/lambda/python/latest/utilities/batch/</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments passed to the handler constructor.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to the handler constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LambdaHandlerType</code> <p>A callable Lambda handler function for DynamoDB Streams.</p> Example <pre><code>handler = MyHandler.get_dynamodb_stream_handler()\n</code></pre> Source code in <code>src/aibs_informatics_aws_lambda/common/handler.py</code> <pre><code>@classmethod\ndef get_dynamodb_stream_handler(cls, *args, **kwargs) -&gt; LambdaHandlerType:\n    \"\"\"Create a handler for processing DynamoDB Stream events.\n\n    Creates a Lambda handler that processes batches of DynamoDB Stream\n    records with partial failure support.\n\n    See Also:\n        https://docs.powertools.aws.dev/lambda/python/latest/utilities/batch/\n\n    Args:\n        *args: Positional arguments passed to the handler constructor.\n        **kwargs: Keyword arguments passed to the handler constructor.\n\n    Returns:\n        A callable Lambda handler function for DynamoDB Streams.\n\n    Example:\n        ```python\n        handler = MyHandler.get_dynamodb_stream_handler()\n        ```\n    \"\"\"\n    processor = BatchProcessor(event_type=EventType.DynamoDBStreams)\n    logger = cls.get_logger(cls.service_name())\n\n    # Create a record handler for each record in batch.\n    def record_handler(record: DynamoDBRecord) -&gt; Optional[JSON]:\n        if not cls.should_process_dynamodb_record(record):\n            logger.info(f\"DynamoDB record {record} will not be processed.\")\n            return None\n        lambda_handler = cls(*args, **kwargs)  # type: ignore[call-arg]\n        lambda_handler.log = logger\n        lambda_handler.add_logger_to_root()\n\n        request = lambda_handler.deserialize_dynamodb_record(record)\n        response = lambda_handler.handle(request=request)\n        if response:\n            lambda_handler.log.info(\"Sending Response\")\n            return lambda_handler.serialize_response(response)\n        lambda_handler.log.info(\"Not sending Response\")\n        return None\n\n    # Now create top-level handler\n    @logger.inject_lambda_context(log_event=True)\n    @batch_processor(record_handler=record_handler, processor=processor)  # type: ignore\n    def handler(event, context: LambdaContext):\n        return processor.response()\n\n    return handler  # type: ignore\n</code></pre>"},{"location":"api/common/handler/#aibs_informatics_aws_lambda.common.handler.LambdaHandler.get_handler","title":"get_handler  <code>classmethod</code>","text":"<pre><code>get_handler(*args, **kwargs)\n</code></pre> <p>Create a Lambda handler function for this handler class.</p> <p>Creates a wrapped handler function that: - Injects Lambda context for logging - Instantiates the handler class - Deserializes the incoming event - Invokes the handle method - Serializes and returns the response</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments passed to the handler constructor.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to the handler constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LambdaHandlerType</code> <p>A callable Lambda handler function suitable for AWS Lambda.</p> Example <pre><code># In your Lambda module\nhandler = MyHandler.get_handler()\n</code></pre> Source code in <code>src/aibs_informatics_aws_lambda/common/handler.py</code> <pre><code>@classmethod\ndef get_handler(cls, *args, **kwargs) -&gt; LambdaHandlerType:\n    \"\"\"Create a Lambda handler function for this handler class.\n\n    Creates a wrapped handler function that:\n    - Injects Lambda context for logging\n    - Instantiates the handler class\n    - Deserializes the incoming event\n    - Invokes the handle method\n    - Serializes and returns the response\n\n    Args:\n        *args: Positional arguments passed to the handler constructor.\n        **kwargs: Keyword arguments passed to the handler constructor.\n\n    Returns:\n        A callable Lambda handler function suitable for AWS Lambda.\n\n    Example:\n        ```python\n        # In your Lambda module\n        handler = MyHandler.get_handler()\n        ```\n    \"\"\"\n\n    logger = cls.get_logger(service=cls.service_name(), add_to_root=False)\n\n    @logger.inject_lambda_context(log_event=True)\n    def handler(event: LambdaEvent, context: LambdaContext) -&gt; Optional[JSON]:\n        lambda_handler = cls(*args, **kwargs)  # type: ignore[call-arg]\n        logger.info(f\"Instantiated {lambda_handler}.\")\n        lambda_handler.log = logger\n        lambda_handler.context = context\n        lambda_handler.add_logger_to_root()\n\n        lambda_handler.log.info(f\"Deserializing event: {event}\")\n\n        request = lambda_handler.deserialize_request(event)\n\n        lambda_handler.log.info(\"Event successfully deserialized. Calling handler...\")\n        response = lambda_handler.handle(request=request)\n\n        lambda_handler.log.info(\n            f\"Handler completed and returned following response: {response}\"\n        )\n        if response:\n            lambda_handler.log.info(\"Serializing response\")\n            return lambda_handler.serialize_response(response)\n\n        return None\n\n    return handler\n</code></pre>"},{"location":"api/common/handler/#aibs_informatics_aws_lambda.common.handler.LambdaHandler.get_sqs_batch_handler","title":"get_sqs_batch_handler  <code>classmethod</code>","text":"<pre><code>get_sqs_batch_handler(\n    *args, queue_type=\"standard\", **kwargs\n)\n</code></pre> <p>Create a handler for processing SQS batch records.</p> <p>Creates a Lambda handler that processes batches of SQS messages with partial failure support, allowing successful messages to be acknowledged even if some fail.</p> See Also <p>https://docs.powertools.aws.dev/lambda/python/latest/utilities/batch/</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments passed to the handler constructor.</p> <code>()</code> <code>queue_type</code> <code>Literal['standard', 'fifo']</code> <p>The SQS queue type - \"standard\" or \"fifo\". Defaults to \"standard\".</p> <code>'standard'</code> <code>**kwargs</code> <p>Keyword arguments passed to the handler constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LambdaHandlerType</code> <p>A callable Lambda handler function for SQS batch processing.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If an invalid queue_type is provided.</p> Example <pre><code>handler = MyHandler.get_sqs_batch_handler(queue_type=\"fifo\")\n</code></pre> Source code in <code>src/aibs_informatics_aws_lambda/common/handler.py</code> <pre><code>@classmethod\ndef get_sqs_batch_handler(\n    cls, *args, queue_type: Literal[\"standard\", \"fifo\"] = \"standard\", **kwargs\n) -&gt; LambdaHandlerType:\n    \"\"\"Create a handler for processing SQS batch records.\n\n    Creates a Lambda handler that processes batches of SQS messages\n    with partial failure support, allowing successful messages to be\n    acknowledged even if some fail.\n\n    See Also:\n        https://docs.powertools.aws.dev/lambda/python/latest/utilities/batch/\n\n    Args:\n        *args: Positional arguments passed to the handler constructor.\n        queue_type (Literal[\"standard\", \"fifo\"]): The SQS queue type - \"standard\" or \"fifo\".\n            Defaults to \"standard\".\n        **kwargs: Keyword arguments passed to the handler constructor.\n\n    Returns:\n        A callable Lambda handler function for SQS batch processing.\n\n    Raises:\n        RuntimeError: If an invalid queue_type is provided.\n\n    Example:\n        ```python\n        handler = MyHandler.get_sqs_batch_handler(queue_type=\"fifo\")\n        ```\n    \"\"\"\n    if queue_type == \"standard\":\n        processor = BatchProcessor(event_type=EventType.SQS)\n    elif queue_type == \"fifo\":\n        processor = SqsFifoPartialProcessor()\n    else:\n        raise RuntimeError(\n            \"An invalid SQS queue_type ({queue_type}) was provided to the \"\n            \"get_sqs_batch_handler() method. Valid values include: \"\n            \"[standard, fifo]\"\n        )\n    logger = cls.get_logger(cls.service_name())\n\n    # Create a record handler for each record in batch.\n    def record_handler(record: SQSRecord) -&gt; Optional[JSON]:\n        if not cls.should_process_sqs_record(record):\n            logger.info(f\"SQS record {record} elected not to be processed.\")\n            return None\n        lambda_handler = cls(*args, **kwargs)\n        lambda_handler.log = logger\n        lambda_handler.add_logger_to_root()\n\n        request = lambda_handler.deserialize_sqs_record(record)\n        response = lambda_handler.handle(request=request)\n        if response:\n            lambda_handler.log.info(\"Sending Response\")\n            return lambda_handler.serialize_response(response)\n        lambda_handler.log.info(\"Not sending Response\")\n        return None\n\n    # Now create top-level handler\n    @logger.inject_lambda_context(log_event=True)\n    def handler(event: dict, context: LambdaContext) -&gt; PartialItemFailureResponse:\n        return process_partial_response(\n            event=event,\n            record_handler=record_handler,\n            processor=processor,\n            context=context,\n        )\n\n    return cast(LambdaHandlerType, handler)\n</code></pre>"},{"location":"api/common/handler/#aibs_informatics_aws_lambda.common.handler.LambdaHandler.load_input__remote","title":"load_input__remote  <code>classmethod</code>","text":"<pre><code>load_input__remote(remote_path)\n</code></pre> <p>Load input data from a remote S3 location.</p> <p>Parameters:</p> Name Type Description Default <code>remote_path</code> <code>S3URI</code> <p>The S3 URI to download the input from.</p> required <p>Returns:</p> Type Description <code>JSON</code> <p>The JSON content from the S3 object.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/handler.py</code> <pre><code>@classmethod\ndef load_input__remote(cls, remote_path: S3URI) -&gt; JSON:\n    \"\"\"Load input data from a remote S3 location.\n\n    Args:\n        remote_path (S3URI): The S3 URI to download the input from.\n\n    Returns:\n        The JSON content from the S3 object.\n    \"\"\"\n    return download_to_json_object(remote_path)\n</code></pre>"},{"location":"api/common/handler/#aibs_informatics_aws_lambda.common.handler.LambdaHandler.should_process_dynamodb_record","title":"should_process_dynamodb_record  <code>classmethod</code>","text":"<pre><code>should_process_dynamodb_record(record)\n</code></pre> <p>Filter for whether to handle an DynamoDB record</p> This allows to filter all stream events based on <ol> <li>The type of event (entry modification, insertion, deletion...)</li> <li>The content of the affected record.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>DynamoDBRecord</code> <p>A DynamoDB record generated from a DynamoDB Stream</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if handler should process request</p> Source code in <code>src/aibs_informatics_aws_lambda/common/handler.py</code> <pre><code>@classmethod\ndef should_process_dynamodb_record(cls, record: DynamoDBRecord) -&gt; bool:\n    \"\"\"Filter for whether to handle an DynamoDB record\n\n    This allows to filter all stream events based on:\n        1. The type of event (entry modification, insertion, deletion...)\n        2. The content of the affected record.\n\n    Args:\n        record (DynamoDBRecord): A DynamoDB record generated from a DynamoDB Stream\n\n    Returns:\n        True if handler should process request\n    \"\"\"\n    return True\n</code></pre>"},{"location":"api/common/handler/#aibs_informatics_aws_lambda.common.handler.LambdaHandler.should_process_sqs_record","title":"should_process_sqs_record  <code>classmethod</code>","text":"<pre><code>should_process_sqs_record(record)\n</code></pre> <p>Filter for whether to handle an SQS Record.</p> <p>This is invoked prior to deserializing and handling that SQS message.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>SQSRecord</code> <p>An SQS record</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if handler should process request</p> Source code in <code>src/aibs_informatics_aws_lambda/common/handler.py</code> <pre><code>@classmethod\ndef should_process_sqs_record(cls, record: SQSRecord) -&gt; bool:\n    \"\"\"Filter for whether to handle an SQS Record.\n\n    This is invoked prior to deserializing and handling that SQS message.\n\n    Args:\n        record (SQSRecord): An SQS record\n\n    Returns:\n        True if handler should process request\n    \"\"\"\n    return True\n</code></pre>"},{"location":"api/common/handler/#aibs_informatics_aws_lambda.common.handler.LambdaHandler.write_output__remote","title":"write_output__remote  <code>classmethod</code>","text":"<pre><code>write_output__remote(output, remote_path)\n</code></pre> <p>Write output data to a remote S3 location.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>JSON</code> <p>The JSON content to upload.</p> required <code>remote_path</code> <code>S3URI</code> <p>The S3 URI to upload the output to.</p> required Source code in <code>src/aibs_informatics_aws_lambda/common/handler.py</code> <pre><code>@classmethod\ndef write_output__remote(cls, output: JSON, remote_path: S3URI) -&gt; None:\n    \"\"\"Write output data to a remote S3 location.\n\n    Args:\n        output (JSON): The JSON content to upload.\n        remote_path (S3URI): The S3 URI to upload the output to.\n    \"\"\"\n    return upload_json(output, remote_path)\n</code></pre>"},{"location":"api/common/logging/","title":"Logging","text":"<p>Logging configuration and utilities for Lambda handlers.</p> <p>Logging utilities for AWS Lambda handlers.</p> <p>Provides logging mixins and helper functions for configuring structured logging with AWS Lambda Powertools.</p>"},{"location":"api/common/logging/#aibs_informatics_aws_lambda.common.logging.LoggingMixins","title":"LoggingMixins","text":"<p>               Bases: <code>HandlerMixins</code></p> <p>Mixin class providing structured logging capabilities.</p> <p>Integrates AWS Lambda Powertools Logger for structured JSON logging with automatic context injection and correlation IDs.</p> <p>Attributes:</p> Name Type Description <code>log</code> <code>Logger</code> <p>Alias for the logger property.</p> <code>logger</code> <code>Logger</code> <p>The AWS Lambda Powertools Logger instance.</p>"},{"location":"api/common/logging/#aibs_informatics_aws_lambda.common.logging.LoggingMixins.log","title":"log  <code>property</code> <code>writable</code>","text":"<pre><code>log\n</code></pre> <p>Alias for the logger property.</p> <p>Returns:</p> Type Description <code>Logger</code> <p>The configured Logger instance.</p>"},{"location":"api/common/logging/#aibs_informatics_aws_lambda.common.logging.LoggingMixins.logger","title":"logger  <code>property</code> <code>writable</code>","text":"<pre><code>logger\n</code></pre> <p>Get the Logger instance, creating one if needed.</p> <p>Returns:</p> Type Description <code>Logger</code> <p>The configured Logger instance for this handler.</p>"},{"location":"api/common/logging/#aibs_informatics_aws_lambda.common.logging.LoggingMixins.add_logger_to_root","title":"add_logger_to_root","text":"<pre><code>add_logger_to_root()\n</code></pre> <p>Add this handler's logger to the root logger.</p> <p>Ensures log messages from other modules are captured with the same structured format.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/logging.py</code> <pre><code>def add_logger_to_root(self):\n    \"\"\"Add this handler's logger to the root logger.\n\n    Ensures log messages from other modules are captured with\n    the same structured format.\n    \"\"\"\n    add_handler_to_logger(self.logger, None)\n</code></pre>"},{"location":"api/common/logging/#aibs_informatics_aws_lambda.common.logging.LoggingMixins.get_logger","title":"get_logger  <code>classmethod</code>","text":"<pre><code>get_logger(service=None, add_to_root=False)\n</code></pre> <p>Create a new Logger instance.</p> <p>Parameters:</p> Name Type Description Default <code>service</code> <code>Optional[str]</code> <p>The service name for the logger. If None, uses default.</p> <code>None</code> <code>add_to_root</code> <code>bool</code> <p>Whether to add the logger handler to the root logger.</p> <code>False</code> <p>Returns:</p> Type Description <code>Logger</code> <p>A configured Logger instance.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/logging.py</code> <pre><code>@classmethod\ndef get_logger(cls, service: Optional[str] = None, add_to_root: bool = False) -&gt; Logger:\n    \"\"\"Create a new Logger instance.\n\n    Args:\n        service (Optional[str]): The service name for the logger. If None, uses default.\n        add_to_root (bool): Whether to add the logger handler to the root logger.\n\n    Returns:\n        A configured Logger instance.\n    \"\"\"\n    return get_service_logger(service=service, add_to_root=add_to_root)\n</code></pre>"},{"location":"api/common/logging/#aibs_informatics_aws_lambda.common.logging.add_handler_to_logger","title":"add_handler_to_logger","text":"<pre><code>add_handler_to_logger(source_logger, target_logger=None)\n</code></pre> <p>Add a source logger's handler to a target logger.</p> <p>Copies the handler from the source logger to the target logger, ensuring consistent log formatting across loggers.</p> <p>Parameters:</p> Name Type Description Default <code>source_logger</code> <code>Logger</code> <p>The Logger whose handler will be copied.</p> required <code>target_logger</code> <code>Union[str, Logger, None]</code> <p>The target logger to receive the handler. Can be a logger name string, a Logger instance, or None for the root logger.</p> <code>None</code> Source code in <code>src/aibs_informatics_aws_lambda/common/logging.py</code> <pre><code>def add_handler_to_logger(\n    source_logger: Logger, target_logger: Union[str, logging.Logger, None] = None\n):\n    \"\"\"Add a source logger's handler to a target logger.\n\n    Copies the handler from the source logger to the target logger,\n    ensuring consistent log formatting across loggers.\n\n    Args:\n        source_logger (Logger): The Logger whose handler will be copied.\n        target_logger (Union[str, logging.Logger, None]): The target logger to receive the handler.\n            Can be a logger name string, a Logger instance, or None\n            for the root logger.\n    \"\"\"\n    handler = source_logger.registered_handler\n\n    if target_logger is None or isinstance(target_logger, str):\n        target_logger = logging.getLogger(target_logger)\n        log_level = min(source_logger.log_level, target_logger.getEffectiveLevel())\n        target_logger.setLevel(log_level)\n    target_logger_handlers = get_all_handlers(target_logger)\n\n    # TODO: This is not avoiding duplicate handlers.\n    # we need to have better comparison logic\n    if handler not in target_logger_handlers:\n        target_logger.addHandler(handler)\n</code></pre>"},{"location":"api/common/logging/#aibs_informatics_aws_lambda.common.logging.get_service_logger","title":"get_service_logger","text":"<pre><code>get_service_logger(\n    service=None, child=False, add_to_root=False\n)\n</code></pre> <p>Create a service logger with optional root logger integration.</p> <p>Parameters:</p> Name Type Description Default <code>service</code> <code>Optional[str]</code> <p>The service name for the logger. If None, uses default.</p> <code>None</code> <code>child</code> <code>bool</code> <p>Whether to create a child logger.</p> <code>False</code> <code>add_to_root</code> <code>bool</code> <p>Whether to add the logger handler to the root logger.</p> <code>False</code> <p>Returns:</p> Type Description <code>Logger</code> <p>A configured Logger instance for the service.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/logging.py</code> <pre><code>def get_service_logger(\n    service: Optional[str] = None, child: bool = False, add_to_root: bool = False\n) -&gt; Logger:\n    \"\"\"Create a service logger with optional root logger integration.\n\n    Args:\n        service (Optional[str]): The service name for the logger. If None, uses default.\n        child (bool): Whether to create a child logger.\n        add_to_root (bool): Whether to add the logger handler to the root logger.\n\n    Returns:\n        A configured Logger instance for the service.\n    \"\"\"\n    service_logger = Logger(service=service, child=child)\n    if add_to_root:\n        add_handler_to_logger(service_logger)\n    return service_logger\n</code></pre>"},{"location":"api/common/metrics/","title":"Metrics","text":"<p>Metrics collection utilities for Lambda handlers.</p> <p>Metrics utilities for AWS Lambda handlers.</p> <p>Provides utilities for collecting and publishing CloudWatch metrics using AWS Lambda Powertools.</p>"},{"location":"api/common/metrics/#aibs_informatics_aws_lambda.common.metrics.EnhancedMetrics","title":"EnhancedMetrics","text":"<p>               Bases: <code>Metrics</code></p> <p>Extended Metrics class with convenience methods.</p> <p>Provides additional helper methods for common metric patterns like counting, duration tracking, and success/failure recording.</p>"},{"location":"api/common/metrics/#aibs_informatics_aws_lambda.common.metrics.EnhancedMetrics.add_count_metric","title":"add_count_metric","text":"<pre><code>add_count_metric(name, value)\n</code></pre> <p>Add a count metric.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The metric name.</p> required <code>value</code> <code>float</code> <p>The count value.</p> required Source code in <code>src/aibs_informatics_aws_lambda/common/metrics.py</code> <pre><code>def add_count_metric(self, name: str, value: float):\n    \"\"\"Add a count metric.\n\n    Args:\n        name (str): The metric name.\n        value (float): The count value.\n    \"\"\"\n    self.add_metric(name=name, unit=MetricUnit.Count, value=value)\n</code></pre>"},{"location":"api/common/metrics/#aibs_informatics_aws_lambda.common.metrics.EnhancedMetrics.add_duration_metric","title":"add_duration_metric","text":"<pre><code>add_duration_metric(start=None, end=None, name='')\n</code></pre> <p>Add a duration metric.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Optional[datetime]</code> <p>The start timestamp. Defaults to module load time.</p> <code>None</code> <code>end</code> <code>Optional[datetime]</code> <p>The end timestamp. Defaults to current time.</p> <code>None</code> <code>name</code> <code>str</code> <p>Prefix for the metric name.</p> <code>''</code> Source code in <code>src/aibs_informatics_aws_lambda/common/metrics.py</code> <pre><code>def add_duration_metric(\n    self, start: Optional[datetime] = None, end: Optional[datetime] = None, name: str = \"\"\n):\n    \"\"\"Add a duration metric.\n\n    Args:\n        start (Optional[datetime]): The start timestamp. Defaults to module load time.\n        end (Optional[datetime]): The end timestamp. Defaults to current time.\n        name (str): Prefix for the metric name.\n    \"\"\"\n    add_duration_metric(start=start, end=end, name=name, metrics=self)\n</code></pre>"},{"location":"api/common/metrics/#aibs_informatics_aws_lambda.common.metrics.EnhancedMetrics.add_failure_metric","title":"add_failure_metric","text":"<pre><code>add_failure_metric(name='')\n</code></pre> <p>Record a failed operation metric.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Prefix for the metric names.</p> <code>''</code> Source code in <code>src/aibs_informatics_aws_lambda/common/metrics.py</code> <pre><code>def add_failure_metric(self, name: str = \"\"):\n    \"\"\"Record a failed operation metric.\n\n    Args:\n        name (str): Prefix for the metric names.\n    \"\"\"\n    add_failure_metric(name=name, metrics=self)\n</code></pre>"},{"location":"api/common/metrics/#aibs_informatics_aws_lambda.common.metrics.EnhancedMetrics.add_success_metric","title":"add_success_metric","text":"<pre><code>add_success_metric(name='')\n</code></pre> <p>Record a successful operation metric.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Prefix for the metric names.</p> <code>''</code> Source code in <code>src/aibs_informatics_aws_lambda/common/metrics.py</code> <pre><code>def add_success_metric(self, name: str = \"\"):\n    \"\"\"Record a successful operation metric.\n\n    Args:\n        name (str): Prefix for the metric names.\n    \"\"\"\n    add_success_metric(name=name, metrics=self)\n</code></pre>"},{"location":"api/common/metrics/#aibs_informatics_aws_lambda.common.metrics.MetricsMixins","title":"MetricsMixins","text":"<p>               Bases: <code>HandlerMixins</code></p> <p>Mixin class providing CloudWatch metrics capabilities.</p> <p>Integrates AWS Lambda Powertools Metrics for automatic metric collection and publishing to CloudWatch.</p>"},{"location":"api/common/metrics/#aibs_informatics_aws_lambda.common.metrics.MetricsMixins.metrics","title":"metrics  <code>property</code> <code>writable</code>","text":"<pre><code>metrics\n</code></pre> <p>Get the metrics collector, creating one if needed.</p> <p>Returns:</p> Type Description <code>EnhancedMetrics</code> <p>The EnhancedMetrics instance for this handler.</p>"},{"location":"api/common/metrics/#aibs_informatics_aws_lambda.common.metrics.MetricsMixins.add_metric","title":"add_metric  <code>classmethod</code>","text":"<pre><code>add_metric(metrics, name, value, unit=MetricUnit.Count)\n</code></pre> <p>Add a metric to the collector.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Metrics</code> <p>The metrics collector.</p> required <code>name</code> <code>str</code> <p>The metric name.</p> required <code>value</code> <code>float</code> <p>The metric value.</p> required <code>unit</code> <code>MetricUnit</code> <p>The metric unit. Defaults to Count.</p> <code>Count</code> Source code in <code>src/aibs_informatics_aws_lambda/common/metrics.py</code> <pre><code>@classmethod\ndef add_metric(\n    cls,\n    metrics: Metrics,\n    name: str,\n    value: float,\n    unit: MetricUnit = MetricUnit.Count,\n):\n    \"\"\"Add a metric to the collector.\n\n    Args:\n        metrics (Metrics): The metrics collector.\n        name (str): The metric name.\n        value (float): The metric value.\n        unit (MetricUnit): The metric unit. Defaults to Count.\n    \"\"\"\n    metrics.add_metric(name=name, unit=unit, value=value)\n</code></pre>"},{"location":"api/common/metrics/#aibs_informatics_aws_lambda.common.metrics.MetricsMixins.get_metrics","title":"get_metrics  <code>classmethod</code>","text":"<pre><code>get_metrics(\n    service=None, namespace=None, **additional_dimensions\n)\n</code></pre> <p>Create a new EnhancedMetrics instance.</p> <p>Parameters:</p> Name Type Description Default <code>service</code> <code>Optional[str]</code> <p>The service name for metrics.</p> <code>None</code> <code>namespace</code> <code>Optional[str]</code> <p>The CloudWatch namespace.</p> <code>None</code> <code>**additional_dimensions</code> <code>str</code> <p>Additional metric dimensions as key-value pairs.</p> <code>{}</code> <p>Returns:</p> Type Description <code>EnhancedMetrics</code> <p>A configured EnhancedMetrics instance.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/metrics.py</code> <pre><code>@classmethod\ndef get_metrics(\n    cls,\n    service: Optional[str] = None,\n    namespace: Optional[str] = None,\n    **additional_dimensions: str,\n) -&gt; EnhancedMetrics:\n    \"\"\"Create a new EnhancedMetrics instance.\n\n    Args:\n        service (Optional[str]): The service name for metrics.\n        namespace (Optional[str]): The CloudWatch namespace.\n        **additional_dimensions (str): Additional metric dimensions as key-value pairs.\n\n    Returns:\n        A configured EnhancedMetrics instance.\n    \"\"\"\n    metrics = EnhancedMetrics(service=service, namespace=namespace)\n    for dimension_name, dimension_value in additional_dimensions.items():\n        metrics.add_dimension(name=dimension_name, value=dimension_value)\n    return metrics\n</code></pre>"},{"location":"api/common/metrics/#aibs_informatics_aws_lambda.common.metrics.add_duration_metric","title":"add_duration_metric","text":"<pre><code>add_duration_metric(\n    start=None, end=None, name=\"\", metrics=None\n)\n</code></pre> <p>Add a duration metric to the metrics collector.</p> <p>Calculates the duration between start and end times and records it as a CloudWatch metric in milliseconds.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>Optional[datetime]</code> <p>The start timestamp. Defaults to module load time.</p> <code>None</code> <code>end</code> <code>Optional[datetime]</code> <p>The end timestamp. Defaults to current time.</p> <code>None</code> <code>name</code> <code>str</code> <p>Prefix for the metric name. Final name is '{name}Duration'.</p> <code>''</code> <code>metrics</code> <code>Optional[Union[EphemeralMetrics, Metrics]]</code> <p>The metrics collector to use. Creates ephemeral if None.</p> <code>None</code> Source code in <code>src/aibs_informatics_aws_lambda/common/metrics.py</code> <pre><code>def add_duration_metric(\n    start: Optional[datetime] = None,\n    end: Optional[datetime] = None,\n    name: str = \"\",\n    metrics: Optional[Union[EphemeralMetrics, Metrics]] = None,\n):\n    \"\"\"Add a duration metric to the metrics collector.\n\n    Calculates the duration between start and end times and records\n    it as a CloudWatch metric in milliseconds.\n\n    Args:\n        start (Optional[datetime]): The start timestamp. Defaults to module load time.\n        end (Optional[datetime]): The end timestamp. Defaults to current time.\n        name (str): Prefix for the metric name. Final name is '{name}Duration'.\n        metrics (Optional[Union[EphemeralMetrics, Metrics]]): The metrics collector to use.\n            Creates ephemeral if None.\n    \"\"\"\n    start = start or DEFAULT_TIME_START\n    end = end or datetime.now(start.tzinfo)\n    duration = end - start\n    if metrics is None:\n        metrics = EphemeralMetrics()\n    metrics.add_metric(\n        name=f\"{name}Duration\", unit=MetricUnit.Milliseconds, value=duration.total_seconds() * 1000\n    )\n</code></pre>"},{"location":"api/common/metrics/#aibs_informatics_aws_lambda.common.metrics.add_failure_metric","title":"add_failure_metric","text":"<pre><code>add_failure_metric(name='', metrics=None)\n</code></pre> <p>Record a failed operation metric.</p> <p>Adds metrics indicating success (0) and failure (1) counts.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Prefix for the metric names.</p> <code>''</code> <code>metrics</code> <code>Optional[Union[EphemeralMetrics, Metrics]]</code> <p>The metrics collector to use. Creates ephemeral if None.</p> <code>None</code> Source code in <code>src/aibs_informatics_aws_lambda/common/metrics.py</code> <pre><code>def add_failure_metric(name: str = \"\", metrics: Optional[Union[EphemeralMetrics, Metrics]] = None):\n    \"\"\"Record a failed operation metric.\n\n    Adds metrics indicating success (0) and failure (1) counts.\n\n    Args:\n        name (str): Prefix for the metric names.\n        metrics (Optional[Union[EphemeralMetrics, Metrics]]): The metrics collector to use.\n            Creates ephemeral if None.\n    \"\"\"\n    if metrics is None:\n        metrics = EphemeralMetrics()\n    metrics.add_metric(name=f\"{name}Success\", unit=MetricUnit.Count, value=0)\n    metrics.add_metric(name=f\"{name}Failure\", unit=MetricUnit.Count, value=1)\n</code></pre>"},{"location":"api/common/metrics/#aibs_informatics_aws_lambda.common.metrics.add_success_metric","title":"add_success_metric","text":"<pre><code>add_success_metric(name='', metrics=None)\n</code></pre> <p>Record a successful operation metric.</p> <p>Adds metrics indicating success (1) and failure (0) counts.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Prefix for the metric names.</p> <code>''</code> <code>metrics</code> <code>Optional[Union[EphemeralMetrics, Metrics]]</code> <p>The metrics collector to use. Creates ephemeral if None.</p> <code>None</code> Source code in <code>src/aibs_informatics_aws_lambda/common/metrics.py</code> <pre><code>def add_success_metric(name: str = \"\", metrics: Optional[Union[EphemeralMetrics, Metrics]] = None):\n    \"\"\"Record a successful operation metric.\n\n    Adds metrics indicating success (1) and failure (0) counts.\n\n    Args:\n        name (str): Prefix for the metric names.\n        metrics (Optional[Union[EphemeralMetrics, Metrics]]): The metrics collector to use.\n            Creates ephemeral if None.\n    \"\"\"\n    if metrics is None:\n        metrics = EphemeralMetrics()\n    metrics.add_metric(name=f\"{name}Success\", unit=MetricUnit.Count, value=1)\n    metrics.add_metric(name=f\"{name}Failure\", unit=MetricUnit.Count, value=0)\n</code></pre>"},{"location":"api/common/models/","title":"Models","text":"<p>Common data models used across Lambda handlers.</p> <p>Data models for Lambda handler configuration and execution.</p> <p>Provides models for Lambda context, handler requests, and serialization utilities.</p>"},{"location":"api/common/models/#aibs_informatics_aws_lambda.common.models.DefaultLambdaContext","title":"DefaultLambdaContext  <code>dataclass</code>","text":"<pre><code>DefaultLambdaContext(\n    _function_name=(\n        lambda: get_env_var(AWS_LAMBDA_FUNCTION_NAME_KEY)\n        or DEFAULT_AWS_LAMBDA_FUNCTION_NAME\n    )(),\n    _function_version=(\n        lambda: get_env_var(\n            AWS_LAMBDA_FUNCTION_VERSION_KEY,\n            default_value=\"1.0\",\n        )\n    )(),\n    _invoked_function_arn=(\n        lambda: get_env_var(\n            AWS_LAMBDA_FUNCTION_ARN_KEY, default_value=\"\"\n        )\n    )(),\n    _memory_limit_in_mb=(\n        lambda: int(\n            get_env_var(\n                AWS_LAMBDA_FUNCTION_MEMORY_SIZE_KEY,\n                default_value=\"1024\",\n            )\n        )\n    )(),\n    _aws_request_id=(\n        lambda: get_env_var(\n            AWS_LAMBDA_FUNCTION_REQUEST_ID_KEY,\n            default_value=\"\",\n        )\n    )(),\n    _log_group_name=(\n        lambda: get_env_var(\n            AWS_LAMBDA_LOG_GROUP_NAME_KEY, default_value=\"\"\n        )\n    )(),\n    _log_stream_name=(\n        lambda: get_env_var(\n            AWS_LAMBDA_LOG_STREAM_NAME_KEY, default_value=\"\"\n        )\n    )(),\n    _identity=(lambda: LambdaCognitoIdentity())(),\n    _client_context=(lambda: LambdaClientContext())(),\n)\n</code></pre> <p>               Bases: <code>LambdaContext</code></p> <p>Standard implementation of LambdaContext for non-Lambda environments.</p> <p>Provides a mock Lambda context for running handlers outside of AWS Lambda, such as in Docker containers or for local testing. All fields can be configured via environment variables.</p>"},{"location":"api/common/models/#aibs_informatics_aws_lambda.common.models.LambdaHandlerRequest","title":"LambdaHandlerRequest  <code>dataclass</code>","text":"<pre><code>LambdaHandlerRequest(\n    handler=custom_field(\n        mm_field=(\n            mm.fields.Function(\n                lambda obj: serialize_handler(obj.handler),\n                deserialize_handler,\n            )\n        )\n    ),\n    event=custom_field(mm_field=(DictField())),\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Request model for dynamic Lambda handler invocation.</p> <p>Contains the handler reference and event payload for routing Lambda invocations to the appropriate handler.</p> <p>Attributes:</p> Name Type Description <code>handler</code> <code>LambdaHandlerType</code> <p>The Lambda handler function to invoke.</p> <code>event</code> <code>LambdaEvent</code> <p>The event payload to pass to the handler.</p>"},{"location":"api/common/models/#aibs_informatics_aws_lambda.common.models.deserialize_handler","title":"deserialize_handler","text":"<pre><code>deserialize_handler(handler)\n</code></pre> <p>Deserialize a handler from its qualified name.</p> <p>Loads a handler from its fully qualified module path. Supports both function handlers and LambdaHandler subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>handler</code> <code>str</code> <p>The fully qualified handler path (e.g., 'module.submodule.HandlerClass').</p> required <p>Returns:</p> Type Description <code>LambdaHandlerType</code> <p>The Lambda handler function ready to be invoked.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the handler cannot be deserialized or is not a valid handler type.</p> Example <pre><code>handler = deserialize_handler('my_module.MyHandler')\nresponse = handler(event, context)\n</code></pre> Source code in <code>src/aibs_informatics_aws_lambda/common/models.py</code> <pre><code>def deserialize_handler(handler: str) -&gt; LambdaHandlerType:\n    \"\"\"Deserialize a handler from its qualified name.\n\n    Loads a handler from its fully qualified module path. Supports both\n    function handlers and LambdaHandler subclasses.\n\n    Args:\n        handler (str): The fully qualified handler path (e.g., 'module.submodule.HandlerClass').\n\n    Returns:\n        The Lambda handler function ready to be invoked.\n\n    Raises:\n        ValueError: If the handler cannot be deserialized or is not a valid handler type.\n\n    Example:\n        ```python\n        handler = deserialize_handler('my_module.MyHandler')\n        response = handler(event, context)\n        ```\n    \"\"\"\n    handler_components = handler.split(\".\")\n\n    handler_module = as_module_type(\".\".join(handler_components[:-1]))\n    handler_name = handler_components[-1]\n\n    handler_code = getattr(handler_module, handler_name)\n    if inspect.isfunction(handler_code):\n        return cast(LambdaHandlerType, handler_code)\n    elif inspect.isclass(handler_code) and issubclass(handler_code, LambdaHandler):\n        logger.debug(f\"Handler code is a class: {handler_code}. Calling `get_handler`...\")\n        return handler_code.get_handler()\n    else:\n        raise ValueError(\n            f\"Unable to deserialize handler: {handler}. \"\n            \"It is not a function or a subclass of LambdaHandler.\"\n        )\n</code></pre>"},{"location":"api/common/models/#aibs_informatics_aws_lambda.common.models.serialize_handler","title":"serialize_handler","text":"<pre><code>serialize_handler(handler)\n</code></pre> <p>Serialize a Lambda handler to its qualified name.</p> <p>Parameters:</p> Name Type Description Default <code>handler</code> <code>LambdaHandlerType</code> <p>The Lambda handler function or class.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The fully qualified module path of the handler.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/models.py</code> <pre><code>def serialize_handler(handler: LambdaHandlerType) -&gt; str:\n    \"\"\"Serialize a Lambda handler to its qualified name.\n\n    Args:\n        handler (LambdaHandlerType): The Lambda handler function or class.\n\n    Returns:\n        The fully qualified module path of the handler.\n    \"\"\"\n    return get_qualified_name(handler)\n</code></pre>"},{"location":"api/common/api/handler/","title":"API Lambda Handler","text":"<p>Base class for creating API Gateway Lambda handlers.</p> <p>API Gateway Lambda handler base class.</p> <p>Provides base classes for creating strongly-typed Lambda handlers that integrate with API Gateway.</p>"},{"location":"api/common/api/handler/#aibs_informatics_aws_lambda.common.api.handler.ApiLambdaHandler","title":"ApiLambdaHandler  <code>dataclass</code>","text":"<pre><code>ApiLambdaHandler(_current_event=None)\n</code></pre> <p>               Bases: <code>LambdaHandler[API_REQUEST, API_RESPONSE]</code>, <code>ApiRoute[API_REQUEST, API_RESPONSE]</code>, <code>Generic[API_REQUEST, API_RESPONSE]</code></p> <p>Base class for API Gateway Lambda handlers.</p> <p>Combines the LambdaHandler capabilities with API Gateway routing, providing automatic request parsing, response formatting, and integration with metrics and logging.</p> <p>          Class Type Parameters:        </p> Name Bound or Constraints Description Default <code>API_REQUEST</code> <p>The request model type.</p> required <code>API_RESPONSE</code> <p>The response model type.</p> required Example <pre><code>@dataclass\nclass MyApiHandler(ApiLambdaHandler[MyRequest, MyResponse]):\n    @classmethod\n    def route_rule(cls) -&gt; str:\n        return \"/users/{user_id}\"\n\n    @classmethod\n    def route_method(cls) -&gt; str:\n        return \"GET\"\n\n    def handle(self, request: MyRequest) -&gt; MyResponse:\n        return MyResponse(...)\n</code></pre>"},{"location":"api/common/api/handler/#aibs_informatics_aws_lambda.common.api.handler.ApiLambdaHandler.api_gateway_caller","title":"api_gateway_caller  <code>property</code>","text":"<pre><code>api_gateway_caller\n</code></pre> <p>Get the caller identifier from the event identity.</p> <p>Returns:</p> Type Description <code>str</code> <p>The caller or user identifier, or 'Unknown' if not available.</p>"},{"location":"api/common/api/handler/#aibs_informatics_aws_lambda.common.api.handler.ApiLambdaHandler.api_gateway_event_identity","title":"api_gateway_event_identity  <code>property</code>","text":"<pre><code>api_gateway_event_identity\n</code></pre> <p>Get the identity information from the request context.</p> <p>Returns:</p> Type Description <code>APIGatewayEventIdentity</code> <p>The API Gateway event identity.</p>"},{"location":"api/common/api/handler/#aibs_informatics_aws_lambda.common.api.handler.ApiLambdaHandler.api_gateway_proxy_event","title":"api_gateway_proxy_event  <code>property</code>","text":"<pre><code>api_gateway_proxy_event\n</code></pre> <p>Get the current event as an APIGatewayProxyEvent.</p> <p>Returns:</p> Type Description <code>APIGatewayProxyEvent</code> <p>The current event as an APIGatewayProxyEvent instance.</p>"},{"location":"api/common/api/handler/#aibs_informatics_aws_lambda.common.api.handler.ApiLambdaHandler.api_gateway_proxy_request_context","title":"api_gateway_proxy_request_context  <code>property</code>","text":"<pre><code>api_gateway_proxy_request_context\n</code></pre> <p>Get the request context from the current event.</p> <p>Returns:</p> Type Description <code>APIGatewayEventRequestContext</code> <p>The API Gateway request context.</p>"},{"location":"api/common/api/handler/#aibs_informatics_aws_lambda.common.api.handler.ApiLambdaHandler.current_event","title":"current_event  <code>property</code> <code>writable</code>","text":"<pre><code>current_event\n</code></pre> <p>Get the current API Gateway proxy event.</p> <p>Returns:</p> Type Description <code>BaseProxyEvent</code> <p>The current proxy event being processed.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no event is currently set.</p>"},{"location":"api/common/api/handler/#aibs_informatics_aws_lambda.common.api.handler.ApiLambdaHandler.add_to_router","title":"add_to_router  <code>classmethod</code>","text":"<pre><code>add_to_router(\n    router, *args, logger=None, metrics=None, **kwargs\n)\n</code></pre> <p>Register this handler with an API Gateway router.</p> <p>Creates a route handler function and registers it with the router using the handler's route rule and method.</p> <p>Parameters:</p> Name Type Description Default <code>router</code> <code>BaseRouter</code> <p>The router to register the handler with.</p> required <code>*args</code> <p>Additional arguments passed to the handler constructor.</p> <code>()</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional logger instance. If None, creates a new one.</p> <code>None</code> <code>metrics</code> <code>Optional[Union[EphemeralMetrics, Metrics]]</code> <p>Optional metrics instance. If None, creates a new one.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the handler constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The registered gateway handler function.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/api/handler.py</code> <pre><code>@classmethod\ndef add_to_router(\n    cls,\n    router: BaseRouter,\n    *args,\n    logger: Optional[Logger] = None,\n    metrics: Optional[Union[EphemeralMetrics, Metrics]] = None,\n    **kwargs,\n) -&gt; Callable:\n    \"\"\"Register this handler with an API Gateway router.\n\n    Creates a route handler function and registers it with the router\n    using the handler's route rule and method.\n\n    Args:\n        router (BaseRouter): The router to register the handler with.\n        *args: Additional arguments passed to the handler constructor.\n        logger (Optional[Logger]): Optional logger instance. If None, creates a new one.\n        metrics (Optional[Union[EphemeralMetrics, Metrics]]): Optional metrics instance.\n            If None, creates a new one.\n        **kwargs: Additional keyword arguments passed to the handler constructor.\n\n    Returns:\n        The registered gateway handler function.\n    \"\"\"\n    logger = logger or cls.get_logger(service=cls.service_name())\n    metrics = metrics or cls.get_metrics()\n\n    @metrics.log_metrics\n    @router.route(rule=cls.route_rule(), method=cls.route_method())\n    def gateway_handler(logger=logger, metrics=metrics, **route_parameters) -&gt; Any:\n        \"\"\"Generic gateway handler\"\"\"\n        start = datetime.now()\n        try:\n            metrics.add_dimension(name=\"route\", value=cls.route_rule())\n            metrics.add_dimension(name=\"handler\", value=cls.handler_name())\n\n            logger.info(f\"Handling {router.current_event.raw_event} event.\")\n\n            cls._parse_event_headers(router.current_event, logger)\n\n            request = cls._parse_event(\n                router.current_event, route_parameters, cast(logging.Logger, logger)\n            )\n\n            logger.debug(f\"Getting dict from {request}\")\n            event = request.to_dict()\n\n            logger.info(f\"Constructed following event from HTTP request: {event}\")\n\n            lambda_handler = cls.get_handler(\n                *args, _current_event=router.current_event, **kwargs\n            )\n\n            logger.info(\"Route handler method constructed. Invoking\")\n            response = lambda_handler(event, router.lambda_context)\n            add_success_metric(metrics=metrics)\n            add_duration_metric(start=start, metrics=metrics)\n            return response\n        except Exception as e:\n            add_failure_metric(metrics=metrics)\n            add_duration_metric(start=start, metrics=metrics)\n            raise e\n\n    return gateway_handler\n</code></pre>"},{"location":"api/common/api/resolver/","title":"API Resolver","text":"<p>Utility class for building API Gateway resolvers.</p> <p>API Gateway resolver builder for Lambda handlers.</p> <p>Provides utilities for building API Gateway REST resolvers with automatic handler discovery and registration.</p>"},{"location":"api/common/api/resolver/#aibs_informatics_aws_lambda.common.api.resolver.ApiResolverBuilder","title":"ApiResolverBuilder  <code>dataclass</code>","text":"<pre><code>ApiResolverBuilder(app=APIGatewayRestResolver())\n</code></pre> <p>               Bases: <code>LoggingMixins</code>, <code>MetricsMixins</code>, <code>PostInitMixin</code></p> <p>Builder for API Gateway REST resolvers with automatic handler registration.</p> <p>Provides a convenient way to build API Gateway resolvers with built-in middleware for validation, logging, and error handling.</p> Example <pre><code>builder = ApiResolverBuilder()\nbuilder.add_handlers(my_handlers_module)\nhandler = builder.get_lambda_handler()\n</code></pre>"},{"location":"api/common/api/resolver/#aibs_informatics_aws_lambda.common.api.resolver.ApiResolverBuilder.add_handlers","title":"add_handlers","text":"<pre><code>add_handlers(target_module, router=None, prefix=None)\n</code></pre> <p>Dynamically add all API Lambda handlers from a module.</p> <p>Discovers all ApiLambdaHandler subclasses in the target module and registers them with the router.</p> <p>Parameters:</p> Name Type Description Default <code>target_module</code> <code>ModuleType</code> <p>The module containing handler classes.</p> required <code>router</code> <code>Optional[BaseRouter]</code> <p>Optional router to add handlers to. If None with prefix, creates a new Router.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>Optional URL prefix for all routes in the module.</p> <code>None</code> Source code in <code>src/aibs_informatics_aws_lambda/common/api/resolver.py</code> <pre><code>def add_handlers(\n    self,\n    target_module: ModuleType,\n    router: Optional[BaseRouter] = None,\n    prefix: Optional[str] = None,\n):\n    \"\"\"Dynamically add all API Lambda handlers from a module.\n\n    Discovers all ApiLambdaHandler subclasses in the target module\n    and registers them with the router.\n\n    Args:\n        target_module (ModuleType): The module containing handler classes.\n        router (Optional[BaseRouter]): Optional router to add handlers to. If None with prefix,\n            creates a new Router.\n        prefix (Optional[str]): Optional URL prefix for all routes in the module.\n    \"\"\"\n\n    if not router and not prefix:\n        router = self.app\n    elif not router:\n        router = Router()\n\n    add_handlers_to_router(\n        router=router,\n        target_module=target_module,\n        logger=self.logger,\n        metrics=self.metrics,\n    )\n\n    if isinstance(router, Router):\n        self.app.include_router(router=router, prefix=prefix)\n</code></pre>"},{"location":"api/common/api/resolver/#aibs_informatics_aws_lambda.common.api.resolver.ApiResolverBuilder.get_lambda_handler","title":"get_lambda_handler","text":"<pre><code>get_lambda_handler(*args, **kwargs)\n</code></pre> <p>Create a Lambda handler function for this resolver.</p> <p>Wraps the handle method with logging context injection and metrics collection.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments (unused).</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments (unused).</p> <code>{}</code> <p>Returns:</p> Type Description <code>LambdaHandlerType</code> <p>A callable Lambda handler function.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/api/resolver.py</code> <pre><code>def get_lambda_handler(self, *args, **kwargs) -&gt; LambdaHandlerType:\n    \"\"\"Create a Lambda handler function for this resolver.\n\n    Wraps the handle method with logging context injection\n    and metrics collection.\n\n    Args:\n        *args: Positional arguments (unused).\n        **kwargs: Keyword arguments (unused).\n\n    Returns:\n        A callable Lambda handler function.\n    \"\"\"\n    lambda_handler = self.handle\n\n    lambda_handler = self.logger.inject_lambda_context(correlation_id_path=API_GATEWAY_REST)(\n        lambda_handler\n    )\n    lambda_handler = self.metrics.log_metrics(capture_cold_start_metric=True)(lambda_handler)  # type: ignore\n\n    return lambda_handler\n</code></pre>"},{"location":"api/common/api/resolver/#aibs_informatics_aws_lambda.common.api.resolver.ApiResolverBuilder.handle","title":"handle","text":"<pre><code>handle(event, context)\n</code></pre> <p>Handle an incoming API Gateway event.</p> <p>Resolves the event to the appropriate handler and returns the response.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>LambdaEvent</code> <p>The Lambda event payload.</p> required <code>context</code> <code>LambdaContext</code> <p>The Lambda context.</p> required <p>Returns:</p> Type Description <code>JSONObject</code> <p>The JSON response from the resolved handler.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If handler execution fails.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/api/resolver.py</code> <pre><code>def handle(self, event: LambdaEvent, context: LambdaContext) -&gt; JSONObject:\n    \"\"\"Handle an incoming API Gateway event.\n\n    Resolves the event to the appropriate handler and returns the response.\n\n    Args:\n        event (LambdaEvent): The Lambda event payload.\n        context (LambdaContext): The Lambda context.\n\n    Returns:\n        The JSON response from the resolved handler.\n\n    Raises:\n        Exception: If handler execution fails.\n    \"\"\"\n    start = datetime.now()\n    try:\n        self.logger.info(f\"Handling API Lambda event: {event}\")\n        response = self.app.resolve(event, context)\n        self.metrics.add_success_metric(self.metric_name_prefix)\n        self.metrics.add_duration_metric(start, name=self.metric_name_prefix)\n        return response\n    except Exception as e:\n        self.logger.error(f\"API Lambda handler failed with following error: {e}\")\n        self.metrics.add_failure_metric(self.metric_name_prefix)\n        self.metrics.add_duration_metric(start, name=self.metric_name_prefix)\n        raise e\n</code></pre>"},{"location":"api/common/api/resolver/#aibs_informatics_aws_lambda.common.api.resolver.ApiResolverBuilder.handle_exception","title":"handle_exception","text":"<pre><code>handle_exception(e)\n</code></pre> <p>Handle uncaught exceptions in request processing.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>Exception</code> <p>The exception that was raised.</p> required <p>Returns:</p> Type Description <p>A Response with status 400 and error details.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/api/resolver.py</code> <pre><code>def handle_exception(self, e: Exception):\n    \"\"\"Handle uncaught exceptions in request processing.\n\n    Args:\n        e (Exception): The exception that was raised.\n\n    Returns:\n        A Response with status 400 and error details.\n    \"\"\"\n    metadata = {\"path\": self.app.current_event.path}\n    self.logger.exception(f\"{e}\", extra=metadata)\n    return Response(\n        status_code=400,\n        content_type=content_types.APPLICATION_JSON,\n        body=json.dumps(\n            {\n                \"request\": self.app.lambda_context.aws_request_id,\n                \"error\": e.args,\n                \"stacktrace\": format_exc(),\n            },\n            indent=True,\n        ),\n    )\n</code></pre>"},{"location":"api/common/api/resolver/#aibs_informatics_aws_lambda.common.api.resolver.ApiResolverBuilder.handle_not_found","title":"handle_not_found","text":"<pre><code>handle_not_found(e)\n</code></pre> <p>Handle requests to non-existent routes.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>NotFoundError</code> <p>The NotFoundError exception.</p> required <p>Returns:</p> Type Description <code>Response</code> <p>A Response with status 418 and error message.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/api/resolver.py</code> <pre><code>def handle_not_found(self, e: NotFoundError) -&gt; Response:\n    \"\"\"Handle requests to non-existent routes.\n\n    Args:\n        e (NotFoundError): The NotFoundError exception.\n\n    Returns:\n        A Response with status 418 and error message.\n    \"\"\"\n    msg = f\"Could not find route {self.app.current_event.path}: {e.msg}\"\n    self.logger.exception(msg)\n    self.metrics.add_count_metric(\"RouteNotFound\", 1)\n    return Response(status_code=418, content_type=content_types.TEXT_PLAIN, body=msg)\n</code></pre>"},{"location":"api/common/api/resolver/#aibs_informatics_aws_lambda.common.api.resolver.ApiResolverBuilder.update_logging_level","title":"update_logging_level","text":"<pre><code>update_logging_level(event)\n</code></pre> <p>Update the logging level based on request headers.</p> <p>Checks for an 'X-Log-Level' header and adjusts the logger level accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>APIGatewayProxyEvent</code> <p>The API Gateway proxy event.</p> required Source code in <code>src/aibs_informatics_aws_lambda/common/api/resolver.py</code> <pre><code>def update_logging_level(self, event: APIGatewayProxyEvent) -&gt; None:\n    \"\"\"Update the logging level based on request headers.\n\n    Checks for an 'X-Log-Level' header and adjusts the logger\n    level accordingly.\n\n    Args:\n        event (APIGatewayProxyEvent): The API Gateway proxy event.\n    \"\"\"\n    if log_level := event.headers.get(\"X-Log-Level\"):\n        try:\n            self.logger.setLevel(log_level)\n        except Exception as e:\n            self.logger.warning(f\"Failed to set log level to {log_level}: {e}\")\n</code></pre>"},{"location":"api/common/api/resolver/#aibs_informatics_aws_lambda.common.api.resolver.ApiResolverBuilder.validate_event","title":"validate_event","text":"<pre><code>validate_event(event)\n</code></pre> <p>Validate the incoming API Gateway event.</p> <p>Override this method to add custom validation logic.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>APIGatewayProxyEvent</code> <p>The API Gateway proxy event to validate.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>If validation fails.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/api/resolver.py</code> <pre><code>def validate_event(self, event: APIGatewayProxyEvent) -&gt; None:\n    \"\"\"Validate the incoming API Gateway event.\n\n    Override this method to add custom validation logic.\n\n    Args:\n        event (APIGatewayProxyEvent): The API Gateway proxy event to validate.\n\n    Raises:\n        Exception: If validation fails.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/common/api/resolver/#aibs_informatics_aws_lambda.common.api.resolver.add_handlers_to_router","title":"add_handlers_to_router","text":"<pre><code>add_handlers_to_router(\n    router, target_module, metrics=None, logger=None\n)\n</code></pre> <p>Add all API handlers from a module to a router.</p> <p>Discovers ApiLambdaHandler subclasses in the target module and registers each with the router.</p> <p>Parameters:</p> Name Type Description Default <code>router</code> <code>BaseRouter</code> <p>The router to register handlers with.</p> required <code>target_module</code> <code>ModuleType</code> <p>The module containing handler classes.</p> required <code>metrics</code> <code>Optional[Union[EphemeralMetrics, Metrics]]</code> <p>Optional metrics collector for the handlers.</p> <code>None</code> <code>logger</code> <code>Optional[Logger]</code> <p>Optional logger for the handlers.</p> <code>None</code> Source code in <code>src/aibs_informatics_aws_lambda/common/api/resolver.py</code> <pre><code>def add_handlers_to_router(\n    router: BaseRouter,\n    target_module: ModuleType,\n    metrics: Optional[Union[EphemeralMetrics, Metrics]] = None,\n    logger: Optional[Logger] = None,\n):\n    \"\"\"Add all API handlers from a module to a router.\n\n    Discovers ApiLambdaHandler subclasses in the target module and\n    registers each with the router.\n\n    Args:\n        router (BaseRouter): The router to register handlers with.\n        target_module (ModuleType): The module containing handler classes.\n        metrics (Optional[Union[EphemeralMetrics, Metrics]]): Optional metrics collector\n            for the handlers.\n        logger (Optional[Logger]): Optional logger for the handlers.\n    \"\"\"\n    target_api_handler_classes = get_target_handler_classes(target_module)\n\n    # Add each lambda handler to the route.\n    for api_handler_class in target_api_handler_classes:\n        api_handler_class.add_to_router(router, logger=logger, metrics=metrics)\n</code></pre>"},{"location":"api/common/api/resolver/#aibs_informatics_aws_lambda.common.api.resolver.get_target_handler_classes","title":"get_target_handler_classes","text":"<pre><code>get_target_handler_classes(target_module)\n</code></pre> <p>Get all ApiLambdaHandler subclasses in a module.</p> <p>Recursively loads all modules from the target package and returns all ApiLambdaHandler subclasses found.</p> <p>Parameters:</p> Name Type Description Default <code>target_module</code> <code>ModuleType</code> <p>The module or package to search.</p> required <p>Returns:</p> Type Description <code>List[ApiLambdaHandler]</code> <p>A list of ApiLambdaHandler subclasses found in the module.</p> Source code in <code>src/aibs_informatics_aws_lambda/common/api/resolver.py</code> <pre><code>def get_target_handler_classes(target_module: ModuleType) -&gt; List[ApiLambdaHandler]:\n    \"\"\"Get all ApiLambdaHandler subclasses in a module.\n\n    Recursively loads all modules from the target package and returns\n    all ApiLambdaHandler subclasses found.\n\n    Args:\n        target_module (ModuleType): The module or package to search.\n\n    Returns:\n        A list of ApiLambdaHandler subclasses found in the module.\n    \"\"\"\n    # Load modules from package root.\n    loaded_modules = load_all_modules_from_pkg(target_module, include_packages=True)\n\n    # Resolve subclasses of GCSApiLambdaHandler found within package root.\n    target_module_paths = [\n        # Along with loaded modules, we also add the root module\n        # to the list of target module paths. Depending on whether\n        # the root module is a module or a package, we must resolve\n        # the string path differently.\n        target_module.__name__,\n        getattr(target_module, \"__module__\", getattr(target_module, \"__package__\")),\n        *list(loaded_modules.keys()),\n    ]\n\n    target_api_handler_classes: List[ApiLambdaHandler] = [\n        api_handler_class\n        for api_handler_class in get_all_subclasses(ApiLambdaHandler, True)  # type: ignore[type-abstract]\n        if (getattr(api_handler_class, \"__module__\") in target_module_paths)\n    ]\n    return target_api_handler_classes\n</code></pre>"},{"location":"api/handlers/","title":"Handlers","text":"<p>The <code>handlers</code> module contains Lambda handler implementations for various AWS services and tasks.</p>"},{"location":"api/handlers/#overview","title":"Overview","text":"Module Description Batch AWS Batch job definition and execution handlers Data Sync Data synchronization and file system handlers Demand Demand execution scaffolding handlers ECR ECR image replication handlers Notifications Notification routing and delivery handlers"},{"location":"api/handlers/batch/","title":"Batch Handlers","text":"<p>Handlers for AWS Batch job definition and execution.</p>"},{"location":"api/handlers/batch/#overview","title":"Overview","text":"Module Description Create <code>CreateDefinitionAndPrepareArgsHandler</code> for creating and preparing AWS Batch job definitions Model Data models for batch operations"},{"location":"api/handlers/batch/#quick-start","title":"Quick Start","text":"<pre><code>from aibs_informatics_aws_lambda.handlers.batch.create import CreateDefinitionAndPrepareArgsHandler\n\nhandler = CreateDefinitionAndPrepareArgsHandler().get_handler()\n</code></pre>"},{"location":"api/handlers/batch/create/","title":"Batch Create","text":"<p>Handler for creating and preparing AWS Batch job definitions.</p> <p>AWS Batch job definition creation handlers.</p> <p>Provides Lambda handlers for creating and registering AWS Batch job definitions.</p>"},{"location":"api/handlers/batch/create/#aibs_informatics_aws_lambda.handlers.batch.create.CreateDefinitionAndPrepareArgsHandler","title":"CreateDefinitionAndPrepareArgsHandler  <code>dataclass</code>","text":"<pre><code>CreateDefinitionAndPrepareArgsHandler()\n</code></pre> <p>               Bases: <code>LambdaHandler[CreateDefinitionAndPrepareArgsRequest, CreateDefinitionAndPrepareArgsResponse]</code></p> <p>Handler for creating AWS Batch job definitions.</p> <p>Registers a job definition with AWS Batch and prepares the arguments needed for job submission.</p>"},{"location":"api/handlers/batch/create/#aibs_informatics_aws_lambda.handlers.batch.create.CreateDefinitionAndPrepareArgsHandler.handle","title":"handle","text":"<pre><code>handle(request)\n</code></pre> <p>Create a job definition and prepare submission arguments.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>CreateDefinitionAndPrepareArgsRequest</code> <p>Request containing job definition configuration.</p> required <p>Returns:</p> Type Description <code>CreateDefinitionAndPrepareArgsResponse</code> <p>Response containing the job definition ARN and submission args.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If job_queue_name is not provided.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/batch/create.py</code> <pre><code>def handle(\n    self, request: CreateDefinitionAndPrepareArgsRequest\n) -&gt; CreateDefinitionAndPrepareArgsResponse:\n    \"\"\"Create a job definition and prepare submission arguments.\n\n    Args:\n        request (CreateDefinitionAndPrepareArgsRequest): Request containing\n            job definition configuration.\n\n    Returns:\n        Response containing the job definition ARN and submission args.\n\n    Raises:\n        ValueError: If job_queue_name is not provided.\n    \"\"\"\n    job_def_builder = BatchJobBuilder(\n        image=request.image\n        if DockerImageUri.is_valid(request.image)\n        else resolve_image_uri(request.image),\n        job_definition_name=request.job_definition_name,\n        job_name=request.job_name or f\"{request.job_definition_name}-{uuid_str()}\",\n        command=request.command,\n        environment=request.environment,\n        job_definition_tags=request.job_definition_tags,\n        resource_requirements=request.resource_requirements,\n        mount_points=request.mount_points,\n        volumes=request.volumes,\n        privileged=request.privileged,\n        job_role_arn=request.job_role_arn,\n    )\n\n    response = register_job_definition(\n        job_definition_name=job_def_builder.job_definition_name,\n        container_properties=job_def_builder.container_properties,\n        retry_strategy=request.retry_strategy or build_retry_strategy(),\n        parameters=None,\n        tags=job_def_builder.job_definition_tags,\n    )\n    job_definition_arn = response[\"jobDefinitionArn\"]\n\n    job_queue_name = request.job_queue_name\n    if not job_queue_name:\n        raise ValueError(\"job_queue_name must be provided\")\n    return CreateDefinitionAndPrepareArgsResponse(\n        job_name=job_def_builder.job_name,\n        job_definition_arn=job_definition_arn,\n        job_queue_arn=job_queue_name,\n        parameters={},\n        container_overrides=job_def_builder.container_overrides__sfn,\n    )\n</code></pre>"},{"location":"api/handlers/batch/create/#aibs_informatics_aws_lambda.handlers.batch.create.DockerImageUri","title":"DockerImageUri","text":"<p>               Bases: <code>ValidatedStr</code></p> <p>Validated string representing a Docker image URI.</p> <p>Validates and parses Docker image URIs from various registries including Docker Hub, ECR, and GitHub Container Registry.</p> Note <p>This class is intended to be moved to <code>aibs-informatics-core</code> once it is considered stable; that refactor is tracked in the team's external issue tracker.</p> <p>Attributes:</p> Name Type Description <code>regex_pattern</code> <code>Pattern</code> <p>Pattern for validating Docker image URIs.</p>"},{"location":"api/handlers/batch/create/#aibs_informatics_aws_lambda.handlers.batch.create.DockerImageUri.registry","title":"registry  <code>property</code>","text":"<pre><code>registry\n</code></pre> <p>Get the registry portion of the image URI.</p> <p>Returns:</p> Type Description <code>str</code> <p>The registry hostname.</p>"},{"location":"api/handlers/batch/create/#aibs_informatics_aws_lambda.handlers.batch.create.DockerImageUri.repo_name","title":"repo_name  <code>property</code>","text":"<pre><code>repo_name\n</code></pre> <p>Get the repository name.</p> <p>Returns:</p> Type Description <code>str</code> <p>The repository name.</p>"},{"location":"api/handlers/batch/create/#aibs_informatics_aws_lambda.handlers.batch.create.DockerImageUri.sha256","title":"sha256  <code>property</code>","text":"<pre><code>sha256\n</code></pre> <p>Get the SHA256 digest if specified.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The SHA256 digest, or None if using a tag.</p>"},{"location":"api/handlers/batch/create/#aibs_informatics_aws_lambda.handlers.batch.create.DockerImageUri.tag","title":"tag  <code>property</code>","text":"<pre><code>tag\n</code></pre> <p>Get the image tag if specified.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The tag, or None if using a SHA digest.</p>"},{"location":"api/handlers/batch/model/","title":"Batch Model","text":"<p>Data models for AWS Batch operations.</p> <p>AWS Batch job models.</p> <p>Defines the request and response models for creating and managing AWS Batch job definitions and submissions.</p>"},{"location":"api/handlers/batch/model/#aibs_informatics_aws_lambda.handlers.batch.model.CreateDefinitionAndPrepareArgsRequest","title":"CreateDefinitionAndPrepareArgsRequest  <code>dataclass</code>","text":"<pre><code>CreateDefinitionAndPrepareArgsRequest(\n    image=custom_field(),\n    job_definition_name=custom_field(),\n    job_queue_name=custom_field(),\n    job_role_arn=custom_field(default=None),\n    job_name=custom_field(default=None),\n    command=custom_field(default_factory=list),\n    environment=custom_field(default_factory=dict),\n    job_definition_tags=custom_field(default_factory=dict),\n    resource_requirements=custom_field(\n        default_factory=list,\n        mm_field=(\n            UnionField(\n                [\n                    (list, ListField(DictField)),\n                    (\n                        ResourceRequirements,\n                        ResourceRequirements.as_mm_field(),\n                    ),\n                ]\n            )\n        ),\n    ),\n    mount_points=custom_field(default_factory=list),\n    volumes=custom_field(default_factory=list),\n    retry_strategy=custom_field(default=None),\n    privileged=custom_field(default=False),\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Request for creating a batch job definition and preparing submission args.</p> <p>Attributes:</p> Name Type Description <code>image</code> <code>str</code> <p>Docker image URI for the job.</p> <code>job_definition_name</code> <code>str</code> <p>Name for the job definition.</p> <code>job_queue_name</code> <code>str</code> <p>Name of the job queue to submit to.</p> <code>job_role_arn</code> <code>Optional[str]</code> <p>Optional IAM role ARN for the job.</p> <code>job_name</code> <code>Optional[str]</code> <p>Optional name for the submitted job.</p> <code>command</code> <code>List[str]</code> <p>Command to run in the container.</p> <code>environment</code> <code>Dict[str, str]</code> <p>Environment variables for the container.</p> <code>job_definition_tags</code> <code>Dict[str, str]</code> <p>Tags to apply to the job definition.</p> <code>resource_requirements</code> <code>Union[List[ResourceRequirementTypeDef], ResourceRequirements]</code> <p>CPU, memory, and GPU requirements.</p> <code>mount_points</code> <code>List[MountPointTypeDef]</code> <p>EFS/volume mount points.</p> <code>volumes</code> <code>List[VolumeTypeDef]</code> <p>Volume definitions.</p> <code>retry_strategy</code> <code>Optional[RetryStrategyTypeDef]</code> <p>Job retry configuration.</p> <code>privileged</code> <code>bool</code> <p>Whether to run in privileged mode.</p>"},{"location":"api/handlers/batch/model/#aibs_informatics_aws_lambda.handlers.batch.model.CreateDefinitionAndPrepareArgsResponse","title":"CreateDefinitionAndPrepareArgsResponse  <code>dataclass</code>","text":"<pre><code>CreateDefinitionAndPrepareArgsResponse(\n    job_name=custom_field(),\n    job_definition_arn=custom_field(),\n    job_queue_arn=custom_field(),\n    parameters=custom_field(),\n    container_overrides=custom_field(),\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Response from creating a batch job definition.</p> <p>Contains the prepared arguments for submitting the batch job.</p> <p>Attributes:</p> Name Type Description <code>job_name</code> <code>str</code> <p>Name for the submitted job.</p> <code>job_definition_arn</code> <code>Optional[str]</code> <p>ARN of the created job definition.</p> <code>job_queue_arn</code> <code>str</code> <p>ARN of the job queue.</p> <code>parameters</code> <code>Dict[str, Any]</code> <p>Job parameters for submission.</p> <code>container_overrides</code> <code>Dict[str, Any]</code> <p>Container override settings.</p>"},{"location":"api/handlers/data-sync/","title":"Data Sync Handlers","text":"<p>Handlers for data synchronization and file system operations.</p>"},{"location":"api/handlers/data-sync/#overview","title":"Overview","text":"Module Description Operations Data sync operations (read, write, sync) File System File system operations (list, stats, remove) Model Data models for data sync operations"},{"location":"api/handlers/data-sync/#available-handlers","title":"Available Handlers","text":""},{"location":"api/handlers/data-sync/#operations","title":"Operations","text":"<ul> <li><code>GetJSONFromFileHandler</code> - Retrieves JSON data from a file</li> <li><code>PutJSONToFileHandler</code> - Writes JSON data to a file</li> <li><code>DataSyncHandler</code> - Simple data sync task</li> <li><code>BatchDataSyncHandler</code> - Handles batch of data sync tasks</li> <li><code>PrepareBatchDataSyncHandler</code> - Prepares batch data sync tasks</li> </ul>"},{"location":"api/handlers/data-sync/#file-system","title":"File System","text":"<ul> <li><code>GetDataPathStatsHandler</code> - Retrieves statistics about data paths</li> <li><code>ListDataPathsHandler</code> - Lists data paths</li> <li><code>OutdatedDataPathScannerHandler</code> - Scans for outdated data paths</li> <li><code>RemoveDataPathsHandler</code> - Removes data paths</li> </ul>"},{"location":"api/handlers/data-sync/#quick-start","title":"Quick Start","text":"<pre><code>from aibs_informatics_aws_lambda.handlers.data_sync.operations import GetJSONFromFileHandler\n\nhandler = GetJSONFromFileHandler().get_handler()\n</code></pre>"},{"location":"api/handlers/data-sync/file-system/","title":"Data Sync File System","text":"<p>Handlers for file system operations.</p>"},{"location":"api/handlers/data-sync/file-system/#handlers","title":"Handlers","text":"<ul> <li><code>GetDataPathStatsHandler</code> - Retrieves statistics about data paths</li> <li><code>ListDataPathsHandler</code> - Lists data paths</li> <li><code>OutdatedDataPathScannerHandler</code> - Scans for outdated data paths</li> <li><code>RemoveDataPathsHandler</code> - Removes data paths</li> </ul> <p>File system operation handlers.</p> <p>Provides Lambda handlers for file system operations including listing paths, getting statistics, scanning for outdated files, and removing paths.</p>"},{"location":"api/handlers/data-sync/file-system/#aibs_informatics_aws_lambda.handlers.data_sync.file_system.GetDataPathStatsHandler","title":"GetDataPathStatsHandler  <code>dataclass</code>","text":"<pre><code>GetDataPathStatsHandler()\n</code></pre> <p>               Bases: <code>LambdaHandler[GetDataPathStatsRequest, GetDataPathStatsResponse]</code></p> <p>Handler for retrieving statistics about a data path.</p> <p>Returns size, modification time, and child path information.</p>"},{"location":"api/handlers/data-sync/file-system/#aibs_informatics_aws_lambda.handlers.data_sync.file_system.GetDataPathStatsHandler.handle","title":"handle","text":"<pre><code>handle(request)\n</code></pre> <p>Get statistics for the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>GetDataPathStatsRequest</code> <p>Request containing the path to analyze.</p> required <p>Returns:</p> Type Description <code>GetDataPathStatsResponse</code> <p>Response containing path statistics and child information.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/data_sync/file_system.py</code> <pre><code>def handle(self, request: GetDataPathStatsRequest) -&gt; GetDataPathStatsResponse:\n    \"\"\"Get statistics for the specified path.\n\n    Args:\n        request (GetDataPathStatsRequest): Request containing the path to analyze.\n\n    Returns:\n        Response containing path statistics and child information.\n    \"\"\"\n    root = get_file_system(request.path)\n    node = root.node\n    return GetDataPathStatsResponse(\n        path=node.path,\n        path_stats=node.path_stats,\n        children={\n            child_path: child_node.path_stats\n            for child_path, child_node in node.children.items()\n        },\n    )\n</code></pre>"},{"location":"api/handlers/data-sync/file-system/#aibs_informatics_aws_lambda.handlers.data_sync.file_system.ListDataPathsHandler","title":"ListDataPathsHandler  <code>dataclass</code>","text":"<pre><code>ListDataPathsHandler()\n</code></pre> <p>               Bases: <code>LambdaHandler[ListDataPathsRequest, ListDataPathsResponse]</code></p> <p>Handler for listing paths under a root directory.</p> <p>Supports include and exclude patterns for filtering results.</p>"},{"location":"api/handlers/data-sync/file-system/#aibs_informatics_aws_lambda.handlers.data_sync.file_system.ListDataPathsHandler.handle","title":"handle","text":"<pre><code>handle(request)\n</code></pre> <p>List all paths under the specified root.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ListDataPathsRequest</code> <p>Request containing the root path and optional patterns.</p> required <p>Returns:</p> Type Description <code>ListDataPathsResponse</code> <p>Response containing the list of matching paths.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/data_sync/file_system.py</code> <pre><code>def handle(self, request: ListDataPathsRequest) -&gt; ListDataPathsResponse:\n    \"\"\"List all paths under the specified root.\n\n    Args:\n        request (ListDataPathsRequest): Request containing the root path and optional patterns.\n\n    Returns:\n        Response containing the list of matching paths.\n    \"\"\"\n    root = get_file_system(request.path)\n    paths: List[DataPath] = sorted([n.path for n in root.node.list_nodes()])\n\n    if request.include_patterns or request.exclude_patterns:\n        new_paths = []\n        for path in paths:\n            rel_path = strip_path_root(path, root.node.path)\n            if request.include_patterns:\n                if not any([i.match(rel_path) for i in request.include_patterns]):\n                    continue\n            if request.exclude_patterns:\n                if any([i.match(rel_path) for i in request.exclude_patterns]):\n                    continue\n            new_paths.append(path)\n        paths = new_paths\n    return ListDataPathsResponse(paths=paths)\n</code></pre>"},{"location":"api/handlers/data-sync/file-system/#aibs_informatics_aws_lambda.handlers.data_sync.file_system.OutdatedDataPathScannerHandler","title":"OutdatedDataPathScannerHandler  <code>dataclass</code>","text":"<pre><code>OutdatedDataPathScannerHandler()\n</code></pre> <p>               Bases: <code>LambdaHandler[OutdatedDataPathScannerRequest, OutdatedDataPathScannerResponse]</code></p> <p>Handler for scanning and identifying outdated data paths.</p> <p>Identifies stale files based on last access time while respecting minimum size thresholds to maintain EFS throughput performance.</p>"},{"location":"api/handlers/data-sync/file-system/#aibs_informatics_aws_lambda.handlers.data_sync.file_system.OutdatedDataPathScannerHandler.handle","title":"handle","text":"<pre><code>handle(request)\n</code></pre> <p>Scan for outdated paths to delete.</p> <p>Uses a two-step process: 1. Identify stale nodes whose days_since_last_accessed exceeds the threshold. 2. Sort stale nodes oldest-first and select for deletion while    maintaining the minimum size requirement.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>OutdatedDataPathScannerRequest</code> <p>Request containing scan parameters.</p> required <p>Returns:</p> Type Description <code>OutdatedDataPathScannerResponse</code> <p>Response containing paths eligible for deletion.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/data_sync/file_system.py</code> <pre><code>def handle(self, request: OutdatedDataPathScannerRequest) -&gt; OutdatedDataPathScannerResponse:\n    \"\"\"Scan for outdated paths to delete.\n\n    Uses a two-step process:\n    1. Identify stale nodes whose days_since_last_accessed exceeds the threshold.\n    2. Sort stale nodes oldest-first and select for deletion while\n       maintaining the minimum size requirement.\n\n    Args:\n        request (OutdatedDataPathScannerRequest): Request containing scan parameters.\n\n    Returns:\n        Response containing paths eligible for deletion.\n    \"\"\"\n    fs = get_file_system(request.path)\n\n    stale_nodes: List[Node] = []\n    days_since_last_accessed = timedelta(days=request.days_since_last_accessed)\n    unvisited_nodes: List[Node] = [fs.node]\n\n    self.logger.info(\n        f\"Checking for nodes older than {request.days_since_last_accessed} days. \"\n        f\"Max depth = {request.max_depth}\"\n    )\n    # Step 1)\n    while unvisited_nodes:\n        node = unvisited_nodes.pop()\n        if (request.current_time - node.last_modified) &gt; days_since_last_accessed:\n            if (\n                request.min_depth is None\n                or (node.depth - fs.node.depth) &gt;= request.min_depth\n                or not node.has_children()\n            ):\n                stale_nodes.append(node)\n            else:\n                unvisited_nodes.extend(node.children.values())\n        elif request.max_depth is None or (node.depth - fs.node.depth) &lt; request.max_depth:\n            unvisited_nodes.extend(node.children.values())\n\n    # Step 2)\n    # Get the current size of the EFS volume, this is used to ensure we do not delete too\n    # many files and allows us to maintain a minimum desired EFS throughput performance.\n    # For more details see: https://docs.aws.amazon.com/efs/latest/ug/performance.html\n    current_efs_size_bytes = fs.node.size_bytes\n    paths_to_delete: List[str] = []\n\n    # Sort so newest nodes are first, nodes are considered starting from the list end (oldest)\n    nodes_to_delete = sorted(stale_nodes, key=lambda n: n.last_modified, reverse=True)\n    while nodes_to_delete and current_efs_size_bytes &gt; request.min_size_bytes_allowed:\n        node = nodes_to_delete.pop()\n        paths_to_delete.append(node.path)\n        current_efs_size_bytes -= node.size_bytes\n\n    return OutdatedDataPathScannerResponse(\n        paths=sorted(paths_to_delete),\n    )\n</code></pre>"},{"location":"api/handlers/data-sync/file-system/#aibs_informatics_aws_lambda.handlers.data_sync.file_system.RemoveDataPathsHandler","title":"RemoveDataPathsHandler  <code>dataclass</code>","text":"<pre><code>RemoveDataPathsHandler()\n</code></pre> <p>               Bases: <code>LambdaHandler[RemoveDataPathsRequest, RemoveDataPathsResponse]</code></p> <p>Handler for removing data paths.</p> <p>Supports removing local paths and EFS paths. S3 path removal is not currently supported.</p>"},{"location":"api/handlers/data-sync/file-system/#aibs_informatics_aws_lambda.handlers.data_sync.file_system.RemoveDataPathsHandler.handle","title":"handle","text":"<pre><code>handle(request)\n</code></pre> <p>Remove the specified paths.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>RemoveDataPathsRequest</code> <p>Request containing the list of paths to remove.</p> required <p>Returns:</p> Type Description <code>RemoveDataPathsResponse</code> <p>Response containing the total bytes removed and paths processed.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/data_sync/file_system.py</code> <pre><code>def handle(self, request: RemoveDataPathsRequest) -&gt; RemoveDataPathsResponse:\n    \"\"\"Remove the specified paths.\n\n    Args:\n        request (RemoveDataPathsRequest): Request containing the list of paths to remove.\n\n    Returns:\n        Response containing the total bytes removed and paths processed.\n    \"\"\"\n    self.logger.info(f\"Removing {len(request.paths)}\")\n\n    mount_points = None\n    size_bytes_removed = 0\n    paths_removed = []\n    for path in request.paths:\n        if isinstance(path, S3URI):\n            # # TODO: add support for S3URI when more guardrails are in place\n            # path_stats = get_s3_path_stats(path)\n            # delete_s3_path(path)\n            # size_bytes_removed += path_stats.size_bytes\n            self.logger.warning(f\"Skipping S3URI path deletion ({path}). Not supported yet.\")\n        else:\n            if isinstance(path, EFSPath):\n                self.logger.info(f\"Converting EFSPath ({path}) to local path\")\n                if mount_points is None:\n                    mount_points = detect_mount_points()\n                path = get_local_path(efs_path=path, mount_points=mount_points)\n            elif not isinstance(path, Path):\n                path = Path(path)\n            try:\n                size_bytes = get_path_size_bytes(path)\n                self.logger.info(f\"Removing {path} (size {size_bytes} bytes)\")\n                remove_path(path)\n                size_bytes_removed += size_bytes\n                paths_removed.append(path)\n            except FileNotFoundError as e:\n                self.logger.warning(f\"File at {path} does not exist anymore. Reason: {e}\")\n    return RemoveDataPathsResponse(size_bytes_removed, paths_removed)\n</code></pre>"},{"location":"api/handlers/data-sync/model/","title":"Data Sync Model","text":"<p>Data models for data synchronization operations.</p> <p>Data synchronization models.</p> <p>Defines the request and response models for data sync operations between S3, EFS, and local file systems.</p>"},{"location":"api/handlers/data-sync/model/#aibs_informatics_aws_lambda.handlers.data_sync.model.GetDataPathStatsRequest","title":"GetDataPathStatsRequest  <code>dataclass</code>","text":"<pre><code>GetDataPathStatsRequest(\n    path=custom_field(mm_field=(DataPathField())),\n)\n</code></pre> <p>               Bases: <code>WithDataPath</code></p> <p>Request for getting statistics about a data path.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>DataPath</code> <p>The data path to get statistics for.</p>"},{"location":"api/handlers/data-sync/model/#aibs_informatics_aws_lambda.handlers.data_sync.model.GetDataPathStatsResponse","title":"GetDataPathStatsResponse  <code>dataclass</code>","text":"<pre><code>GetDataPathStatsResponse(\n    path=custom_field(mm_field=(DataPathField())),\n    path_stats=custom_field(\n        mm_field=(PathStats.as_mm_field())\n    ),\n    children=custom_field(\n        mm_field=(\n            DictField(\n                keys=(StringField()),\n                values=(PathStats.as_mm_field()),\n            )\n        )\n    ),\n)\n</code></pre> <p>               Bases: <code>WithDataPath</code></p> <p>Response containing data path statistics.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>DataPath</code> <p>The data path.</p> <code>path_stats</code> <code>PathStats</code> <p>Statistics for the path.</p> <code>children</code> <code>Dict[str, PathStats]</code> <p>Statistics for child paths keyed by name.</p>"},{"location":"api/handlers/data-sync/model/#aibs_informatics_aws_lambda.handlers.data_sync.model.ListDataPathsRequest","title":"ListDataPathsRequest  <code>dataclass</code>","text":"<pre><code>ListDataPathsRequest(\n    path=custom_field(mm_field=(DataPathField())),\n    include=custom_field(\n        default=None,\n        mm_field=(\n            UnionField(\n                [\n                    (str, StringField()),\n                    (list, ListField(StringField())),\n                ]\n            )\n        ),\n    ),\n    exclude=custom_field(\n        default=None,\n        mm_field=(\n            UnionField(\n                [\n                    (str, StringField()),\n                    (list, ListField(StringField())),\n                ]\n            )\n        ),\n    ),\n)\n</code></pre> <p>               Bases: <code>WithDataPath</code></p> <p>Request for listing files under a data path.</p> <p>Supports filtering with include/exclude regex patterns.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>DataPath</code> <p>The data path under which to list files.</p> <code>include</code> <code>Optional[Union[str, List[str]]]</code> <p>Optional regex pattern(s) for files to include. If multiple patterns, includes files matching any pattern.</p> <code>exclude</code> <code>Optional[Union[str, List[str]]]</code> <p>Optional regex pattern(s) for files to exclude. Exclude patterns take precedence over include patterns.</p>"},{"location":"api/handlers/data-sync/model/#aibs_informatics_aws_lambda.handlers.data_sync.model.ListDataPathsResponse","title":"ListDataPathsResponse  <code>dataclass</code>","text":"<pre><code>ListDataPathsResponse(\n    paths=custom_field(\n        default_factory=list,\n        mm_field=(ListField(DataPathField())),\n    )\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Response containing listed data paths.</p> <p>Attributes:</p> Name Type Description <code>paths</code> <code>List[DataPath]</code> <p>List of data paths found.</p>"},{"location":"api/handlers/data-sync/model/#aibs_informatics_aws_lambda.handlers.data_sync.model.OutdatedDataPathScannerRequest","title":"OutdatedDataPathScannerRequest  <code>dataclass</code>","text":"<pre><code>OutdatedDataPathScannerRequest(\n    path=custom_field(mm_field=(DataPathField())),\n    days_since_last_accessed=custom_field(\n        default=0, mm_field=(FloatField())\n    ),\n    max_depth=custom_field(\n        default=None, mm_field=(IntegerField())\n    ),\n    min_depth=custom_field(\n        default=None, mm_field=(IntegerField())\n    ),\n    min_size_bytes_allowed=custom_field(\n        default=0, mm_field=(IntegerField())\n    ),\n    current_time=custom_field(\n        default_factory=get_current_time,\n        mm_field=(CustomAwareDateTime()),\n    ),\n)\n</code></pre> <p>               Bases: <code>WithDataPath</code></p> <p>Request for scanning outdated data paths.</p> <p>Scans for paths that haven't been accessed within a specified time.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>DataPath</code> <p>The root path to scan.</p> <code>days_since_last_accessed</code> <code>float</code> <p>Minimum days since last access to be outdated.</p> <code>max_depth</code> <code>Optional[int]</code> <p>Maximum directory depth to scan.</p> <code>min_depth</code> <code>Optional[int]</code> <p>Minimum directory depth to scan.</p> <code>min_size_bytes_allowed</code> <code>int</code> <p>Minimum size threshold for paths to include.</p> <code>current_time</code> <code>datetime</code> <p>Reference time for calculating age.</p>"},{"location":"api/handlers/data-sync/model/#aibs_informatics_aws_lambda.handlers.data_sync.model.OutdatedDataPathScannerResponse","title":"OutdatedDataPathScannerResponse  <code>dataclass</code>","text":"<pre><code>OutdatedDataPathScannerResponse(\n    paths=custom_field(\n        default_factory=list,\n        mm_field=(ListField(DataPathField())),\n    )\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Response containing outdated data paths.</p> <p>Attributes:</p> Name Type Description <code>paths</code> <code>List[DataPath]</code> <p>List of paths identified as outdated.</p>"},{"location":"api/handlers/data-sync/model/#aibs_informatics_aws_lambda.handlers.data_sync.model.RemoveDataPathsRequest","title":"RemoveDataPathsRequest  <code>dataclass</code>","text":"<pre><code>RemoveDataPathsRequest(\n    paths=custom_field(\n        default_factory=list,\n        mm_field=(ListField(DataPathField())),\n    )\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Request for removing data paths.</p> <p>Attributes:</p> Name Type Description <code>paths</code> <code>List[DataPath]</code> <p>List of data paths to remove.</p>"},{"location":"api/handlers/data-sync/model/#aibs_informatics_aws_lambda.handlers.data_sync.model.RemoveDataPathsResponse","title":"RemoveDataPathsResponse  <code>dataclass</code>","text":"<pre><code>RemoveDataPathsResponse(\n    size_bytes_removed=custom_field(),\n    paths_removed=custom_field(\n        default_factory=list,\n        mm_field=(ListField(DataPathField())),\n    ),\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Response from removing data paths.</p> <p>Attributes:</p> Name Type Description <code>size_bytes_removed</code> <code>int</code> <p>Total bytes removed.</p> <code>paths_removed</code> <code>List[DataPath]</code> <p>List of paths that were removed.</p>"},{"location":"api/handlers/data-sync/model/#aibs_informatics_aws_lambda.handlers.data_sync.model.WithDataPath","title":"WithDataPath  <code>dataclass</code>","text":"<pre><code>WithDataPath(path=custom_field(mm_field=(DataPathField())))\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Base class for models that contain a data path.</p> <p>Provides convenience properties for accessing the path as different types.</p> <p>Attributes:</p> Name Type Description <code>path</code> <code>DataPath</code> <p>The data path (S3, EFS, or local).</p>"},{"location":"api/handlers/data-sync/model/#aibs_informatics_aws_lambda.handlers.data_sync.model.WithDataPath.efs_path","title":"efs_path  <code>property</code>","text":"<pre><code>efs_path\n</code></pre> <p>Get the path as an EFS path if applicable.</p> <p>Returns:</p> Type Description <code>Optional[EFSPath]</code> <p>The EFS path or None if not an EFS path.</p>"},{"location":"api/handlers/data-sync/model/#aibs_informatics_aws_lambda.handlers.data_sync.model.WithDataPath.local_path","title":"local_path  <code>property</code>","text":"<pre><code>local_path\n</code></pre> <p>Get the path as a local path if applicable.</p> <p>Returns:</p> Type Description <code>Optional[Path]</code> <p>The local path or None if not a local path.</p>"},{"location":"api/handlers/data-sync/model/#aibs_informatics_aws_lambda.handlers.data_sync.model.WithDataPath.s3_uri","title":"s3_uri  <code>property</code>","text":"<pre><code>s3_uri\n</code></pre> <p>Get the path as an S3 URI if applicable.</p> <p>Returns:</p> Type Description <code>Optional[S3Path]</code> <p>The S3 path or None if not an S3 path.</p>"},{"location":"api/handlers/data-sync/operations/","title":"Data Sync Operations","text":"<p>Handlers for data synchronization operations.</p>"},{"location":"api/handlers/data-sync/operations/#handlers","title":"Handlers","text":"<ul> <li><code>GetJSONFromFileHandler</code> - Retrieves JSON data from a file</li> <li><code>PutJSONToFileHandler</code> - Writes JSON data to a file</li> <li><code>DataSyncHandler</code> - Simple data sync task</li> <li><code>BatchDataSyncHandler</code> - Handles batch of data sync tasks</li> <li><code>PrepareBatchDataSyncHandler</code> - Prepares batch data sync tasks</li> </ul> <p>Data synchronization operation handlers.</p> <p>Provides Lambda handlers for reading, writing, and synchronizing data between local file systems and S3.</p>"},{"location":"api/handlers/data-sync/operations/#aibs_informatics_aws_lambda.handlers.data_sync.operations.BatchDataSyncHandler","title":"BatchDataSyncHandler  <code>dataclass</code>","text":"<pre><code>BatchDataSyncHandler()\n</code></pre> <p>               Bases: <code>LambdaHandler[BatchDataSyncRequest, BatchDataSyncResponse]</code></p> <p>Handler for processing batches of data synchronization requests.</p> <p>Processes multiple sync requests sequentially, with optional partial failure handling.</p>"},{"location":"api/handlers/data-sync/operations/#aibs_informatics_aws_lambda.handlers.data_sync.operations.BatchDataSyncHandler.handle","title":"handle","text":"<pre><code>handle(request)\n</code></pre> <p>Process a batch of data sync requests.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>BatchDataSyncRequest</code> <p>Request containing a list of sync requests.</p> required <p>Returns:</p> Type Description <code>BatchDataSyncResponse</code> <p>Response containing aggregated results and any failed requests.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If a sync fails and allow_partial_failure is False.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/data_sync/operations.py</code> <pre><code>def handle(self, request: BatchDataSyncRequest) -&gt; BatchDataSyncResponse:\n    \"\"\"Process a batch of data sync requests.\n\n    Args:\n        request (BatchDataSyncRequest): Request containing a list of sync requests.\n\n    Returns:\n        Response containing aggregated results and any failed requests.\n\n    Raises:\n        Exception: If a sync fails and allow_partial_failure is False.\n    \"\"\"\n    self.logger.info(f\"Received {len(request.requests)} requests to transfer\")\n    if isinstance(request.requests, S3URI):\n        self.logger.info(f\"Request is stored at {request.requests}... fetching content.\")\n        _ = download_to_json(request.requests)\n        assert isinstance(_, list)\n        batch_requests = [DataSyncRequest.from_dict(__) for __ in _]\n    else:\n        batch_requests = request.requests\n\n    batch_result = BatchDataSyncResult()\n    response = BatchDataSyncResponse(result=batch_result, failed_requests=[])\n\n    for i, _ in enumerate(batch_requests):\n        sync_operations = DataSyncOperations(_)\n        self.logger.info(\n            f\"[{i + 1}/{len(batch_requests)}] \"\n            f\"Syncing content from {_.source_path} to {_.destination_path}\"\n        )\n        try:\n            result = sync_operations.sync(\n                source_path=_.source_path,\n                destination_path=_.destination_path,\n                source_path_prefix=_.source_path_prefix,\n            )\n            if result.bytes_transferred is not None:\n                batch_result.add_bytes_transferred(result.bytes_transferred)\n            if result.files_transferred is not None:\n                batch_result.add_files_transferred(result.files_transferred)\n            batch_result.increment_successful_requests_count()\n\n            if result.bytes_transferred:\n                result.add_bytes_transferred(result.bytes_transferred)\n        except Exception as e:\n            batch_result.increment_failed_requests_count()\n            response.add_failed_request(_)\n            self.logger.error(\n                f\"Failed to sync content from {_.source_path} to {_.destination_path}\"\n            )\n            self.logger.error(e)\n            if not request.allow_partial_failure:\n                raise e\n    return response\n</code></pre>"},{"location":"api/handlers/data-sync/operations/#aibs_informatics_aws_lambda.handlers.data_sync.operations.DataSyncHandler","title":"DataSyncHandler  <code>dataclass</code>","text":"<pre><code>DataSyncHandler()\n</code></pre> <p>               Bases: <code>LambdaHandler[DataSyncRequest, DataSyncResponse]</code></p> <p>Handler for synchronizing data between source and destination paths.</p> <p>Supports syncing between local file systems and S3.</p>"},{"location":"api/handlers/data-sync/operations/#aibs_informatics_aws_lambda.handlers.data_sync.operations.DataSyncHandler.handle","title":"handle","text":"<pre><code>handle(request)\n</code></pre> <p>Synchronize data from source to destination.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>DataSyncRequest</code> <p>Request containing source and destination paths.</p> required <p>Returns:</p> Type Description <code>DataSyncResponse</code> <p>Response containing the sync result.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/data_sync/operations.py</code> <pre><code>def handle(self, request: DataSyncRequest) -&gt; DataSyncResponse:\n    \"\"\"Synchronize data from source to destination.\n\n    Args:\n        request (DataSyncRequest): Request containing source and destination paths.\n\n    Returns:\n        Response containing the sync result.\n    \"\"\"\n    sync_operations = DataSyncOperations(request)\n    result = sync_operations.sync_task(request)\n    return DataSyncResponse(request=request, result=result)\n</code></pre>"},{"location":"api/handlers/data-sync/operations/#aibs_informatics_aws_lambda.handlers.data_sync.operations.GetJSONFromFileHandler","title":"GetJSONFromFileHandler  <code>dataclass</code>","text":"<pre><code>GetJSONFromFileHandler()\n</code></pre> <p>               Bases: <code>LambdaHandler[GetJSONFromFileRequest, GetJSONFromFileResponse]</code></p> <p>Handler for retrieving JSON content from a file.</p> <p>Supports loading JSON from both local files and S3 locations.</p>"},{"location":"api/handlers/data-sync/operations/#aibs_informatics_aws_lambda.handlers.data_sync.operations.GetJSONFromFileHandler.handle","title":"handle","text":"<pre><code>handle(request)\n</code></pre> <p>Load JSON content from the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>GetJSONFromFileRequest</code> <p>Request containing the file path.</p> required <p>Returns:</p> Type Description <code>GetJSONFromFileResponse</code> <p>Response containing the loaded JSON content.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If the content cannot be fetched.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/data_sync/operations.py</code> <pre><code>def handle(self, request: GetJSONFromFileRequest) -&gt; GetJSONFromFileResponse:\n    \"\"\"Load JSON content from the specified path.\n\n    Args:\n        request (GetJSONFromFileRequest): Request containing the file path.\n\n    Returns:\n        Response containing the loaded JSON content.\n\n    Raises:\n        Exception: If the content cannot be fetched.\n    \"\"\"\n    try:\n        path = request.path\n\n        self.logger.info(f\"Fetching content from {path}\")\n        if isinstance(path, S3URI):\n            self.logger.info(\"Downloading from S3\")\n            content = download_to_json(s3_path=path)\n        else:\n            self.logger.info(\"Loading from path\")\n            content = load_json(path)\n        return GetJSONFromFileResponse(content=content)\n    except Exception as e:\n        self.logger.error(f\"Could not fetch content from {request.path}\")\n        raise e\n</code></pre>"},{"location":"api/handlers/data-sync/operations/#aibs_informatics_aws_lambda.handlers.data_sync.operations.PrepareBatchDataSyncHandler","title":"PrepareBatchDataSyncHandler  <code>dataclass</code>","text":"<pre><code>PrepareBatchDataSyncHandler()\n</code></pre> <p>               Bases: <code>LambdaHandler[PrepareBatchDataSyncRequest, PrepareBatchDataSyncResponse]</code></p> <p>Handler for preparing batch data synchronization requests.</p> <p>Analyzes the source path and partitions files into optimally-sized batches for parallel processing using the bin-packing algorithm.</p>"},{"location":"api/handlers/data-sync/operations/#aibs_informatics_aws_lambda.handlers.data_sync.operations.PrepareBatchDataSyncHandler.build_node_batches","title":"build_node_batches  <code>classmethod</code>","text":"<pre><code>build_node_batches(nodes, batch_size_bytes_limit)\n</code></pre> <p>Batch nodes based on threshold</p> <p>This is a version of the classic \"Bin Packing\" problem. https://en.wikipedia.org/wiki/Bin_packing_problem</p> <p>The following solutions implements the First-fit decreasing algorithm.</p> Notes <ul> <li>nodes can have sizes greater than the limit</li> </ul> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>List[Node]</code> <p>List of nodes to batch.</p> required <code>batch_size_bytes_limit</code> <code>int</code> <p>Size limit in bytes for a batch of nodes.</p> required <p>Returns:</p> Type Description <code>List[List[Node]]</code> <p>List of node batches (list of lists).</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/data_sync/operations.py</code> <pre><code>@classmethod\ndef build_node_batches(\n    cls, nodes: List[Node], batch_size_bytes_limit: int\n) -&gt; List[List[Node]]:\n    \"\"\"Batch nodes based on threshold\n\n    This is a version of the classic \"Bin Packing\" problem.\n    https://en.wikipedia.org/wiki/Bin_packing_problem\n\n    The following solutions implements the First-fit decreasing algorithm.\n\n    Notes:\n        - nodes can have sizes greater than the limit\n\n    Args:\n        nodes (List[Node]): List of nodes to batch.\n        batch_size_bytes_limit (int): Size limit in bytes for a batch of nodes.\n\n    Returns:\n        List of node batches (list of lists).\n    \"\"\"\n\n    ## We will use a revised version of the bin packing problem:\n    # https://en.wikipedia.org/wiki/Bin_packing_problem\n\n    # Step 1:   and then sort the nodes by size (descending order)\n    unbatched_nodes = sorted(nodes, key=lambda node: node.size_bytes, reverse=True)\n\n    # Step 2:   Group nodes in order to maximize the data synced per request\n    #           (bin packing problem)\n    node_batches: List[List[Node]] = []\n\n    # (Optimize) Convert all nodes that are larger than the threshold into single requests.\n    while unbatched_nodes and unbatched_nodes[0].size_bytes &gt; batch_size_bytes_limit:\n        node_batches.append([unbatched_nodes.pop(0)])\n\n    # Compute the batch node sizes (they should all be larger than the)\n    node_batch_sizes = [\n        sum([node.size_bytes for node in node_batch]) for node_batch in node_batches\n    ]\n\n    for node in unbatched_nodes:\n        for i in range(len(node_batch_sizes)):\n            if node_batch_sizes[i] + node.size_bytes &lt;= batch_size_bytes_limit:\n                node_batch_sizes[i] += node.size_bytes\n                node_batches[i].append(node)\n                break\n        else:\n            node_batch_sizes.append(node.size_bytes)\n            node_batches.append([node])\n\n    return node_batches\n</code></pre>"},{"location":"api/handlers/data-sync/operations/#aibs_informatics_aws_lambda.handlers.data_sync.operations.PrepareBatchDataSyncHandler.handle","title":"handle","text":"<pre><code>handle(request)\n</code></pre> <p>Prepare batch data sync requests.</p> <p>Partitions the source data into optimally-sized batches.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>PrepareBatchDataSyncRequest</code> <p>Request containing source path and configuration.</p> required <p>Returns:</p> Type Description <code>PrepareBatchDataSyncResponse</code> <p>Response containing prepared batch requests.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/data_sync/operations.py</code> <pre><code>def handle(self, request: PrepareBatchDataSyncRequest) -&gt; PrepareBatchDataSyncResponse:\n    \"\"\"Prepare batch data sync requests.\n\n    Partitions the source data into optimally-sized batches.\n\n    Args:\n        request (PrepareBatchDataSyncRequest):\n            Request containing source path and configuration.\n\n    Returns:\n        Response containing prepared batch requests.\n    \"\"\"\n    self.logger.info(\"Preparing S3 Batch Sync Requests\")\n    root: Union[S3FileSystem, LocalFileSystem]\n    if isinstance(request.source_path, S3URI):\n        root = S3FileSystem.from_path(request.source_path)\n    else:\n        root = LocalFileSystem.from_path(request.source_path)\n    batch_size_bytes_limit = request.batch_size_bytes_limit or self.DEFAULT_SOFT_MAX_BYTES\n\n    ## We will use a revised version of the bin packing problem:\n    # https://en.wikipedia.org/wiki/Bin_packing_problem\n\n    # Step 1A: Partition nodes s.t. we deal with fewer paths in total.\n    self.logger.info(f\"Partitioning batch size bytes limit: {batch_size_bytes_limit}\")\n    nodes = root.partition(size_bytes_limit=batch_size_bytes_limit)\n\n    batch_data_sync_requests: List[BatchDataSyncRequest] = []\n\n    node_batches = self.build_node_batches(nodes, batch_size_bytes_limit)\n    self.logger.info(f\"Batched {len(nodes)} nodes into {len(node_batches)} batches\")\n    for node_batch in node_batches:\n        data_sync_requests: List[DataSyncRequest] = []\n        for node in sorted(node_batch):\n            data_sync_requests.append(\n                DataSyncRequest(\n                    source_path=self.build_source_path(request, node),\n                    destination_path=self.build_destination_path(request, node),\n                    source_path_prefix=request.source_path_prefix,\n                    max_concurrency=request.max_concurrency,\n                    retain_source_data=request.retain_source_data,\n                    require_lock=request.require_lock,\n                    force=request.force,\n                    size_only=request.size_only,\n                    fail_if_missing=request.fail_if_missing,\n                    include_detailed_response=request.include_detailed_response,\n                    remote_to_local_config=request.remote_to_local_config,\n                )\n            )\n        batch_data_sync_requests.append(BatchDataSyncRequest(requests=data_sync_requests))\n\n    if request.temporary_request_payload_path:\n        self.logger.info(\n            f\"Uploading batch requests to {request.temporary_request_payload_path}\"\n        )\n        new_batch_data_sync_requests = []\n\n        for i, batch_data_sync_request in enumerate(batch_data_sync_requests):\n            upload_json(\n                [cast(DataSyncRequest, _).to_dict() for _ in batch_data_sync_request.requests],\n                s3_path=(\n                    s3_path := request.temporary_request_payload_path / f\"request_{i}.json\"\n                ),\n            )\n            new_batch_data_sync_requests.append(BatchDataSyncRequest(requests=s3_path))\n        return PrepareBatchDataSyncResponse(requests=new_batch_data_sync_requests)\n    else:\n        return PrepareBatchDataSyncResponse(requests=batch_data_sync_requests)\n</code></pre>"},{"location":"api/handlers/data-sync/operations/#aibs_informatics_aws_lambda.handlers.data_sync.operations.PutJSONToFileHandler","title":"PutJSONToFileHandler  <code>dataclass</code>","text":"<pre><code>PutJSONToFileHandler()\n</code></pre> <p>               Bases: <code>LambdaHandler[PutJSONToFileRequest, PutJSONToFileResponse]</code></p> <p>Handler for writing JSON content to a file.</p> <p>Supports writing to both local files and S3 locations. If no path is provided, generates a scratch S3 path.</p>"},{"location":"api/handlers/data-sync/operations/#aibs_informatics_aws_lambda.handlers.data_sync.operations.PutJSONToFileHandler.handle","title":"handle","text":"<pre><code>handle(request)\n</code></pre> <p>Write JSON content to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>PutJSONToFileRequest</code> <p>Request containing the content and optional path.</p> required <p>Returns:</p> Type Description <code>Optional[PutJSONToFileResponse]</code> <p>Response containing the path where content was written.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no path is provided and bucket name cannot be inferred.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/data_sync/operations.py</code> <pre><code>def handle(self, request: PutJSONToFileRequest) -&gt; Optional[PutJSONToFileResponse]:\n    \"\"\"Write JSON content to the specified path.\n\n    Args:\n        request (PutJSONToFileRequest): Request containing the content and optional path.\n\n    Returns:\n        Response containing the path where content was written.\n\n    Raises:\n        ValueError: If no path is provided and bucket name cannot be inferred.\n    \"\"\"\n    path, content = request.path, request.content\n\n    if path is None:\n        bucket_name = get_env_var(DEFAULT_BUCKET_NAME_ENV_VAR, default_value=None)\n        if bucket_name is None:\n            raise ValueError(\n                \"No path provided and Could not infer bucket \"\n                f\"name from {DEFAULT_BUCKET_NAME_ENV_VAR} environment variable\"\n            )\n        path = S3URI.build(\n            bucket_name=bucket_name,\n            key=get_s3_scratch_key(\n                content=content,\n                unique_id=UniqueID(self.context.aws_request_id),\n            ),\n        )\n\n    self.logger.info(f\"Writing content to {path}\")\n    self.logger.info(f\"Content to write: {content}\")\n    if isinstance(path, S3URI):\n        self.logger.info(\"Uploading to S3\")\n        upload_json(content, s3_path=path, extra_args=SCRATCH_EXTRA_ARGS)\n    else:\n        self.logger.info(\"Writing to file\")\n        path.parent.mkdir(parents=True, exist_ok=True)\n        path.write_text(json.dumps(content, indent=4, sort_keys=True))\n    return PutJSONToFileResponse(path=path)\n</code></pre>"},{"location":"api/handlers/data-sync/operations/#aibs_informatics_aws_lambda.handlers.data_sync.operations.get_s3_scratch_key","title":"get_s3_scratch_key","text":"<pre><code>get_s3_scratch_key(\n    filename=None, content=None, unique_id=None\n)\n</code></pre> <p>Generates a scratch file s3 key</p> <p>The key is constructed from filename, content and unique ID.</p> <p>If filename is not provided, a hexdigest is created from content (which will be random if content is None).</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Optional[str]</code> <p>Optional name of file. If None, file hash is generated.</p> <code>None</code> <code>content</code> <code>Optional[JSON]</code> <p>Optional content of file to put. Only used if filename is not provided. Defaults to None.</p> <code>None</code> <code>unique_id</code> <code>Optional[UniqueID]</code> <p>A unique ID used in key generation. If None, a random UUID is generated.</p> <code>None</code> <p>Returns:</p> Type Description <code>S3Key</code> <p>S3 Scratch key (not gauranteed to be empty)</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/data_sync/operations.py</code> <pre><code>def get_s3_scratch_key(\n    filename: Optional[str] = None,\n    content: Optional[JSON] = None,\n    unique_id: Optional[UniqueID] = None,\n) -&gt; S3Key:\n    \"\"\"Generates a scratch file s3 key\n\n    The key is constructed from filename, content and unique ID.\n\n    If filename is not provided, a hexdigest is created from content (which will\n    be random if content is None).\n\n    Args:\n        filename (Optional[str]): Optional name of file.\n            If None, file hash is generated.\n        content (Optional[JSON]): Optional content of file to put.\n            Only used if filename is not provided. Defaults to None.\n        unique_id (Optional[UniqueID]): A unique ID used in key generation.\n            If None, a random UUID is generated.\n\n    Returns:\n        S3 Scratch key (not gauranteed to be empty)\n    \"\"\"\n    file_hash = filename or sha256_hexdigest(content=content)\n    return S3Key(f\"scratch/{unique_id or UniqueID.create()}/{file_hash}\")\n</code></pre>"},{"location":"api/handlers/demand/","title":"Demand Handlers","text":"<p>Handlers for demand execution scaffolding.</p>"},{"location":"api/handlers/demand/#overview","title":"Overview","text":"Module Description Scaffolding <code>PrepareDemandScaffoldingHandler</code> for preparing demand execution Context Manager Context management utilities Model Data models for demand operations"},{"location":"api/handlers/demand/#quick-start","title":"Quick Start","text":"<pre><code>from aibs_informatics_aws_lambda.handlers.demand.scaffolding import PrepareDemandScaffoldingHandler\n\nhandler = PrepareDemandScaffoldingHandler().get_handler()\n</code></pre>"},{"location":"api/handlers/demand/context-manager/","title":"Demand Context Manager","text":"<p>Context management utilities for demand execution.</p> <p>Demand execution context management.</p> <p>Provides context managers and utilities for setting up and managing demand execution environments including EFS volumes, batch jobs, and data synchronization.</p>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.BatchEFSConfiguration","title":"BatchEFSConfiguration  <code>dataclass</code>","text":"<pre><code>BatchEFSConfiguration(mount_point_config, read_only=False)\n</code></pre> <p>Configuration for mounting an EFS volume in AWS Batch.</p> <p>Encapsulates the mount point configuration and generates the necessary AWS Batch volume and mount point type definitions.</p> <p>Attributes:</p> Name Type Description <code>mount_point_config</code> <code>MountPointConfiguration</code> <p>The underlying mount point configuration.</p> <code>read_only</code> <code>bool</code> <p>Whether the mount should be read-only.</p> <code>mount_point</code> <code>MountPointTypeDef</code> <p>Generated AWS Batch mount point type definition.</p> <code>volume</code> <code>VolumeTypeDef</code> <p>Generated AWS Batch volume type definition.</p> Example <pre><code>config = BatchEFSConfiguration.build(\n    access_point=\"fsap-12345\",\n    mount_path=\"/opt/efs/scratch\",\n    read_only=False\n)\n</code></pre>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.BatchEFSConfiguration.mount_path","title":"mount_path  <code>property</code>","text":"<pre><code>mount_path\n</code></pre> <p>Get the mount path for this configuration.</p> <p>Returns:</p> Type Description <code>Path</code> <p>The container path where this volume is mounted.</p>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.BatchEFSConfiguration.build","title":"build  <code>classmethod</code>","text":"<pre><code>build(access_point, mount_path, read_only=False)\n</code></pre> <p>Build a BatchEFSConfiguration from an access point.</p> <p>Parameters:</p> Name Type Description Default <code>access_point</code> <code>str</code> <p>The EFS access point ID.</p> required <code>mount_path</code> <code>Union[Path, str]</code> <p>Path where the volume will be mounted.</p> required <code>read_only</code> <code>bool</code> <p>Whether the mount should be read-only.</p> <code>False</code> <p>Returns:</p> Type Description <p>Configured BatchEFSConfiguration instance.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/demand/context_manager.py</code> <pre><code>@classmethod\ndef build(cls, access_point: str, mount_path: Union[Path, str], read_only: bool = False):\n    \"\"\"Build a BatchEFSConfiguration from an access point.\n\n    Args:\n        access_point (str): The EFS access point ID.\n        mount_path (Union[Path, str]): Path where the volume will be mounted.\n        read_only (bool): Whether the mount should be read-only.\n\n    Returns:\n        Configured BatchEFSConfiguration instance.\n    \"\"\"\n    mount_point_config = MountPointConfiguration.build(\n        mount_point=mount_path,\n        access_point=access_point,\n    )\n    return BatchEFSConfiguration(mount_point_config=mount_point_config, read_only=read_only)\n</code></pre>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.DemandExecutionContextManager","title":"DemandExecutionContextManager  <code>dataclass</code>","text":"<pre><code>DemandExecutionContextManager(\n    demand_execution,\n    scratch_vol_configuration,\n    shared_vol_configuration,\n    tmp_vol_configuration=None,\n    configuration=ContextManagerConfiguration(),\n    env_base=EnvBase.from_env(),\n)\n</code></pre> <p>Manages the context and configuration for demand executions.</p> <p>Coordinates EFS volume configurations, data synchronization requests, and AWS Batch job building for demand executions.</p> <p>This class handles: - Path resolution between container and EFS paths - Pre-execution data sync setup (inputs) - Post-execution data sync setup (outputs) - Batch job builder configuration - Working directory and cleanup management</p> <p>Attributes:</p> Name Type Description <code>demand_execution</code> <code>DemandExecution</code> <p>The demand execution to manage.</p> <code>scratch_vol_configuration</code> <code>BatchEFSConfiguration</code> <p>EFS configuration for scratch storage.</p> <code>shared_vol_configuration</code> <code>BatchEFSConfiguration</code> <p>EFS configuration for shared/input storage.</p> <code>tmp_vol_configuration</code> <code>Optional[BatchEFSConfiguration]</code> <p>Optional EFS configuration for temp storage.</p> <code>configuration</code> <code>ContextManagerConfiguration</code> <p>Context manager configuration options.</p> <code>env_base</code> <code>EnvBase</code> <p>Environment base for resource naming.</p> Example <pre><code>context_manager = DemandExecutionContextManager.from_demand_execution(\n    demand_execution=execution,\n    env_base=env_base,\n)\nbatch_builder = context_manager.batch_job_builder\n</code></pre>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.DemandExecutionContextManager.batch_job_builder","title":"batch_job_builder  <code>property</code>","text":"<pre><code>batch_job_builder\n</code></pre> <p>Get or create the batch job builder for this execution.</p> <p>Lazily creates the BatchJobBuilder on first access with all necessary configurations.</p> <p>Returns:</p> Type Description <code>BatchJobBuilder</code> <p>Configured BatchJobBuilder instance.</p>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.DemandExecutionContextManager.batch_job_queue_name","title":"batch_job_queue_name  <code>property</code>","text":"<pre><code>batch_job_queue_name\n</code></pre> <p>Get the batch job queue name for this execution.</p> <p>Returns:</p> Type Description <code>str</code> <p>The AWS Batch job queue name.</p>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.DemandExecutionContextManager.container_shared_path","title":"container_shared_path  <code>property</code>","text":"<pre><code>container_shared_path\n</code></pre> <p>Returns the container path for the shared volume</p> Example <p>/opt/efs/shared</p> <p>Returns:</p> Type Description <code>Path</code> <p>container path for shared volume</p>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.DemandExecutionContextManager.container_tmp_path","title":"container_tmp_path  <code>property</code>","text":"<pre><code>container_tmp_path\n</code></pre> <p>Returns the container path for the tmp volume</p> Example <p>/opt/efs/scratch/tmp</p> <p>Returns:</p> Type Description <code>Path</code> <p>container path for tmp volume</p>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.DemandExecutionContextManager.container_working_path","title":"container_working_path  <code>property</code>","text":"<pre><code>container_working_path\n</code></pre> <p>Returns the container path for the working data path for the demand execution</p> Example <p>/opt/efs/scratch/{EXECUTION_ID}</p> <p>Returns:</p> Type Description <code>Path</code> <p>container path for working data path directory</p>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.DemandExecutionContextManager.efs_mount_points","title":"efs_mount_points  <code>property</code>","text":"<pre><code>efs_mount_points\n</code></pre> <p>Returns a list of mount points for the EFS volumes used by the aws batch job</p> <p>Returns:</p> Type Description <code>List[MountPointConfiguration]</code> <p>list of mount point configurations</p>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.DemandExecutionContextManager.efs_shared_path","title":"efs_shared_path  <code>property</code>","text":"<pre><code>efs_shared_path\n</code></pre> <p>Returns the global EFS path for shared (inputs) data path</p> Example <p>efs://fs-12345678:/shared</p> <p>Returns:</p> Type Description <code>EFSPath</code> <p>EFS URI for shared data path directory</p>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.DemandExecutionContextManager.efs_tmp_path","title":"efs_tmp_path  <code>property</code>","text":"<pre><code>efs_tmp_path\n</code></pre> <p>Returns the global EFS path for tmp data path (scratch)</p> Example <p>efs://fs-12345678:/scratch/tmp</p> <p>Returns:</p> Type Description <code>EFSPath</code> <p>EFS URI for tmp data path directory</p>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.DemandExecutionContextManager.efs_working_path","title":"efs_working_path  <code>property</code>","text":"<pre><code>efs_working_path\n</code></pre> <p>Returns the global EFS path for working data path for the demand execution</p> Example <p>efs://fs-12345678:/scratch/{EXECUTION_ID}</p> <p>Returns:</p> Type Description <code>EFSPath</code> <p>EFS URI for working data path directory</p>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.DemandExecutionContextManager.post_execution_data_sync_requests","title":"post_execution_data_sync_requests  <code>property</code>","text":"<pre><code>post_execution_data_sync_requests\n</code></pre> <p>Generate data sync requests for post-execution output upload.</p> <p>Creates requests to sync output data from EFS to S3 after the batch job completes.</p> <p>Returns:</p> Type Description <code>List[PrepareBatchDataSyncRequest]</code> <p>List of data sync requests for output data.</p>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.DemandExecutionContextManager.post_execution_remove_data_paths_requests","title":"post_execution_remove_data_paths_requests  <code>property</code>","text":"<pre><code>post_execution_remove_data_paths_requests\n</code></pre> <p>Generates remove data paths requests for post-execution data sync</p> <p>Returns:</p> Type Description <code>List[RemoveDataPathsRequest]</code> <p>list of remove data paths requests</p>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.DemandExecutionContextManager.pre_execution_data_sync_requests","title":"pre_execution_data_sync_requests  <code>property</code>","text":"<pre><code>pre_execution_data_sync_requests\n</code></pre> <p>Generate data sync requests for pre-execution input staging.</p> <p>Creates requests to sync input data from S3 to EFS before the batch job runs.</p> <p>Returns:</p> Type Description <code>List[PrepareBatchDataSyncRequest]</code> <p>List of data sync requests for input data.</p>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.DemandExecutionContextManager.from_demand_execution","title":"from_demand_execution  <code>classmethod</code>","text":"<pre><code>from_demand_execution(\n    demand_execution, env_base, configuration=None\n)\n</code></pre> <p>Create a context manager from a demand execution.</p> <p>Factory method that sets up default EFS configurations and creates the context manager.</p> <p>Parameters:</p> Name Type Description Default <code>demand_execution</code> <code>DemandExecution</code> <p>The demand execution to manage.</p> required <code>env_base</code> <code>EnvBase</code> <p>Environment base for resource resolution.</p> required <code>configuration</code> <code>Optional[ContextManagerConfiguration]</code> <p>Optional context manager configuration.</p> <code>None</code> <p>Returns:</p> Type Description <p>Configured DemandExecutionContextManager instance.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/demand/context_manager.py</code> <pre><code>@classmethod\ndef from_demand_execution(\n    cls,\n    demand_execution: DemandExecution,\n    env_base: EnvBase,\n    configuration: Optional[ContextManagerConfiguration] = None,\n):\n    \"\"\"Create a context manager from a demand execution.\n\n    Factory method that sets up default EFS configurations and\n    creates the context manager.\n\n    Args:\n        demand_execution (DemandExecution): The demand execution to manage.\n        env_base (EnvBase): Environment base for resource resolution.\n        configuration (Optional[ContextManagerConfiguration]): Optional context\n            manager configuration.\n\n    Returns:\n        Configured DemandExecutionContextManager instance.\n    \"\"\"\n    vol_configuration = get_batch_efs_configuration(\n        env_base=env_base,\n        container_path=f\"/opt/efs{EFS_SCRATCH_PATH}\",\n        access_point_name=EFS_SCRATCH_ACCESS_POINT_NAME,\n        read_only=False,\n    )\n    shared_vol_configuration = get_batch_efs_configuration(\n        env_base=env_base,\n        container_path=f\"/opt/efs{EFS_SHARED_PATH}\",\n        access_point_name=EFS_SHARED_ACCESS_POINT_NAME,\n        read_only=True,\n    )\n    tmp_vol_configuration = None\n\n    logger.info(f\"Using following efs configuration: {vol_configuration}\")\n    return DemandExecutionContextManager(\n        demand_execution=demand_execution,\n        scratch_vol_configuration=vol_configuration,\n        shared_vol_configuration=shared_vol_configuration,\n        tmp_vol_configuration=tmp_vol_configuration,\n        configuration=configuration or ContextManagerConfiguration(),\n        env_base=env_base,\n    )\n</code></pre>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.generate_batch_job_builder","title":"generate_batch_job_builder","text":"<pre><code>generate_batch_job_builder(\n    demand_execution,\n    env_base,\n    working_path,\n    tmp_path,\n    scratch_mount_point,\n    shared_mount_point,\n    tmp_mount_point=None,\n    env_file_write_mode=EnvFileWriteMode.ALWAYS,\n)\n</code></pre> <p>Generate a BatchJobBuilder for the demand execution.</p> <p>Creates a fully configured BatchJobBuilder with: - Command construction with pre-commands for setup - Environment variable handling (direct or via env file) - Volume and mount point configurations - Resource requirements from demand execution</p> <p>Parameters:</p> Name Type Description Default <code>demand_execution</code> <code>DemandExecution</code> <p>The demand execution to build a job for.</p> required <code>env_base</code> <code>EnvBase</code> <p>Environment base for resource naming.</p> required <code>working_path</code> <code>EFSPath</code> <p>EFS path for the working directory.</p> required <code>tmp_path</code> <code>EFSPath</code> <p>EFS path for temporary files.</p> required <code>scratch_mount_point</code> <code>MountPointConfiguration</code> <p>Mount configuration for scratch volume.</p> required <code>shared_mount_point</code> <code>MountPointConfiguration</code> <p>Mount configuration for shared volume.</p> required <code>tmp_mount_point</code> <code>Optional[MountPointConfiguration]</code> <p>Optional mount configuration for tmp volume.</p> <code>None</code> <code>env_file_write_mode</code> <code>EnvFileWriteMode</code> <p>How to handle environment variable files.</p> <code>ALWAYS</code> <p>Returns:</p> Type Description <code>BatchJobBuilder</code> <p>Configured BatchJobBuilder ready for job submission.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no command is specified in the demand execution.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/demand/context_manager.py</code> <pre><code>def generate_batch_job_builder(  # noqa: C901\n    demand_execution: DemandExecution,\n    env_base: EnvBase,\n    working_path: EFSPath,\n    tmp_path: EFSPath,\n    scratch_mount_point: MountPointConfiguration,\n    shared_mount_point: MountPointConfiguration,\n    tmp_mount_point: Optional[MountPointConfiguration] = None,\n    env_file_write_mode: EnvFileWriteMode = EnvFileWriteMode.ALWAYS,\n) -&gt; BatchJobBuilder:\n    \"\"\"Generate a BatchJobBuilder for the demand execution.\n\n    Creates a fully configured BatchJobBuilder with:\n    - Command construction with pre-commands for setup\n    - Environment variable handling (direct or via env file)\n    - Volume and mount point configurations\n    - Resource requirements from demand execution\n\n    Args:\n        demand_execution (DemandExecution): The demand execution to build a job for.\n        env_base (EnvBase): Environment base for resource naming.\n        working_path (EFSPath): EFS path for the working directory.\n        tmp_path (EFSPath): EFS path for temporary files.\n        scratch_mount_point (MountPointConfiguration): Mount configuration for scratch volume.\n        shared_mount_point (MountPointConfiguration): Mount configuration for shared volume.\n        tmp_mount_point (Optional[MountPointConfiguration]): Optional mount configuration\n            for tmp volume.\n        env_file_write_mode (EnvFileWriteMode): How to handle environment variable files.\n\n    Returns:\n        Configured BatchJobBuilder ready for job submission.\n\n    Raises:\n        ValueError: If no command is specified in the demand execution.\n    \"\"\"\n    logger.info(\"Constructing BatchJobBuilder instance\")\n\n    demand_execution = demand_execution.copy()\n    efs_mount_points = [scratch_mount_point, shared_mount_point]\n    if tmp_mount_point is not None:\n        efs_mount_points.append(tmp_mount_point)\n    logger.info(f\"Resolving local paths of working dir = {working_path} and tmp dir = {tmp_path}\")\n    container_working_path = get_local_path(working_path, mount_points=efs_mount_points)\n    container_tmp_path = get_local_path(tmp_path, mount_points=efs_mount_points)\n\n    logger.info(f\"Setting container working directory = {container_working_path}\")\n\n    EXECUTION_ID_VAR = \"EXECUTION_ID\"\n    WORKING_DIR_VAR = \"WORKING_DIR\"\n    TMPDIR_VAR = \"TMPDIR\"\n\n    environment: Dict[str, str] = {\n        EXECUTION_ID_VAR: demand_execution.execution_id,\n        WORKING_DIR_VAR: f\"{container_working_path}\",\n        TMPDIR_VAR: f\"{container_tmp_path}\",\n    }\n\n    for job_param in demand_execution.execution_parameters.job_params:\n        job_param.update_environment(environment)\n\n    logger.info(f\"Environment updated with {len(environment)} environment variables.\")\n\n    pre_commands = [\n        f\"mkdir -p ${{{WORKING_DIR_VAR}}}\".split(\" \"),\n        f\"mkdir -p ${{{TMPDIR_VAR}}}\".split(\" \"),\n        f\"cd ${{{WORKING_DIR_VAR}}}\".split(\" \"),\n    ]\n\n    logger.info(f\"Initial pre-commands: {pre_commands}\")\n\n    command = deepcopy(demand_execution.execution_parameters.command)\n    if not command:\n        logger.warning(\"No command specified, trying to resolve from manifest\")\n        # TODO: add logic to resolve default command from manifest\n        raise ValueError(\"Must specify command for demand execution\")\n\n    logger.info(f\"Command extracted from demand execution: {command}\")\n\n    # ------------------------------------------------------------------\n    ##  Environment File Conditional Configuration Logic\n    #\n    # This step tries to write the environment variables to a file that will be mounted\n    # to the container at runtime. This only works if the local machine that runs this code\n    # has access to EFS file system.\n    #\n    logger.info(\n        \"Attempting to create environment file for environment variables. \"\n        f\"Using {working_path} directory to write file.\"\n    )\n    # Here we define three path variables. They point to env file from the perspective of:\n    #   1. The container path\n    #   2. An EFS URI (pointing to location on EFS file system)\n    #   3. The (future) path on the local machine running this code.\n    container_environment_file = container_working_path / \".demand.env\"\n    efs_environment_file_uri = get_efs_path(\n        container_environment_file, mount_points=efs_mount_points\n    )\n    local_environment_file = get_local_path(efs_environment_file_uri, raise_if_unmounted=False)\n\n    # If the local environment file is not None, then the file is writable from this local machine\n    # We will now write a portion of environment variables to files that can be written.\n    if local_environment_file is None or env_file_write_mode == EnvFileWriteMode.NEVER:\n        # If the environment file cannot be written to, then the environment variables are\n        # passed directly to the container. This is a fallback option and will fail if the\n        # environment variables are too long.\n        if local_environment_file is None:\n            reason = f\"Could not write environment variables to file {efs_environment_file_uri}.\"\n        else:\n            reason = \"Environment file write mode set to NEVER.\"\n\n        logger.warning(\n            f\"{reason} Environment variables will be passed directly to the container. \"\n            \"THIS MAY CAUSE THE CONTAINER TO FAIL IF THE ENVIRONMENT VARIABLES \"\n            \"ARE LONGER THAN 8192 CHARACTERS!!!\"\n        )\n\n    else:\n        if env_file_write_mode == EnvFileWriteMode.IF_REQUIRED:\n            env_size = sum([sys.getsizeof(k) + sys.getsizeof(v) for k, v in environment.items()])\n\n            if env_size &gt; 8192 * 0.9:\n                logger.info(\n                    f\"Environment variables are too large to pass directly to container \"\n                    \"(&gt; 90% of 8192). Writing environment variables to file \"\n                    f\"{efs_environment_file_uri}.\"\n                )\n                confirm_write = True\n            else:\n                confirm_write = False\n        elif env_file_write_mode == EnvFileWriteMode.ALWAYS:\n            logger.info(f\"Writing environment variables to file {efs_environment_file_uri}.\")\n            confirm_write = True\n\n        if confirm_write:\n            # Steps for writing environment variables to file:\n            #   1. Identify all environment variables that are not referenced in the command\n            #       if not referenced, then add to environment file.\n            #   2. Write environment file\n            #   3. Add environment file to command\n            ENVIRONMENT_FILE_VAR = \"_ENVIRONMENT_FILE\"\n\n            # Step 1:, split environment variables based on reference are referenced in the command\n            writable_environment = environment.copy()\n            required_environment: Dict[str, str] = {}\n            for arg in command + [_ for c in pre_commands for _ in c]:\n                for match in re.findall(r\"\\$\\{?([\\w]+)\\}?\", arg):\n                    if match in writable_environment:\n                        required_environment[match] = writable_environment.pop(match)\n\n            # Add the environment file variable to the required environment variables\n            environment = required_environment.copy()\n            environment[ENVIRONMENT_FILE_VAR] = container_environment_file.as_posix()\n\n            # Step 2: write to the environment file\n            local_environment_file.parent.mkdir(parents=True, exist_ok=True)\n            write_env_file(writable_environment, local_environment_file)\n\n            # Finally, add the environment file to the command\n            pre_commands.append(f\". ${{{ENVIRONMENT_FILE_VAR}}}\".split(\" \"))\n\n    # ------------------------------------------------------------------\n\n    command_string = \" &amp;&amp; \".join([\" \".join(_) for _ in pre_commands + [command]])\n    logger.info(f\"Final command string created: '{command_string}'\")\n    vol_configurations = [\n        BatchEFSConfiguration(scratch_mount_point, read_only=False),\n        BatchEFSConfiguration(shared_mount_point, read_only=True),\n    ]\n    if tmp_mount_point:\n        vol_configurations.append(BatchEFSConfiguration(tmp_mount_point, read_only=False))\n    logger.info(\"Constructing BatchJobBuilder instance...\")\n    assert demand_execution.execution_platform.aws_batch is not None\n    return BatchJobBuilder(\n        image=demand_execution.execution_image,\n        job_definition_name=env_base.get_job_name(\n            demand_execution.execution_type, demand_execution.get_execution_hash(False)\n        ),\n        job_name=env_base.get_job_name(\n            demand_execution.execution_type, demand_execution.get_execution_hash(True)\n        ),\n        command=[\"/bin/bash\", \"-c\", command_string],\n        environment=environment,\n        job_definition_tags={\"USER\": demand_execution.execution_metadata.user or \"unknown\"},\n        resource_requirements=to_resource_requirements(\n            gpu=demand_execution.resource_requirements.gpu,\n            memory=demand_execution.resource_requirements.memory,\n            vcpus=demand_execution.resource_requirements.vcpus,\n        ),\n        mount_points=[_.mount_point for _ in vol_configurations],\n        volumes=[_.volume for _ in vol_configurations],\n        env_base=env_base,\n        # TODO: need to make this configurable\n        privileged=True,\n        job_role_arn=demand_execution.execution_platform.aws_batch.job_role\n        if demand_execution.execution_platform.aws_batch\n        else None,\n    )\n</code></pre>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.get_batch_efs_configuration","title":"get_batch_efs_configuration","text":"<pre><code>get_batch_efs_configuration(\n    env_base,\n    container_path,\n    access_point_name,\n    file_system_name=None,\n    read_only=False,\n)\n</code></pre> <p>Get a BatchEFSConfiguration by resolving resources from AWS.</p> <p>Resolves EFS file system and access point by name/tags and creates the corresponding batch configuration.</p> <p>Parameters:</p> Name Type Description Default <code>env_base</code> <code>EnvBase</code> <p>Environment base for tag-based resource resolution.</p> required <code>container_path</code> <code>str</code> <p>Path where the volume will be mounted.</p> required <code>access_point_name</code> <code>str</code> <p>Name of the EFS access point.</p> required <code>file_system_name</code> <code>Optional[str]</code> <p>Optional file system name to resolve.</p> <code>None</code> <code>read_only</code> <code>bool</code> <p>Whether the mount should be read-only.</p> <code>False</code> <p>Returns:</p> Type Description <code>BatchEFSConfiguration</code> <p>Configured BatchEFSConfiguration instance.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/demand/context_manager.py</code> <pre><code>def get_batch_efs_configuration(\n    env_base: EnvBase,\n    container_path: str,\n    access_point_name: str,\n    file_system_name: Optional[str] = None,\n    read_only: bool = False,\n) -&gt; BatchEFSConfiguration:\n    \"\"\"Get a BatchEFSConfiguration by resolving resources from AWS.\n\n    Resolves EFS file system and access point by name/tags and\n    creates the corresponding batch configuration.\n\n    Args:\n        env_base (EnvBase): Environment base for tag-based resource resolution.\n        container_path (str): Path where the volume will be mounted.\n        access_point_name (str): Name of the EFS access point.\n        file_system_name (Optional[str]): Optional file system name to resolve.\n        read_only (bool): Whether the mount should be read-only.\n\n    Returns:\n        Configured BatchEFSConfiguration instance.\n    \"\"\"\n    # TODO: add support for file_system_name (learn how to resolve file system name)\n    if file_system_name:\n        file_system_name = env_base.get_resource_name(file_system_name)\n        file_system = get_efs_file_system(name=file_system_name, tags={\"env_base\": env_base})\n        file_system_id = file_system[\"FileSystemId\"]\n        logger.info(\n            f\"Using file system {file_system_id} with name {file_system_name}. \"\n            f\"Will search for access point {access_point_name}.\"\n        )\n    else:\n        logger.info(\n            f\"No file system name provided. \"\n            f\"Will search for access point {access_point_name} directly.\"\n        )\n        file_system_id = None\n\n    access_point = get_efs_access_point(\n        file_system_id=file_system_id,\n        access_point_name=access_point_name,\n        access_point_tags={\"env_base\": env_base},\n    )\n    logger.info(f\"Using access point {access_point_name}\")\n    return BatchEFSConfiguration.build(\n        access_point=access_point[\"AccessPointId\"],\n        mount_path=container_path,\n        read_only=read_only,\n    )\n</code></pre>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.get_batch_job_queue_name","title":"get_batch_job_queue_name","text":"<pre><code>get_batch_job_queue_name(demand_execution)\n</code></pre> <p>Get the batch job queue name from a demand execution.</p> <p>Parameters:</p> Name Type Description Default <code>demand_execution</code> <code>DemandExecution</code> <p>The demand execution to get the queue name from.</p> required <p>Returns:</p> Type Description <p>The AWS Batch job queue name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the demand execution lacks an AWS Batch platform.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/demand/context_manager.py</code> <pre><code>def get_batch_job_queue_name(demand_execution: DemandExecution):\n    \"\"\"Get the batch job queue name from a demand execution.\n\n    Args:\n        demand_execution (DemandExecution): The demand execution to get the queue name from.\n\n    Returns:\n        The AWS Batch job queue name.\n\n    Raises:\n        ValueError: If the demand execution lacks an AWS Batch platform.\n    \"\"\"\n    aws_batch_exec_platform = demand_execution.execution_platform.aws_batch\n    if aws_batch_exec_platform is None:\n        raise ValueError(\"Demand execution does not have an AWS Batch execution platform\")\n    return aws_batch_exec_platform.job_queue_name\n</code></pre>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.update_demand_execution_parameter_inputs","title":"update_demand_execution_parameter_inputs","text":"<pre><code>update_demand_execution_parameter_inputs(\n    demand_execution,\n    container_shared_path,\n    container_working_path,\n    isolate_inputs=False,\n)\n</code></pre> <p>Modifies demand execution input destinations with the location of the volume configuration</p> <p>This updates the input destinations to a deterministic location under the volume configuration specified. This ensures that inputs shared between jobs can used the same cached results.</p> The structure of the path for any input param is comprised of <ul> <li>volume's mount_path (where on container this volume is mounted)</li> <li>a sha256 hash value of the parmeter's remote value</li> </ul> <p>PATTERN: {MOUNT_PATH}/{SHA256_HASH(PARAM_REMOTE_VALUE)}</p> Example <p>Given volume:     - mount path (/opt/efs/shared) Given execution parameter inputs:     - X (s3://bucket/prefix/A)     - Y (s3://bucket/prefix/A)     - Z (s3://bucket/prefix/B) Output:    - X -&gt; /opt/efs/shared/abcdef...AAAA    - Y -&gt; /opt/efs/shared/abcdef...AAAA    - Z -&gt; /opt/efs/shared/abcdef...BBBB</p> <p>Parameters:</p> Name Type Description Default <code>demand_execution</code> <code>DemandExecution</code> <p>Demand execution object to modify (copied)</p> required <code>container_shared_path</code> <code>Path</code> <p>Path where the shared volume is mounted.</p> required <code>container_working_path</code> <code>Path</code> <p>Path where the working directory is mounted.</p> required <code>isolate_inputs</code> <code>bool</code> <p>flag to determine if inputs should be isolated</p> <code>False</code> <p>Returns:</p> Type Description <code>DemandExecution</code> <p>a demand execution with modified execution parameter inputs</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/demand/context_manager.py</code> <pre><code>def update_demand_execution_parameter_inputs(\n    demand_execution: DemandExecution,\n    container_shared_path: Path,\n    container_working_path: Path,\n    isolate_inputs: bool = False,\n) -&gt; DemandExecution:\n    \"\"\"Modifies demand execution input destinations with the location of the volume configuration\n\n    This updates the input destinations to a deterministic location under the volume configuration\n    specified. This ensures that inputs shared between jobs can used the same cached results.\n\n    The structure of the path for any input param is comprised of:\n        - volume's mount_path (where on container this volume is mounted)\n        - a sha256 hash value of the parmeter's remote value\n\n    PATTERN: {MOUNT_PATH}/{SHA256_HASH(PARAM_REMOTE_VALUE)}\n\n    Example:\n        Given volume:\n            - mount path (/opt/efs/shared)\n        Given execution parameter inputs:\n            - X (s3://bucket/prefix/A)\n            - Y (s3://bucket/prefix/A)\n            - Z (s3://bucket/prefix/B)\n        Output:\n           - X -&gt; /opt/efs/shared/abcdef...AAAA\n           - Y -&gt; /opt/efs/shared/abcdef...AAAA\n           - Z -&gt; /opt/efs/shared/abcdef...BBBB\n\n    Args:\n        demand_execution (DemandExecution): Demand execution object to modify (copied)\n        container_shared_path (Path): Path where the shared volume is mounted.\n        container_working_path (Path): Path where the working directory is mounted.\n        isolate_inputs (bool): flag to determine if inputs should be isolated\n\n    Returns:\n        a demand execution with modified execution parameter inputs\n    \"\"\"\n\n    demand_execution = demand_execution.copy()\n    execution_params = demand_execution.execution_parameters\n    updated_params = {}\n    for param in execution_params.downloadable_job_param_inputs:\n        if isolate_inputs:\n            local = container_working_path / param.value\n            logger.info(f\"Isolating input {param.name} from shared volume. Local path: {local}\")\n        else:\n            local = container_shared_path / sha256_hexdigest(param.remote_value)\n            logger.info(f\"Using shared volume for input {param.name}. Local path: {local}\")\n\n        new_resolvable = Resolvable(local=local.as_posix(), remote=param.remote_value)\n        updated_params[param.name] = new_resolvable\n\n    execution_params.update_params(**updated_params)\n    return demand_execution\n</code></pre>"},{"location":"api/handlers/demand/context-manager/#aibs_informatics_aws_lambda.handlers.demand.context_manager.update_demand_execution_parameter_outputs","title":"update_demand_execution_parameter_outputs","text":"<pre><code>update_demand_execution_parameter_outputs(\n    demand_execution, container_working_path\n)\n</code></pre> <p>Modifies the demand execution output's local paths relative to the container working path</p> <p>This updates the output destinations to their absolute location under the container working path.</p> <p>PATTERN: {CONTAINER_WORKING_PATH}/{PARAM_VALUE}</p> Example <p>Given volume:     - mount path working dir (/opt/efs/scratch/UUID) Given execution parameter outputs:     - X (s3://bucket/prefix/A)     - Y (s3://bucket/prefix/B)     - Z (s3://bucket/prefix/C) Output (local value):    - X -&gt; /opt/efs/scratch/UUID/X    - Y -&gt; /opt/efs/scratch/UUID/Y    - Z -&gt; /opt/efs/scratch/UUID/Z</p> <p>Parameters:</p> Name Type Description Default <code>demand_execution</code> <code>DemandExecution</code> <p>Demand execution object to modify (copied)</p> required <code>container_working_path</code> <code>Path</code> <p>Path where the working directory is mounted.</p> required <p>Returns:</p> Type Description <code>DemandExecution</code> <p>a demand execution with modified execution parameter inputs</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/demand/context_manager.py</code> <pre><code>def update_demand_execution_parameter_outputs(\n    demand_execution: DemandExecution,\n    container_working_path: Path,\n) -&gt; DemandExecution:\n    \"\"\"Modifies the demand execution output's local paths relative to the container working path\n\n    This updates the output destinations to their absolute location under the container working\n    path.\n\n    PATTERN: {CONTAINER_WORKING_PATH}/{PARAM_VALUE}\n\n    Example:\n        Given volume:\n            - mount path working dir (/opt/efs/scratch/UUID)\n        Given execution parameter outputs:\n            - X (s3://bucket/prefix/A)\n            - Y (s3://bucket/prefix/B)\n            - Z (s3://bucket/prefix/C)\n        Output (local value):\n           - X -&gt; /opt/efs/scratch/UUID/X\n           - Y -&gt; /opt/efs/scratch/UUID/Y\n           - Z -&gt; /opt/efs/scratch/UUID/Z\n\n    Args:\n        demand_execution (DemandExecution): Demand execution object to modify (copied)\n        container_working_path (Path): Path where the working directory is mounted.\n\n    Returns:\n        a demand execution with modified execution parameter inputs\n    \"\"\"\n\n    demand_execution = demand_execution.copy()\n    execution_params = demand_execution.execution_parameters\n    updated_params = {\n        param.name: Uploadable(\n            local=(container_working_path / param.value).as_posix(),\n            remote=param.remote_value,\n        )\n        for param in execution_params.uploadable_job_param_outputs\n    }\n    execution_params.update_params(**updated_params)\n    return demand_execution\n</code></pre>"},{"location":"api/handlers/demand/model/","title":"Demand Model","text":"<p>Data models for demand execution operations.</p> <p>Demand execution data models.</p> <p>Defines the request, response, and configuration models for demand execution scaffolding and management.</p>"},{"location":"api/handlers/demand/model/#aibs_informatics_aws_lambda.handlers.demand.model.ContextManagerConfiguration","title":"ContextManagerConfiguration  <code>dataclass</code>","text":"<pre><code>ContextManagerConfiguration(\n    isolate_inputs=custom_field(default=True),\n    cleanup_inputs=custom_field(default=True),\n    cleanup_working_dir=custom_field(default=True),\n    env_file_write_mode=custom_field(\n        mm_field=(EnumField(EnvFileWriteMode)),\n        default=(EnvFileWriteMode.ALWAYS),\n    ),\n    input_data_sync_configuration=custom_field(\n        default_factory=DataSyncConfiguration,\n        mm_field=(DataSyncConfiguration.as_mm_field()),\n    ),\n    output_data_sync_configuration=custom_field(\n        default_factory=DataSyncConfiguration,\n        mm_field=(DataSyncConfiguration.as_mm_field()),\n    ),\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Configuration for demand execution context management.</p> <p>Controls behavior of input/output handling, cleanup, and environment variable management.</p> <p>Attributes:</p> Name Type Description <code>isolate_inputs</code> <code>bool</code> <p>If True, copy inputs to working directory instead of using shared scratch. Useful for mutable inputs.</p> <code>cleanup_inputs</code> <code>bool</code> <p>If True, remove input data after execution.</p> <code>cleanup_working_dir</code> <code>bool</code> <p>If True, remove working directory after execution.</p> <code>env_file_write_mode</code> <code>EnvFileWriteMode</code> <p>How to handle environment variable files.</p> <code>input_data_sync_configuration</code> <code>DataSyncConfiguration</code> <p>Configuration for input data sync.</p> <code>output_data_sync_configuration</code> <code>DataSyncConfiguration</code> <p>Configuration for output data sync.</p>"},{"location":"api/handlers/demand/model/#aibs_informatics_aws_lambda.handlers.demand.model.DataSyncConfiguration","title":"DataSyncConfiguration  <code>dataclass</code>","text":"<pre><code>DataSyncConfiguration(\n    temporary_request_payload_path=custom_field(\n        default=None, mm_field=(S3Path.as_mm_field())\n    ),\n    force=custom_field(default=False),\n    size_only=custom_field(default=True),\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Configuration for data synchronization behavior.</p> <p>Controls how data is synced between S3 and EFS for demand executions.</p> <p>Attributes:</p> Name Type Description <code>temporary_request_payload_path</code> <code>Optional[S3Path]</code> <p>Optional S3 path for storing large request payloads that exceed state machine limits.</p> <code>force</code> <code>bool</code> <p>If True, sync data even if it already exists and passes checksum/size validation.</p> <code>size_only</code> <code>bool</code> <p>If True, only check file sizes when validating sync. If False, also verify checksums.</p>"},{"location":"api/handlers/demand/model/#aibs_informatics_aws_lambda.handlers.demand.model.DemandExecutionCleanupConfigs","title":"DemandExecutionCleanupConfigs  <code>dataclass</code>","text":"<pre><code>DemandExecutionCleanupConfigs(\n    data_sync_requests=custom_field(\n        mm_field=(\n            UnionField(\n                [\n                    (\n                        list,\n                        ListField(\n                            PrepareBatchDataSyncRequest.as_mm_field()\n                        ),\n                    ),\n                    (\n                        list,\n                        ListField(\n                            DataSyncRequest.as_mm_field()\n                        ),\n                    ),\n                ]\n            )\n        )\n    ),\n    remove_data_paths_requests=custom_field(\n        mm_field=(\n            ListField(RemoveDataPathsRequest.as_mm_field())\n        ),\n        default_factory=list,\n    ),\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Cleanup configurations generated for a demand execution.</p> <p>Contains the data sync requests and path removal requests to execute after the demand execution completes.</p> <p>Attributes:</p> Name Type Description <code>data_sync_requests</code> <code>List[Union[DataSyncRequest, PrepareBatchDataSyncRequest]]</code> <p>Requests for syncing output data.</p> <code>remove_data_paths_requests</code> <code>List[RemoveDataPathsRequest]</code> <p>Requests to remove temporary data.</p>"},{"location":"api/handlers/demand/model/#aibs_informatics_aws_lambda.handlers.demand.model.DemandExecutionSetupConfigs","title":"DemandExecutionSetupConfigs  <code>dataclass</code>","text":"<pre><code>DemandExecutionSetupConfigs(\n    data_sync_requests=custom_field(\n        mm_field=(\n            UnionField(\n                [\n                    (\n                        list,\n                        ListField(\n                            PrepareBatchDataSyncRequest.as_mm_field()\n                        ),\n                    ),\n                    (\n                        list,\n                        ListField(\n                            DataSyncRequest.as_mm_field()\n                        ),\n                    ),\n                ]\n            )\n        )\n    ),\n    batch_create_request=custom_field(\n        mm_field=(\n            CreateDefinitionAndPrepareArgsRequest.as_mm_field()\n        )\n    ),\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Setup configurations generated for a demand execution.</p> <p>Contains the data sync requests and batch job configuration needed to run the demand execution.</p> <p>Attributes:</p> Name Type Description <code>data_sync_requests</code> <code>List[Union[DataSyncRequest, PrepareBatchDataSyncRequest]]</code> <p>Requests for syncing input data.</p> <code>batch_create_request</code> <code>CreateDefinitionAndPrepareArgsRequest</code> <p>Request to create the batch job.</p>"},{"location":"api/handlers/demand/model/#aibs_informatics_aws_lambda.handlers.demand.model.DemandFileSystemConfigurations","title":"DemandFileSystemConfigurations  <code>dataclass</code>","text":"<pre><code>DemandFileSystemConfigurations(\n    shared=custom_field(\n        mm_field=(FileSystemConfiguration.as_mm_field()),\n        default_factory=FileSystemConfiguration,\n    ),\n    scratch=custom_field(\n        mm_field=(FileSystemConfiguration.as_mm_field()),\n        default_factory=FileSystemConfiguration,\n    ),\n    tmp=custom_field(\n        mm_field=(FileSystemConfiguration.as_mm_field()),\n        default=None,\n    ),\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Collection of file system configurations for demand execution.</p> <p>Attributes:</p> Name Type Description <code>shared</code> <code>FileSystemConfiguration</code> <p>Configuration for the shared/input volume (read-only).</p> <code>scratch</code> <code>FileSystemConfiguration</code> <p>Configuration for the scratch/working volume (read-write).</p> <code>tmp</code> <code>Optional[FileSystemConfiguration]</code> <p>Optional configuration for a dedicated tmp volume.</p>"},{"location":"api/handlers/demand/model/#aibs_informatics_aws_lambda.handlers.demand.model.EnvFileWriteMode","title":"EnvFileWriteMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Modes for writing environment files in batch jobs.</p> <p>Controls whether environment variables are written to a file on EFS or passed directly to the container.</p> <p>Attributes:</p> Name Type Description <code>NEVER</code> <p>Never write env file, always pass variables directly.</p> <code>ALWAYS</code> <p>Always write env file to avoid variable size limits.</p> <code>IF_REQUIRED</code> <p>Write env file only if variables exceed size threshold.</p>"},{"location":"api/handlers/demand/model/#aibs_informatics_aws_lambda.handlers.demand.model.FileSystemConfiguration","title":"FileSystemConfiguration  <code>dataclass</code>","text":"<pre><code>FileSystemConfiguration(\n    file_system=None, access_point=None, container_path=None\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Configuration for an EFS file system mount.</p> <p>Attributes:</p> Name Type Description <code>file_system</code> <code>Optional[str]</code> <p>Optional file system ID or name.</p> <code>access_point</code> <code>Optional[str]</code> <p>Optional access point ID or name.</p> <code>container_path</code> <code>Optional[str]</code> <p>Optional custom container mount path.</p>"},{"location":"api/handlers/demand/model/#aibs_informatics_aws_lambda.handlers.demand.model.PrepareDemandScaffoldingRequest","title":"PrepareDemandScaffoldingRequest  <code>dataclass</code>","text":"<pre><code>PrepareDemandScaffoldingRequest(\n    demand_execution=custom_field(\n        mm_field=(DemandExecution.as_mm_field())\n    ),\n    file_system_configurations=custom_field(\n        mm_field=(\n            DemandFileSystemConfigurations.as_mm_field()\n        ),\n        default_factory=DemandFileSystemConfigurations,\n    ),\n    context_manager_configuration=custom_field(\n        mm_field=(\n            ContextManagerConfiguration.as_mm_field()\n        ),\n        default_factory=ContextManagerConfiguration,\n    ),\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Request for preparing demand execution scaffolding.</p> <p>Attributes:</p> Name Type Description <code>demand_execution</code> <code>DemandExecution</code> <p>The demand execution to prepare infrastructure for.</p> <code>file_system_configurations</code> <code>DemandFileSystemConfigurations</code> <p>EFS mount configurations.</p> <code>context_manager_configuration</code> <code>ContextManagerConfiguration</code> <p>Execution context settings.</p>"},{"location":"api/handlers/demand/model/#aibs_informatics_aws_lambda.handlers.demand.model.PrepareDemandScaffoldingResponse","title":"PrepareDemandScaffoldingResponse  <code>dataclass</code>","text":"<pre><code>PrepareDemandScaffoldingResponse(\n    demand_execution=custom_field(\n        mm_field=(DemandExecution.as_mm_field())\n    ),\n    setup_configs=custom_field(\n        mm_field=(DemandExecutionSetupConfigs.as_mm_field())\n    ),\n    cleanup_configs=custom_field(\n        mm_field=(\n            DemandExecutionCleanupConfigs.as_mm_field()\n        )\n    ),\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Response from preparing demand execution scaffolding.</p> <p>Attributes:</p> Name Type Description <code>demand_execution</code> <code>DemandExecution</code> <p>The updated demand execution with resolved paths.</p> <code>setup_configs</code> <code>DemandExecutionSetupConfigs</code> <p>Configurations for pre-execution setup.</p> <code>cleanup_configs</code> <code>DemandExecutionCleanupConfigs</code> <p>Configurations for post-execution cleanup.</p>"},{"location":"api/handlers/demand/scaffolding/","title":"Demand Scaffolding","text":"<p>Handler for preparing demand execution scaffolding.</p> <p>Demand execution scaffolding handler.</p> <p>Provides Lambda handlers for preparing demand execution scaffolding, including file system setup and batch job configuration.</p>"},{"location":"api/handlers/demand/scaffolding/#aibs_informatics_aws_lambda.handlers.demand.scaffolding.PrepareDemandScaffoldingHandler","title":"PrepareDemandScaffoldingHandler  <code>dataclass</code>","text":"<pre><code>PrepareDemandScaffoldingHandler()\n</code></pre> <p>               Bases: <code>LambdaHandler[PrepareDemandScaffoldingRequest, PrepareDemandScaffoldingResponse]</code></p> <p>Handler for preparing demand execution scaffolding.</p> <p>Sets up the necessary infrastructure for demand executions including: - EFS volume configurations for scratch, shared, and tmp storage - Pre-execution data sync requests for input data - Post-execution data sync requests for output data - Batch job builder configuration</p> Example <pre><code>handler = PrepareDemandScaffoldingHandler.get_handler()\nresponse = handler(event, context)\n</code></pre>"},{"location":"api/handlers/demand/scaffolding/#aibs_informatics_aws_lambda.handlers.demand.scaffolding.PrepareDemandScaffoldingHandler.handle","title":"handle","text":"<pre><code>handle(request)\n</code></pre> <p>Prepare scaffolding for a demand execution.</p> <p>Sets up EFS configurations, creates the execution context manager, and generates setup and cleanup configurations.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>PrepareDemandScaffoldingRequest</code> <p>Request containing demand execution details and file system configurations.</p> required <p>Returns:</p> Type Description <code>PrepareDemandScaffoldingResponse</code> <p>Response containing the updated demand execution and</p> <code>PrepareDemandScaffoldingResponse</code> <p>setup/cleanup configurations.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/demand/scaffolding.py</code> <pre><code>def handle(self, request: PrepareDemandScaffoldingRequest) -&gt; PrepareDemandScaffoldingResponse:\n    \"\"\"Prepare scaffolding for a demand execution.\n\n    Sets up EFS configurations, creates the execution context manager,\n    and generates setup and cleanup configurations.\n\n    Args:\n        request (PrepareDemandScaffoldingRequest): Request containing demand execution\n            details and file system configurations.\n\n    Returns:\n        Response containing the updated demand execution and\n        setup/cleanup configurations.\n    \"\"\"\n    scratch_vol_configuration = construct_batch_efs_configuration(\n        env_base=self.env_base,\n        file_system=request.file_system_configurations.scratch.file_system,\n        access_point=request.file_system_configurations.scratch.access_point\n        if request.file_system_configurations.scratch.access_point\n        else EFS_SCRATCH_ACCESS_POINT_NAME,\n        container_path=request.file_system_configurations.scratch.container_path\n        if request.file_system_configurations.scratch.container_path\n        else f\"/opt/efs{EFS_SCRATCH_PATH}\",\n        read_only=False,\n    )\n\n    shared_vol_configuration = construct_batch_efs_configuration(\n        env_base=self.env_base,\n        file_system=request.file_system_configurations.shared.file_system,\n        access_point=request.file_system_configurations.shared.access_point\n        if request.file_system_configurations.shared.access_point\n        else EFS_SHARED_ACCESS_POINT_NAME,\n        container_path=request.file_system_configurations.shared.container_path\n        if request.file_system_configurations.shared.container_path\n        else f\"/opt/efs{EFS_SHARED_PATH}\",\n        read_only=True,\n    )\n\n    if request.file_system_configurations.tmp is not None:\n        tmp_vol_configuration = construct_batch_efs_configuration(\n            env_base=self.env_base,\n            file_system=request.file_system_configurations.tmp.file_system,\n            access_point=request.file_system_configurations.tmp.access_point\n            if request.file_system_configurations.tmp.access_point\n            else EFS_TMP_ACCESS_POINT_NAME,\n            container_path=request.file_system_configurations.tmp.container_path\n            if request.file_system_configurations.tmp.container_path\n            else f\"/opt/efs{EFS_TMP_PATH}\",\n            read_only=False,\n        )\n    else:\n        tmp_vol_configuration = None\n\n    context_manager = DemandExecutionContextManager(\n        demand_execution=request.demand_execution,\n        scratch_vol_configuration=scratch_vol_configuration,\n        shared_vol_configuration=shared_vol_configuration,\n        tmp_vol_configuration=tmp_vol_configuration,\n        configuration=request.context_manager_configuration,\n        env_base=self.env_base,\n    )\n    batch_job_builder = context_manager.batch_job_builder\n\n    self.setup_file_system(context_manager)\n    setup_configs = DemandExecutionSetupConfigs(\n        data_sync_requests=[\n            sync_request.from_dict(sync_request.to_dict())\n            for sync_request in context_manager.pre_execution_data_sync_requests\n        ],\n        batch_create_request=CreateDefinitionAndPrepareArgsRequest(\n            image=batch_job_builder.image,\n            job_definition_name=batch_job_builder.job_definition_name,\n            job_name=batch_job_builder.job_name,\n            job_queue_name=context_manager.batch_job_queue_name,\n            job_definition_tags=batch_job_builder.job_definition_tags,\n            command=batch_job_builder.command,\n            environment=batch_job_builder.environment,\n            resource_requirements=batch_job_builder.resource_requirements,\n            mount_points=batch_job_builder.mount_points,\n            volumes=batch_job_builder.volumes,\n            retry_strategy=build_retry_strategy(num_retries=5),\n            privileged=batch_job_builder.privileged,\n            job_role_arn=batch_job_builder.job_role_arn,\n        ),\n    )\n\n    cleanup_configs = DemandExecutionCleanupConfigs(\n        data_sync_requests=[\n            sync_request.from_dict(sync_request.to_dict())\n            for sync_request in context_manager.post_execution_data_sync_requests\n        ],\n        remove_data_paths_requests=context_manager.post_execution_remove_data_paths_requests,\n    )\n\n    return PrepareDemandScaffoldingResponse(\n        demand_execution=context_manager.demand_execution,\n        setup_configs=setup_configs,\n        cleanup_configs=cleanup_configs,\n    )\n</code></pre>"},{"location":"api/handlers/demand/scaffolding/#aibs_informatics_aws_lambda.handlers.demand.scaffolding.PrepareDemandScaffoldingHandler.setup_file_system","title":"setup_file_system","text":"<pre><code>setup_file_system(context_manager)\n</code></pre> <p>Sets up working directory for file system</p> <p>Parameters:</p> Name Type Description Default <code>context_manager</code> <code>DemandExecutionContextManager</code> <p>context manager</p> required Source code in <code>src/aibs_informatics_aws_lambda/handlers/demand/scaffolding.py</code> <pre><code>def setup_file_system(self, context_manager: DemandExecutionContextManager):\n    \"\"\"Sets up working directory for file system\n\n    Args:\n        context_manager (DemandExecutionContextManager): context manager\n    \"\"\"\n    working_path = context_manager.container_working_path  # noqa: F841\n</code></pre>"},{"location":"api/handlers/demand/scaffolding/#aibs_informatics_aws_lambda.handlers.demand.scaffolding.construct_batch_efs_configuration","title":"construct_batch_efs_configuration","text":"<pre><code>construct_batch_efs_configuration(\n    env_base,\n    container_path,\n    file_system,\n    access_point,\n    read_only=False,\n)\n</code></pre> <p>Construct a BatchEFSConfiguration for a volume.</p> <p>Creates a mount point configuration based on the provided file system and access point parameters, resolving resources by tags if names are provided.</p> <p>Parameters:</p> Name Type Description Default <code>env_base</code> <code>EnvBase</code> <p>Environment base for resource name resolution.</p> required <code>container_path</code> <code>Union[Path, str]</code> <p>Path where the volume will be mounted in the container.</p> required <code>file_system</code> <code>Optional[str]</code> <p>File system ID or name (optional, resolved via tags).</p> required <code>access_point</code> <code>Optional[str]</code> <p>Access point ID or name (optional, resolved via tags).</p> required <code>read_only</code> <code>bool</code> <p>Whether the mount should be read-only.</p> <code>False</code> <p>Returns:</p> Type Description <code>BatchEFSConfiguration</code> <p>Configured BatchEFSConfiguration for use with AWS Batch.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/demand/scaffolding.py</code> <pre><code>def construct_batch_efs_configuration(\n    env_base: EnvBase,\n    container_path: Union[Path, str],\n    file_system: Optional[str],\n    access_point: Optional[str],\n    read_only: bool = False,\n) -&gt; BatchEFSConfiguration:\n    \"\"\"Construct a BatchEFSConfiguration for a volume.\n\n    Creates a mount point configuration based on the provided file system\n    and access point parameters, resolving resources by tags if names\n    are provided.\n\n    Args:\n        env_base (EnvBase): Environment base for resource name resolution.\n        container_path (Union[Path, str]): Path where the volume will be mounted in the container.\n        file_system (Optional[str]): File system ID or name (optional, resolved via tags).\n        access_point (Optional[str]): Access point ID or name (optional, resolved via tags).\n        read_only (bool): Whether the mount should be read-only.\n\n    Returns:\n        Configured BatchEFSConfiguration for use with AWS Batch.\n    \"\"\"\n    mount_point_config = MountPointConfiguration.build(\n        mount_point=container_path,\n        access_point=access_point,\n        file_system=file_system,\n        access_point_tags={\"env_base\": env_base},\n        file_system_tags={\"env_base\": env_base},\n    )\n    return BatchEFSConfiguration(mount_point_config=mount_point_config, read_only=read_only)\n</code></pre>"},{"location":"api/handlers/ecr/","title":"ECR Handlers","text":"<p>Handlers for ECR image operations.</p>"},{"location":"api/handlers/ecr/#overview","title":"Overview","text":"Module Description Replicate Image <code>ImageReplicatorHandler</code> for replicating ECR images"},{"location":"api/handlers/ecr/#quick-start","title":"Quick Start","text":"<pre><code>from aibs_informatics_aws_lambda.handlers.ecr.replicate_image import ImageReplicatorHandler\n\nhandler = ImageReplicatorHandler().get_handler()\n</code></pre>"},{"location":"api/handlers/ecr/replicate-image/","title":"Replicate Image","text":"<p>Handler for replicating ECR images between repositories.</p> <p>ECR image replication handlers.</p> <p>Provides Lambda handlers for replicating Docker images between ECR repositories.</p>"},{"location":"api/handlers/ecr/replicate-image/#aibs_informatics_aws_lambda.handlers.ecr.replicate_image.ImageReplicatorHandler","title":"ImageReplicatorHandler  <code>dataclass</code>","text":"<pre><code>ImageReplicatorHandler()\n</code></pre> <p>               Bases: <code>LambdaHandler[ReplicateImageRequest, ReplicateImageResponse]</code></p> <p>Handler for replicating ECR images between repositories.</p> <p>Wraps the ECRImageReplicator to provide Lambda integration.</p>"},{"location":"api/handlers/ecr/replicate-image/#aibs_informatics_aws_lambda.handlers.ecr.replicate_image.ImageReplicatorHandler.handle","title":"handle","text":"<pre><code>handle(request)\n</code></pre> <p>Replicate an ECR image to a target repository.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>ReplicateImageRequest</code> <p>Request containing source and target image specifications.</p> required <p>Returns:</p> Type Description <code>ReplicateImageResponse</code> <p>Response containing the replication result.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/ecr/replicate_image.py</code> <pre><code>def handle(self, request: ReplicateImageRequest) -&gt; ReplicateImageResponse:\n    \"\"\"Replicate an ECR image to a target repository.\n\n    Args:\n        request (ReplicateImageRequest): Request containing source and target\n            image specifications.\n\n    Returns:\n        Response containing the replication result.\n    \"\"\"\n    return ECRImageReplicator().process_request(request)\n</code></pre>"},{"location":"api/handlers/notifications/","title":"Notification Handlers","text":"<p>Handlers for notification routing and delivery.</p>"},{"location":"api/handlers/notifications/#overview","title":"Overview","text":"Module Description Router <code>NotificationRouter</code> for routing notifications Model Data models for notifications Notifiers Base notifier and implementations"},{"location":"api/handlers/notifications/#notifiers","title":"Notifiers","text":"Notifier Description SES Send notifications via Amazon SES SNS Send notifications via Amazon SNS"},{"location":"api/handlers/notifications/#quick-start","title":"Quick Start","text":"<pre><code>from aibs_informatics_aws_lambda.handlers.notifications.router import NotificationRouter\n\nhandler = NotificationRouter().get_handler()\n</code></pre>"},{"location":"api/handlers/notifications/model/","title":"Notification Model","text":"<p>Data models for notification operations.</p> <p>Notification data models.</p> <p>Defines the request and response models for the notification system.</p>"},{"location":"api/handlers/notifications/model/#aibs_informatics_aws_lambda.handlers.notifications.model.MESSAGE_KEY_ALIASES","title":"MESSAGE_KEY_ALIASES  <code>module-attribute</code>","text":"<pre><code>MESSAGE_KEY_ALIASES = ['content', 'body']\n</code></pre> <p>Alternative field names accepted for the message content.</p>"},{"location":"api/handlers/notifications/model/#aibs_informatics_aws_lambda.handlers.notifications.model.NotificationContent","title":"NotificationContent  <code>dataclass</code>","text":"<pre><code>NotificationContent(\n    subject=custom_field(mm_field=(StringField())),\n    message=custom_field(mm_field=(StringField())),\n    content_type=custom_field(\n        mm_field=(EnumField(NotificationContentType)),\n        default=(NotificationContentType.PLAIN_TEXT),\n    ),\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Content of a notification message.</p> <p>Attributes:</p> Name Type Description <code>subject</code> <code>str</code> <p>The subject line of the notification.</p> <code>message</code> <code>str</code> <p>The body content of the notification.</p> <code>content_type</code> <code>NotificationContentType</code> <p>The format of the message content.</p>"},{"location":"api/handlers/notifications/model/#aibs_informatics_aws_lambda.handlers.notifications.model.NotificationContentType","title":"NotificationContentType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Content types for notification messages.</p> <p>Attributes:</p> Name Type Description <code>PLAIN_TEXT</code> <p>Plain text content type.</p> <code>HTML</code> <p>HTML formatted content.</p> <code>JSON</code> <p>JSON structured content.</p>"},{"location":"api/handlers/notifications/model/#aibs_informatics_aws_lambda.handlers.notifications.model.NotificationRequest","title":"NotificationRequest  <code>dataclass</code>","text":"<pre><code>NotificationRequest(\n    content=custom_field(\n        mm_field=(NotificationContent.as_mm_field())\n    ),\n    targets=custom_field(\n        mm_field=(\n            ListField(\n                UnionField(\n                    [\n                        (_, _.as_mm_field())\n                        for _ in [\n                            SESEmailTarget,\n                            SNSTopicTarget,\n                        ]\n                    ]\n                )\n            )\n        )\n    ),\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Request model for sending notifications.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>NotificationContent</code> <p>The notification content to deliver.</p> <code>targets</code> <code>List[Union[SESEmailTarget, SNSTopicTarget]]</code> <p>List of delivery targets (SES or SNS).</p>"},{"location":"api/handlers/notifications/model/#aibs_informatics_aws_lambda.handlers.notifications.model.NotificationResponse","title":"NotificationResponse  <code>dataclass</code>","text":"<pre><code>NotificationResponse(\n    results=custom_field(\n        mm_field=(ListField(NotifierResult.as_mm_field()))\n    ),\n)\n</code></pre> <p>               Bases: <code>SchemaModel</code></p> <p>Response model for notification delivery.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>List[NotifierResult]</code> <p>List of results for each notification target.</p>"},{"location":"api/handlers/notifications/router/","title":"Notification Router","text":"<p>Routes notifications to the appropriate notifier based on configuration.</p> <p>Notification routing handler.</p> <p>Provides the main Lambda handler for routing notifications to appropriate delivery channels.</p>"},{"location":"api/handlers/notifications/router/#aibs_informatics_aws_lambda.handlers.notifications.router.NotificationRouter","title":"NotificationRouter  <code>dataclass</code>","text":"<pre><code>NotificationRouter(notifiers=list())\n</code></pre> <p>               Bases: <code>LambdaHandler[NotificationRequest, NotificationResponse]</code></p> <p>Handler for routing notifications to appropriate delivery channels.</p> <p>Routes notifications to different notifiers (SES, SNS, etc.) based on the target specification using a chain-of-responsibility pattern.</p> <p>Each notifier attempts to parse and handle the target. If successful, it delivers the notification. If not, the next notifier is tried.</p> <p>Attributes:</p> Name Type Description <code>notifiers</code> <code>List[Notifier]</code> <p>List of notifier instances to try in order.</p> Example <pre><code>handler = NotificationRouter.get_handler()\n# Or with custom notifiers\nhandler = NotificationRouter(notifiers=[SESNotifier()]).get_handler()\n</code></pre>"},{"location":"api/handlers/notifications/router/#aibs_informatics_aws_lambda.handlers.notifications.router.NotificationRouter.handle","title":"handle","text":"<pre><code>handle(request)\n</code></pre> <p>Route and deliver notifications to targets.</p> <p>Attempts to deliver the notification content to each target using the available notifiers.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>NotificationRequest</code> <p>Request containing content and target specifications.</p> required <p>Returns:</p> Type Description <code>NotificationResponse</code> <p>Response containing results for each target.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/notifications/router.py</code> <pre><code>def handle(self, request: NotificationRequest) -&gt; NotificationResponse:\n    \"\"\"Route and deliver notifications to targets.\n\n    Attempts to deliver the notification content to each target\n    using the available notifiers.\n\n    Args:\n        request (NotificationRequest): Request containing content and target specifications.\n\n    Returns:\n        Response containing results for each target.\n    \"\"\"\n    results: List[NotifierResult] = []\n    for target in request.targets:\n        for notifier in self.notifiers:\n            try:\n                target = notifier.parse_target(target=target)\n            except Exception as e:\n                self.logger.error(f\"Could not parse target {target} with {str(notifier)}: {e}\")\n                continue\n            else:\n                self.logger.info(f\"{str(notifier)} handling target {target}\")\n                notifier.notify(content=request.content, target=target)\n                break\n        else:\n            self.logger.error(f\"No notifier could handle target {target}\")\n            results.append(\n                NotifierResult(\n                    target=target.to_dict(),\n                    success=False,\n                    response=\"No notifier could handle target\",\n                )\n            )\n    return NotificationResponse(results=results)\n</code></pre>"},{"location":"api/handlers/notifications/notifiers/base/","title":"Base Notifier","text":"<p>Base class for notification delivery implementations.</p> <p>Base notifier class for notification delivery.</p> <p>Provides the abstract base class for implementing notification delivery to different channels.</p>"},{"location":"api/handlers/notifications/notifiers/base/#aibs_informatics_aws_lambda.handlers.notifications.notifiers.base.Notifier","title":"Notifier  <code>dataclass</code>","text":"<pre><code>Notifier()\n</code></pre> <p>               Bases: <code>Generic[NOTIFIER_TARGET]</code></p> <p>Abstract base class for notification delivery implementations.</p> <p>Defines the interface for sending notifications to specific targets. Subclasses implement the actual delivery logic for different channels (e.g., SES, SNS).</p> <p>          Class Type Parameters:        </p> Name Bound or Constraints Description Default <code>NOTIFIER_TARGET</code> <p>The target model type for this notifier.</p> required Example <pre><code>@dataclass\nclass MyNotifier(Notifier[MyTarget]):\n    def notify(self, content: NotificationContent, target: MyTarget) -&gt; NotifierResult:\n        # Implement delivery logic\n        return NotifierResult(success=True, ...)\n</code></pre>"},{"location":"api/handlers/notifications/notifiers/base/#aibs_informatics_aws_lambda.handlers.notifications.notifiers.base.Notifier.notifier_target_class","title":"notifier_target_class  <code>classmethod</code>","text":"<pre><code>notifier_target_class()\n</code></pre> <p>Get the target class type for this notifier.</p> <p>Returns:</p> Type Description <code>Type[NOTIFIER_TARGET]</code> <p>The target model class.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/notifications/notifiers/base.py</code> <pre><code>@classmethod\ndef notifier_target_class(cls) -&gt; Type[NOTIFIER_TARGET]:\n    \"\"\"Get the target class type for this notifier.\n\n    Returns:\n        The target model class.\n    \"\"\"\n    return cls.__orig_bases__[0].__args__[0]  # type: ignore\n</code></pre>"},{"location":"api/handlers/notifications/notifiers/base/#aibs_informatics_aws_lambda.handlers.notifications.notifiers.base.Notifier.notify","title":"notify  <code>abstractmethod</code>","text":"<pre><code>notify(content, target)\n</code></pre> <p>Deliver a notification to the target.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>NotificationContent</code> <p>The notification content to deliver.</p> required <code>target</code> <code>NOTIFIER_TARGET</code> <p>The delivery target specification.</p> required <p>Returns:</p> Type Description <code>NotifierResult</code> <p>Result indicating success or failure.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/notifications/notifiers/base.py</code> <pre><code>@abstractmethod\ndef notify(self, content: NotificationContent, target: NOTIFIER_TARGET) -&gt; NotifierResult:\n    \"\"\"Deliver a notification to the target.\n\n    Args:\n        content (NotificationContent): The notification content to deliver.\n        target (NOTIFIER_TARGET): The delivery target specification.\n\n    Returns:\n        Result indicating success or failure.\n    \"\"\"\n    raise NotImplementedError(\"Please implement `notify` method\")  # pragma: no cover\n</code></pre>"},{"location":"api/handlers/notifications/notifiers/base/#aibs_informatics_aws_lambda.handlers.notifications.notifiers.base.Notifier.parse_target","title":"parse_target  <code>classmethod</code>","text":"<pre><code>parse_target(target)\n</code></pre> <p>Parse a target from a dictionary or validate an existing target.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Union[Dict[str, Any], NOTIFIER_TARGET]</code> <p>Either a dictionary to parse or an existing target object.</p> required <p>Returns:</p> Type Description <code>NOTIFIER_TARGET</code> <p>The parsed or validated target object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the target cannot be parsed.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/notifications/notifiers/base.py</code> <pre><code>@classmethod\ndef parse_target(cls, target: Union[Dict[str, Any], NOTIFIER_TARGET]) -&gt; NOTIFIER_TARGET:\n    \"\"\"Parse a target from a dictionary or validate an existing target.\n\n    Args:\n        target (Union[Dict[str, Any], NOTIFIER_TARGET]): Either a dictionary to parse or\n            an existing target object.\n\n    Returns:\n        The parsed or validated target object.\n\n    Raises:\n        ValueError: If the target cannot be parsed.\n    \"\"\"\n    if isinstance(target, cls.notifier_target_class()):\n        return target\n    elif isinstance(target, dict):\n        return cls.notifier_target_class().from_dict(target)\n    else:\n        raise ValueError(f\"Could not parse target {target} as {cls.notifier_target_class()}\")\n</code></pre>"},{"location":"api/handlers/notifications/notifiers/ses/","title":"SES Notifier","text":"<p>Sends notifications via Amazon Simple Email Service (SES).</p> <p>SES email notification delivery.</p> <p>Provides notification delivery via Amazon Simple Email Service (SES).</p>"},{"location":"api/handlers/notifications/notifiers/ses/#aibs_informatics_aws_lambda.handlers.notifications.notifiers.ses.DEFAULT_SOURCE_EMAIL_ADDRESS","title":"DEFAULT_SOURCE_EMAIL_ADDRESS  <code>module-attribute</code>","text":"<pre><code>DEFAULT_SOURCE_EMAIL_ADDRESS = (\n    \"marmotdev@alleninstitute.org\"\n)\n</code></pre> <p>Default source email address if environment variable is not set.</p>"},{"location":"api/handlers/notifications/notifiers/ses/#aibs_informatics_aws_lambda.handlers.notifications.notifiers.ses.SOURCE_EMAIL_ADDRESS_ENV_VAR","title":"SOURCE_EMAIL_ADDRESS_ENV_VAR  <code>module-attribute</code>","text":"<pre><code>SOURCE_EMAIL_ADDRESS_ENV_VAR = 'SOURCE_EMAIL_ADDRESS'\n</code></pre> <p>Environment variable name for the source email address.</p>"},{"location":"api/handlers/notifications/notifiers/ses/#aibs_informatics_aws_lambda.handlers.notifications.notifiers.ses.SESNotifier","title":"SESNotifier  <code>dataclass</code>","text":"<pre><code>SESNotifier()\n</code></pre> <p>               Bases: <code>Notifier[SESEmailTarget]</code></p> <p>Notifier implementation for sending emails via Amazon SES.</p> <p>Sends notification content as email to specified recipients using Amazon Simple Email Service.</p> <p>The source email address is configured via the SOURCE_EMAIL_ADDRESS environment variable, with a fallback to the default address.</p> Example <pre><code>notifier = SESNotifier()\nresult = notifier.notify(\n    content=NotificationContent(subject=\"Hello\", message=\"World\"),\n    target=SESEmailTarget(recipients=[\"user@example.com\"])\n)\n</code></pre>"},{"location":"api/handlers/notifications/notifiers/ses/#aibs_informatics_aws_lambda.handlers.notifications.notifiers.ses.SESNotifier.notify","title":"notify","text":"<pre><code>notify(content, target)\n</code></pre> <p>Send an email notification via SES.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>NotificationContent</code> <p>The notification content including subject and message.</p> required <code>target</code> <code>SESEmailTarget</code> <p>The email target containing recipient addresses.</p> required <p>Returns:</p> Type Description <code>NotifierResult</code> <p>Result indicating success or failure with response details.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/notifications/notifiers/ses.py</code> <pre><code>def notify(self, content: NotificationContent, target: SESEmailTarget) -&gt; NotifierResult:\n    \"\"\"Send an email notification via SES.\n\n    Args:\n        content (NotificationContent): The notification content including subject and message.\n        target (SESEmailTarget): The email target containing recipient addresses.\n\n    Returns:\n        Result indicating success or failure with response details.\n    \"\"\"\n    try:\n        source = EmailAddress(\n            get_env_var(\n                SOURCE_EMAIL_ADDRESS_ENV_VAR, default_value=DEFAULT_SOURCE_EMAIL_ADDRESS\n            )\n        )\n\n        response = send_simple_email(\n            source=source,\n            to_addresses=target.recipients,\n            subject=content.subject,\n            body=content.message,\n            # TODO: in future we may want to support html emails\n        )\n        return NotifierResult(\n            response=json.dumps(response),\n            success=(200 &lt;= response[\"ResponseMetadata\"][\"HTTPStatusCode\"] &lt; 300),\n            target=target.to_dict(),\n        )\n    except Exception as e:\n        return NotifierResult(\n            response=str(e),\n            success=False,\n            target=target.to_dict(),\n        )\n</code></pre>"},{"location":"api/handlers/notifications/notifiers/sns/","title":"SNS Notifier","text":"<p>Sends notifications via Amazon Simple Notification Service (SNS).</p> <p>SNS topic notification delivery.</p> <p>Provides notification delivery via Amazon Simple Notification Service (SNS).</p>"},{"location":"api/handlers/notifications/notifiers/sns/#aibs_informatics_aws_lambda.handlers.notifications.notifiers.sns.SNSNotifier","title":"SNSNotifier  <code>dataclass</code>","text":"<pre><code>SNSNotifier()\n</code></pre> <p>               Bases: <code>Notifier[SNSTopicTarget]</code></p> <p>Notifier implementation for publishing to Amazon SNS topics.</p> <p>Publishes notification content to SNS topics for fanout to subscribers (email, SMS, HTTP endpoints, etc.).</p> Example <pre><code>notifier = SNSNotifier()\nresult = notifier.notify(\n    content=NotificationContent(subject=\"Alert\", message=\"Critical event\"),\n    target=SNSTopicTarget(topic=\"arn:aws:sns:us-east-1:123456789012:my-topic\")\n)\n</code></pre>"},{"location":"api/handlers/notifications/notifiers/sns/#aibs_informatics_aws_lambda.handlers.notifications.notifiers.sns.SNSNotifier.notify","title":"notify","text":"<pre><code>notify(content, target)\n</code></pre> <p>Publish a notification to an SNS topic.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>NotificationContent</code> <p>The notification content including subject and message.</p> required <code>target</code> <code>SNSTopicTarget</code> <p>The SNS topic target containing the topic ARN.</p> required <p>Returns:</p> Type Description <code>NotifierResult</code> <p>Result indicating success or failure with response details.</p> Source code in <code>src/aibs_informatics_aws_lambda/handlers/notifications/notifiers/sns.py</code> <pre><code>def notify(self, content: NotificationContent, target: SNSTopicTarget) -&gt; NotifierResult:\n    \"\"\"Publish a notification to an SNS topic.\n\n    Args:\n        content (NotificationContent): The notification content including subject and message.\n        target (SNSTopicTarget): The SNS topic target containing the topic ARN.\n\n    Returns:\n        Result indicating success or failure with response details.\n    \"\"\"\n    try:\n        response = publish_to_topic(\n            message=content.message,\n            subject=content.subject,\n            topic_arn=target.topic,\n        )\n        return NotifierResult(\n            response=json.dumps(response),\n            success=(200 &lt;= response[\"ResponseMetadata\"][\"HTTPStatusCode\"] &lt; 300),\n            target=target.to_dict(),\n        )\n    except Exception as e:\n        return NotifierResult(\n            target=target.to_dict(),\n            success=False,\n            response=str(e),\n        )\n</code></pre>"},{"location":"developer/","title":"Developer Guide","text":"<p>This guide provides information for developers who want to contribute to the AIBS Informatics AWS Lambda library.</p>"},{"location":"developer/#development-setup","title":"Development Setup","text":""},{"location":"developer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>Git</li> <li>Make (optional, but recommended)</li> <li>Docker (for testing Lambda containers)</li> </ul>"},{"location":"developer/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>git clone https://github.com/AllenInstitute/aibs-informatics-aws-lambda.git\ncd aibs-informatics-aws-lambda\n</code></pre>"},{"location":"developer/#install-dependencies","title":"Install Dependencies","text":"<p>Using make:</p> <pre><code>make install\n</code></pre> <p>Using uv (manual):</p> <pre><code>uv sync --group dev\n</code></pre>"},{"location":"developer/#project-structure","title":"Project Structure","text":"<pre><code>aibs-informatics-aws-lambda/\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 aibs_informatics_aws_lambda/\n\u2502       \u251c\u2500\u2500 main.py                 # CLI entry point\n\u2502       \u251c\u2500\u2500 common/                 # Base classes and utilities\n\u2502       \u2514\u2500\u2500 handlers/               # Lambda handler implementations\n\u251c\u2500\u2500 test/                           # Test files\n\u251c\u2500\u2500 docker/                         # Docker configuration\n\u251c\u2500\u2500 docs/                           # Documentation\n\u251c\u2500\u2500 pyproject.toml                  # Project configuration\n\u2514\u2500\u2500 Makefile                        # Build automation\n</code></pre>"},{"location":"developer/#creating-a-new-handler","title":"Creating a New Handler","text":""},{"location":"developer/#basic-handler","title":"Basic Handler","text":"<pre><code>from aibs_informatics_aws_lambda.common.handler import LambdaHandler\n\nclass MyCustomHandler(LambdaHandler):\n    \"\"\"Custom handler for processing events.\"\"\"\n\n    def handle(self, event, context):\n        \"\"\"Process the incoming event.\n\n        Args:\n            event: The Lambda event payload\n            context: The Lambda context object\n\n        Returns:\n            The response payload\n        \"\"\"\n        # Your processing logic here\n        return {\"status\": \"success\", \"data\": event}\n</code></pre>"},{"location":"developer/#handler-with-requestresponse-models","title":"Handler with Request/Response Models","text":"<pre><code>from dataclasses import dataclass\nfrom aibs_informatics_aws_lambda.common.handler import LambdaHandler\nfrom aibs_informatics_core.models import SchemaModel\n\n@dataclass\nclass MyRequest(SchemaModel):\n    input_path: str\n    output_path: str\n\n@dataclass\nclass MyResponse(SchemaModel):\n    status: str\n    files_processed: int\n\nclass TypedHandler(LambdaHandler[MyRequest, MyResponse]):\n    def handle(self, request: MyRequest, context) -&gt; MyResponse:\n        # Process with typed request\n        return MyResponse(status=\"success\", files_processed=10)\n</code></pre>"},{"location":"developer/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nmake test\n\n# Run tests with coverage\nmake test coverage-server\n\n# Run specific test file\npytest test/aibs_informatics_aws_lambda/handlers/test_data_sync.py\n</code></pre>"},{"location":"developer/#building-documentation","title":"Building Documentation","text":"<pre><code># Serve documentation locally\nmake docs-serve\n\n# Build documentation\nmake docs-build\n</code></pre>"},{"location":"developer/#docker-development","title":"Docker Development","text":"<p>The package includes Docker support for Lambda container build:</p> <pre><code># Build the Docker image\nmake docker-build\n</code></pre>"},{"location":"developer/#contributing","title":"Contributing","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch (<code>git checkout -b feature/my-feature</code>)</li> <li>Make your changes</li> <li>Run tests and linting</li> <li>Commit your changes (<code>git commit -am 'Add my feature'</code>)</li> <li>Push to the branch (<code>git push origin feature/my-feature</code>)</li> <li>Create a Pull Request</li> </ol> <p>Please see CONTRIBUTING.md for detailed guidelines.</p>"},{"location":"user-guide/cli-usage/","title":"CLI Usage","text":"<p>The <code>aibs-informatics-aws-lambda</code> package provides a CLI tool for invoking lambda functions locally.</p>"},{"location":"user-guide/cli-usage/#command-overview","title":"Command Overview","text":"<pre><code>handle-lambda-request [-h] [--handler-qualified-name HANDLER_QUALIFIED_NAME] [--payload PAYLOAD] [--response-location RESPONSE_LOCATION]\n</code></pre>"},{"location":"user-guide/cli-usage/#options","title":"Options","text":"Option Description <code>--handler-qualified-name</code>, <code>--handler-name</code>, <code>--handler</code> Handler function qualified name. If not provided, will try to load from <code>AWS_LAMBDA_FUNCTION_HANDLER</code> or <code>_HANDLER</code> env variables <code>--payload</code>, <code>--event</code>, <code>-e</code> Event payload of function. If not provided, will try to load from <code>AWS_LAMBDA_EVENT_PAYLOAD</code> env variable <code>--response-location</code>, <code>-o</code> Optional response location to store response at. Can be S3 or local file. If not provided, will load from <code>AWS_LAMBDA_EVENT_RESPONSE_LOCATION</code> env variable"},{"location":"user-guide/cli-usage/#examples","title":"Examples","text":""},{"location":"user-guide/cli-usage/#invoking-with-json-payload","title":"Invoking with JSON Payload","text":"<pre><code>handle-lambda-request \\\n    --handler-qualified-name aibs_informatics_aws_lambda.handlers.data_sync.operations.GetJSONFromFileHandler \\\n    --payload '{\"path\": \"/path/to/file.json\"}' \\\n    --response-location /tmp/response.json\n</code></pre>"},{"location":"user-guide/cli-usage/#invoking-with-payload-from-file","title":"Invoking with Payload from File","text":"<pre><code>handle-lambda-request \\\n    --handler-qualified-name aibs_informatics_aws_lambda.handlers.data_sync.operations.GetJSONFromFileHandler \\\n    --payload file:///path/to/payload.json \\\n    --response-location /tmp/response.json\n</code></pre>"},{"location":"user-guide/cli-usage/#invoking-with-payload-from-s3","title":"Invoking with Payload from S3","text":"<pre><code>handle-lambda-request \\\n    --handler-qualified-name aibs_informatics_aws_lambda.handlers.data_sync.operations.GetJSONFromFileHandler \\\n    --payload s3://my-bucket/payload.json \\\n    --response-location s3://my-bucket/response.json\n</code></pre>"},{"location":"user-guide/cli-usage/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code>export AWS_LAMBDA_FUNCTION_HANDLER=\"aibs_informatics_aws_lambda.handlers.data_sync.operations.GetJSONFromFileHandler\"\nexport AWS_LAMBDA_EVENT_PAYLOAD='{\"path\": \"/path/to/file.json\"}'\nexport AWS_LAMBDA_EVENT_RESPONSE_LOCATION=\"/tmp/response.json\"\n\nhandle-lambda-request\n</code></pre>"},{"location":"user-guide/cli-usage/#available-handlers","title":"Available Handlers","text":""},{"location":"user-guide/cli-usage/#data-sync-operations","title":"Data Sync Operations","text":"Handler Description <code>aibs_informatics_aws_lambda.handlers.data_sync.operations.GetJSONFromFileHandler</code> Retrieves JSON data from a file <code>aibs_informatics_aws_lambda.handlers.data_sync.operations.PutJSONToFileHandler</code> Writes JSON data to a file <code>aibs_informatics_aws_lambda.handlers.data_sync.operations.DataSyncHandler</code> Simple data sync task <code>aibs_informatics_aws_lambda.handlers.data_sync.operations.BatchDataSyncHandler</code> Handles batch of data sync tasks <code>aibs_informatics_aws_lambda.handlers.data_sync.operations.PrepareBatchDataSyncHandler</code> Prepares batch data sync tasks"},{"location":"user-guide/cli-usage/#data-sync-file-system","title":"Data Sync File System","text":"Handler Description <code>aibs_informatics_aws_lambda.handlers.data_sync.file_system.GetDataPathStatsHandler</code> Retrieves statistics about data paths <code>aibs_informatics_aws_lambda.handlers.data_sync.file_system.ListDataPathsHandler</code> Lists data paths <code>aibs_informatics_aws_lambda.handlers.data_sync.file_system.OutdatedDataPathScannerHandler</code> Scans for outdated data paths <code>aibs_informatics_aws_lambda.handlers.data_sync.file_system.RemoveDataPathsHandler</code> Removes data paths"},{"location":"user-guide/cli-usage/#aws-batch","title":"AWS Batch","text":"Handler Description <code>aibs_informatics_aws_lambda.handlers.batch.create.CreateDefinitionAndPrepareArgsHandler</code> Creates and prepares AWS Batch job definitions"},{"location":"user-guide/cli-usage/#demand","title":"Demand","text":"Handler Description <code>aibs_informatics_aws_lambda.handlers.demand.scaffolding.PrepareDemandScaffoldingHandler</code> Prepares scaffolding for demand execution"},{"location":"user-guide/cli-usage/#ecr","title":"ECR","text":"Handler Description <code>aibs_informatics_aws_lambda.handlers.ecr.replicate_image.ImageReplicatorHandler</code> Replicates ECR images between repositories"},{"location":"user-guide/cli-usage/#notifications","title":"Notifications","text":"Handler Description <code>aibs_informatics_aws_lambda.handlers.notifications.router.NotificationRouter</code> Routes notifications to appropriate notifier"},{"location":"user-guide/getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with the AIBS Informatics AWS Lambda library.</p>"},{"location":"user-guide/getting-started/#installation","title":"Installation","text":""},{"location":"user-guide/getting-started/#using-pip","title":"Using pip","text":"<pre><code>pip install aibs-informatics-aws-lambda\n</code></pre>"},{"location":"user-guide/getting-started/#using-uv","title":"Using uv","text":"<pre><code>uv add aibs-informatics-aws-lambda\n</code></pre>"},{"location":"user-guide/getting-started/#basic-concepts","title":"Basic Concepts","text":""},{"location":"user-guide/getting-started/#lambda-handlers","title":"Lambda Handlers","text":"<p>The <code>LambdaHandler</code> class provides a base class for creating strongly typed lambda functions with features like serialization/deserialization, logging, and metrics.</p> <pre><code>from dataclasses import dataclass\nfrom aibs_informatics_core.models.base import SchemaModel\nfrom aibs_informatics_aws_lambda.common.handler import LambdaHandler\n\n@dataclass\nclass MyRequest(SchemaModel):\n    name: str\n\n@dataclass\nclass MyResponse(SchemaModel):\n    message: str\n\nclass MyHandler(LambdaHandler[MyRequest, MyResponse]):\n    def handle(self, request: MyRequest) -&gt; MyResponse:\n        return MyResponse(message=f\"Hello, {request.name}!\")\n\n# Create handler function for AWS Lambda\nhandler = MyHandler.get_handler()\n</code></pre>"},{"location":"user-guide/getting-started/#api-gateway-handlers","title":"API Gateway Handlers","text":"<p>For API Gateway integrations, use <code>ApiLambdaHandler</code>:</p> <pre><code>from dataclasses import dataclass\nfrom aibs_informatics_core.models.base import SchemaModel\nfrom aibs_informatics_aws_lambda.common.api.handler import ApiLambdaHandler\n\n@dataclass\nclass UserRequest(SchemaModel):\n    user_id: str\n\n@dataclass\nclass UserResponse(SchemaModel):\n    name: str\n    email: str\n\n@dataclass\nclass GetUserHandler(ApiLambdaHandler[UserRequest, UserResponse]):\n    @classmethod\n    def route_rule(cls) -&gt; str:\n        return \"/users/{user_id}\"\n\n    @classmethod\n    def route_method(cls) -&gt; str:\n        return \"GET\"\n\n    def handle(self, request: UserRequest) -&gt; UserResponse:\n        # Fetch user and return response\n        return UserResponse(name=\"John Doe\", email=\"john@example.com\")\n</code></pre>"},{"location":"user-guide/getting-started/#using-built-in-handlers","title":"Using Built-in Handlers","text":"<p>The library provides several ready-to-use handlers for common tasks:</p> <pre><code>from aibs_informatics_aws_lambda.handlers.data_sync.operations import GetJSONFromFileHandler\n\n# Use directly as a Lambda handler\nhandler = GetJSONFromFileHandler().get_handler()\n</code></pre>"},{"location":"user-guide/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about CLI Usage for local testing</li> <li>Explore the API Reference for detailed documentation</li> <li>See the Developer Guide for contribution guidelines</li> </ul>"}]}